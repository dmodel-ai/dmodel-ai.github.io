<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Alex Sanchez-Stern and Anish Tondwalkar" />
  <title>Understanding Models Understanding Nullability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <link rel="stylesheet" href="sidenote.css" />
  <script src="sidenotes.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Understanding Models Understanding Nullability</h1>
<p class="author"><a href="https://www.alexsanchezstern.com">Alex
Sanchez-Stern</a> and <a href="https://ani.sh">Anish Tondwalkar</a></p>
<p class="date"><span class="math inline">\(d_{model}\)</span></p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Large language models have demonstrated impressive emergent
capabilities, including the ability to write code, but this ability
requires a model of program semantics that is little understood. Recent
interpretability work has shown the ability to extract internal
representations of natural language concepts, raising the possibility
that similar techniques could be used to extract program semantics
concepts. In this work we study how large language models represent
nullability of program values. We measure how well models of various
sizes complete programs that use nullable values, and then extract an
internal representation of nullability.
</div>
</header>
<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>The last five years have shown us that Large Language Models can
effectively write programs in many
domains.<span class="aside AT">AT: cite</span> This is an impressive
capability given that writing programs involves having a working
understanding of many aspects of their semantics. But though we know
that these large models understand programs to an extent, we still don’t
know what form that understanding takes; where it has a deep
understanding and where it uses heuristic reasoning, how it represents
program knowledge, and what kinds of situations will challenge its
capabilities.</p>
<p>Fortunately, recent work in model interpretability and representation
engineering<span class="aside AT">AT: which work?</span> has produced
promising results which give hope towards understanding more and more of
the internal thought processes of LLMs. Here at <span
class="math inline">\(d_{model}\)</span> , we can think of no better
place to apply these new techniques than program understanding, where
there are many abstract properties that can be symbolically determined.
The vast work done in programming language theory over the past hundred
years provides many tools for scaling an understanding of the internal
thought processes of language models as they write
code.<span class="aside AT">AT: for examples, see cite, cite</span></p>
<p>In that spirit, we wanted to start with a simple property that comes
up in every programming languages, nullability. Nullable values are
represented differently across languages, null pointers in C++ or Java,
with explicit Option types in Rust, and with special nil or None values
in dynamic languages like Javascript, Lisp, or Python. In every case,
understanding where values can be nullable is necessary for even their
most basic uses, and misunderstanding where they are nullable can often
be a source of bugs, like a null pointer dereference.</p>
<p>Do our models understand when a value is nullable? They must, to be
able to write code that deals with nullable values, but we haven’t known
what form this knowledge takes, what situations are likely to confuse
the model. Until now.<span class=aside>big claim!</span></p>
<p>In this work, we contribute:</p>
<ul>
<li><p>A microbenchmark of 15 programs that test basic model
understanding of the flow of nullability through a program (sec. <a
href="#sec:bench">3</a>).</p></li>
<li><p>We find that models develop an internal concept of nullability as
they scale up and are trained for longer. (sec. <a
href="#sec:probing">4</a>)</p></li>
<li><p>We find that models begin to understand nullability in a local
scope, satisfying many requirements of the python typechecker, before
they start to understand how nullability flows across the program.
(sec. <a href="#sec:bench">3</a>, sec. <a
href="#sec:results">4.4</a>)</p></li>
</ul>
<h1 data-number="2" id="overview"><span
class="header-section-number">2</span> Overview</h1>
<p>Understanding the flow of nullability across programs is an essential
part of writing most code, and misunderstandings are often a source of
bugs. For models to write code, they must learn to track nullability in
some form. In this work, we’ll explore ways to measure nullability
understanding in language models, and use that to show how the
understanding of nullability changes over various model parameters.</p>
<p>Lets say you’re writing a Python program with your LLM assistant.
You’ve reached some point where you need to do something with a variable
called <code>num</code>. Maybe you’re building a list of numbers called
<code>positive_nums</code>. How do you proceed?</p>
<p>The answer often depends on the context in which you’re working. If
<code>num</code> and <code>positive_nums</code> are the things in scope,
then you might guess that you should write the lines:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>And if <code>num</code> is always a concrete number, as its name
would suggest, then this is probably the correct code. But variable
names don’t alway convey everything important about them, and it might
be the case that <code>num</code> could be None. If so, you’ll instead
want to write:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>In this case, the way you want to use <code>num</code> depends on
whether it could be None or not. That is, whether <code>num</code> is
“nullable”. In Python that means having an Optional type
(<code>Optional[int]</code> rather than <code>int</code>).</p>
<p>Determining whether <code>num</code> is nullable in this context
amounts to <em>type inference</em>, and it can be quite complicated in
the worst case. Fortunately, in many cases it’s quite simple, involving
applying just a few rules. For instance, if <code>num</code> is the
parameter to a function you’re inside, and the function declares the
type of <code>num</code> in its parameter list, then you can determine
nullability from that type. So, if your context is:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: <span class="bu">int</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you don’t need to check for None, whereas if it’s:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you <em>do</em> need a None check.</p>
<p>You could instead just ask your LLM assistant to complete the line.
But how does your assistant know if <code>num</code> is nullable? Our
experiments show that LLMs learn to approximate the same typing rules,
by analyzing millions of programs.</p>
<p>If we ask an LLM early in it’s pre-training process to complete the
program above, it produces:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num.is_a():</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        ...</span></code></pre></div>
<p>This is correct Python syntax, but it only works if <code>num</code>
is an object with a <code>is_a()</code> method, instead of an optional
integer.</p>
<p>Train the LLM for a little longer, and it’ll produce:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        ...</span></code></pre></div>
<p>This is closer, in that its figured out that <code>num</code> is a
number instead of an object, but it still isn’t reading the function
type signature and realizing that <code>num</code> could be None. Keep
training it though, and eventually it will learn to insert the None test
depending on the type signature of the function.</p>
<p>This rule is pretty simple alone, so relatively small models can
learn it, relatively early in their pre-training process. Other, more
complicated rules can take a little longer to learn. For instance, if
your program is:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>then <code>num</code> is a non-nullable number, and you can complete
the condition with <code>&lt; 0</code>.</p>
<p>But if instead you’re dealing with</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>Then you’ll want a None check first.</p>
<p>This rule takes models a little longer to learn, but your
highly-trained LLM assistant should make quick work of it. Our
experiments show that as these rules get more and more complex, it takes
LLMs longer and longer to learn them, and it also takes LLMs of more and
more parameters to learn them at all.</p>
<p>We can measure whether LLMs understand these rules by just asking for
completions, what we call an “external” measurement of the models
understanding. But there are many places where variables appear where a
completion won’t tell you what type the model thinks the variable has.
We would still like to know whether the model thinks these variables are
nullable at those locations, so we can instead look for an “internal”
measurement of the models understanding.</p>
<p>We do so by looking at the activations of the model, meaning the
values of each perceptron in the hidden layers. Together, these values
give the entire internal state of the model at each token, and they can
tell us what the model is “thinking” when processing that token. With
the right tests, we can tell if the model is “thinking” that the current
token is an optional variable, or a non-optional variable.</p>
<p>By the end of this post, we’ll be able to build a probe that uses the
models activations to determine whether the model thinks a variable read
corresponds to a nullable variable, and show that internal knowledge
like so:</p>
<figure>
<img src="images/reading_diagram.svg"
alt="A diagram showing a simple program, and the probes nullability predictions for each variable load." />
<figcaption aria-hidden="true">A diagram showing a simple program, and
the probes nullability predictions for each variable load.</figcaption>
</figure>
<p>In Sec. <a href="#sec:bench">3</a> we’ll describe our external tests
of nullability understanding in more detail, and in Sec. <a
href="#sec:probing">4</a> we’ll describe measuring the models internal
states in detail. Finally, we’ll go over some related work in Sec. <a
href="#sec:related">5</a>.</p>
<h1 data-number="3" id="sec:bench"><span
class="header-section-number">3</span> Measuring Nullability
Understanding
Externally<span class="aside AT">AT: Externally = token-level?</span></h1>
<p>We begin by measuring model nullability understanding externally,
because it provides a “skyline” or upper-bound estimate on our ability
to extract internal concepts of nullability. To do this, we have the
model complete simple partial programs that require an understanding of
nullability. We refer to this suite of programs as
<code>NullabilityEval</code>. All of the tests in this benchmark suite
are composed of three functions or less, where the largest function is
seven lines long.</p>
<p>We measure the difficulty of these tests by measuring how models of
different sizes perform. We pay particular focus to the Pythia model
suite<span class="citation" data-cites="biderman23">(Biderman et al.
2023)</span>, as they have checkpoints available across training runs
and various scales. For measuring performance at larger sizes, we’ve
included Qwen2.5-Coder-32B<span class="aside AT">AT: cite</span>, Llama
3.1 405B Instruct<span class="aside AT">AT: cite</span>, and DeepSeek-V3
(671B)<span class="aside AT">AT: cite</span>.</p>
<p>Our first test is:</p>
<p><em>Test 1</em>:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span></code></pre></div>
<p>The program<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> is constructed such that there are a
very limited number of valid next lines in the program, and all of them
demonstrate some knowledge of the concept of nullability.<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>We find that Pythia models as small as 2.8b can successfully complete
this test, and that they learn to complete the test in the first third
of training. Larger Pythia models learn to complete this test earlier,
with Pythia 12b able to complete the test 20% of the way into training
and Pythia 2.8b able to complete it 28% of the way into training.</p>
<h2 data-number="3.1" id="understanding-typing-rules"><span
class="header-section-number">3.1</span> Understanding Typing Rules</h2>
<p>How deep is this understanding of nullability? We give a syntax and
semantics of a minimalist subset of python that captures nullability,
and study how well our models perform on each rule. The full syntax and
typing rules of our subset of Python are described in Appendix <a
href="#sec:commonrules">B.1</a>.</p>
<p>With these basic rules, we can construct basic program prefixes that
test the models understanding of nullability.</p>
<p>So, do Pythia models 1.4b and up understand the semantics of all of
the typing rules necessary for
this<span class="aside AT">AT: which</span> test, or are the confounded
by semiotic information like variable names, whitespace, statement
orderings, and constant values?</p>
<p>For example, while many of the Pythia models can complete:</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span></code></pre></div>
<p>with a proper None check, changing the variable names and the
constants into:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>foo <span class="op">=</span> [<span class="dv">60</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">33</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>bar: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> corejoice <span class="kw">in</span> foo:</span></code></pre></div>
<p>causes all the Pythia models to fail the test.
Fortunately,<span class="aside AT">AT: ideally this is made more concrete, for example by referencing concrete rules or having a plot or table or something</span>
many simpler typing rules do not exhibit such a strong reliance on
variable naming and constants; in this case, it’s the for loop that
causes the model to be confused with certain variable names. <a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a></p>
<h2 data-number="3.2" id="interprocedural-analysis"><span
class="header-section-number">3.2</span> Interprocedural Analysis</h2>
<p>We can challenge the model more, adding layers of indirection between
the source and sink of nullable values, and testing the model’s
<em>interprocedural</em> understanding. Here’s one such test:</p>
<p><em>Test 2</em></p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(positive_numbers(some_numbers))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positive_numbers(numbers: <span class="bu">list</span>[Optional[<span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> numbers:</span></code></pre></div>
<p>In this test, we’ve taken the same logic and spread it across
<code>main</code> and another function, <code>positive_numbers</code>.
Ideally, the model would have to think a little harder to understand
that the <code>some_numbers</code> is flowing through the function call
into the body of <code>positive_numbers</code>, causing the for loop
body to need a <code>None</code> check. In practice though, we find this
test is actually <em>easier</em> for the model to pass, with models as
small as Pythia 1b passing it, and Pythia 12b learning to pass it 13% of
the way into training.</p>
<p>Because of the type annotations on the <code>positive_numbers</code>
function, the model doesn’t need to pay attention to <code>main</code>
at all. It can just look at <code>positive_numbers</code>, and use the
type annotation to know that result contains
<code>Optional[int]</code>s, so that since <code>num</code> is Nullable,
it must be checked for None before proceeding. Looking at the type
annotation turns out to be easier for the model than scanning through a
list to determine if there are None and non-None values, resulting in an
easier test overall.</p>
<p>So how would we <em>actually</em> test for interprocedural
nullability understanding in the model? Well, the type annotations on
Python functions aren’t required<a href="#fn4" class="footnote-ref"
id="fnref4" role="doc-noteref"><sup>4</sup></a>, so we can instead
provide the model with an unannotated function, and see if it still
understands the flow of values from the call site into the function
body. Here’s a test that does that:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>Our base set of typing rules (listed as “Common Rules”) don’t handle
unannotated functions though, so we’re going to have to add some more,
and here we’re faced with a choice. The typing rules for normal Python
say that functions without return type annotations return the Any type,
and arguments without a type annotation have the type Any. In fact,
normal mypy will not check unannotated functions at <em>all</em>, even
for internal consistency; the <code>--check-untyped-defs</code> option
will add some checking back, but the types of arguments and return type
will still be Any. In Python, a value of any type can be converted to an
Any, and an Any can be converted to any value type.</p>
<p>This means that it would be technically type safe to do anything in
the body of <code>process_value</code>, including just returning the
argument, without a static type error. But at runtime, code that
exploits this fact would still fail.</p>
<p>If we want code that actually makes sense at runtime, we can
strengthen our type checker a bit, by requiring that there be some valid
(non-Any) type for the function that works at the call site and in the
function body. We still won’t be requiring that this type is actually
written down anywhere, but we will be requiring that it exist. This is
called “type inference”, when we let the checker pick a type for us, as
long as there is one that works. We’ll call this augmented type system
mypy++.<span class="aside AT">AT: audience should know what type inference is</span></p>
<p>In Appendix <a href="#sec:unannotatedfuncs">B.2</a>, we formalize the
unannotated function rules for mypy vs mypy++.</p>
<p>This test is a bit trickier than our previous ones, and we find that
there’s no consistent threshold of size at which Pythia models can pass
it. Pythia 1b, 2.8b, and 6.9b pass the test in their final revisions,
but Pythia 410m, 1.4b, and 12b don’t. The bigger models all have points
in training where they can pass the test, but only intermittently. Even
6.9b, the best performing size on this test, fails the test in its
second-to-last available revision<a href="#fn5" class="footnote-ref"
id="fnref5" role="doc-noteref"><sup>5</sup></a>.
<span class="aside AT">AT: should we say something about
12b not solving it?</span></p>
<p>What the models <em>can</em> do well, however, is learn to pass these
tests in the mypy type system (as opposed to mypy++). In that system,
where they don’t need to reason globally about the functions but only
locally, this test is one of the easiest for the models to complete.
<span class="aside AT">AT: perhaps somewhere we should discuss implications for RL</span></p>
<p>Since this test suite is meant to be informative beyond the sizes of
the Pythia models, we also add another layer of indirection to add more
difficulty, in this test:</p>
<p><em>Test 4</em></p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> guard:</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>With two layers of indirection, we start to hit the limits of the
capabilities of even frontier models. Llama 405B is unable to
successfully pass this test, as are smaller models like Qwen Coder 32B,
while DeepSeek V3 (671B parameters) is able to pass it. However, Pythia
6.9B is still able to pass this test pretty consistently.</p>
<h2 data-number="3.3" id="generating-type-annotations"><span
class="header-section-number">3.3</span> Generating Type
Annotations</h2>
<p>Finally, we can test how good the models are at writing their own
type annotations for functions. Since most of the publicly available
Python code is not type-annotated, you could imagine that LLM’s can
reason about dataflow correctness without annotations better than they
can write their own typing annotations. The next program tests the
models ability to write its own type annotations; the trailling colon
makes the type expression the only valid completion<a href="#fn6"
class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p><em>Test 5</em>:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  square <span class="op">=</span> get_square(number)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number:</span></code></pre></div>
<p>None of the Pythia models are able to succesfully pass this test,
demonstrating that writing these annotaions is indeed difficult for
LLM’s.<span class="aside AT">AT: maybe too strong. "indeed more difficult than"?</span>
Qwen Coder 32B is also incapable of passing this test, but both Llama
405B and DeepSeek V3 pass it.</p>
<h2 data-number="3.4"
id="external-test-results-across-training-and-scale"><span
class="header-section-number">3.4</span> External Test Results Across
Training and Scale</h2>
<p>We wrote three variations of each of these tests, resulting in 15
tests total. Below, you can see the number of passing tests for each
model.</p>
<span class=aside>more through exploration of why?</span>
<figure id="fig:hl_scale">
<img src="images/hl_model_results.svg"
alt="Figure 1: A bar graph showing how several sizes of model perform on the high-level nullability tests" />
<figcaption aria-hidden="true">Figure 1: A bar graph showing how several
sizes of model perform on the high-level nullability tests</figcaption>
</figure>
<p>In Fig. <a href="#fig:hl_scale">1</a>, we can see the number of
passing tests for each model. We can see that generally models get
better with scale.
<span class=aside>I don't think this footnote is clear enough to add anything</span>
<a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> Model performance on these tests is
approximately logarithmic in model size: models of 2.8 billion
parameters can pass about half the tests, but it takes more than 405
billion parameters to pass all of the tests. This matches previous post-
and pre-training evaluations of the capabilities of large language
models<span class=aside>cite, Kaplan et al, Chincilla</span>, indicating
that these tests are well distributed.</p>
<figure id="fig:hl_mypy">
<img src="images/hl_mypy_vs_grep_models.svg"
alt="Figure 2: A bar graph showing how the Pythia models perform in mypy vs mypy++" />
<figcaption aria-hidden="true">Figure 2: A bar graph showing how the
Pythia models perform in mypy vs mypy++</figcaption>
</figure>
<p>In Fig. <a href="#fig:hl_mypy">2</a>, we can see the test result for
the pythia models using the mypy and mypy++ type systems (Fig. <a
href="#fig:hl_scale">1</a> uses mypy++). As we expected, the mypy
results (red bar) are always above the mypy++ results (blue bar), as
mypy++ is a stricter type system. There are six tests in the dataset
involving non-annotated functions, and using the weaker mypy typesystem
causes up to five more tests to pass than using mypy++<a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p>Next, we want to understand the training dynamics at play here. Below
we can see how Pythia 6.9b performs on the tests during training from
step 2 to
143000:<span class=aside>let's use the same axes scaling at the tigges
paper</span></p>
<figure id="fig:hl_time">
<img src="images/hl_revision_results.svg"
alt="Figure 3: A line graph showing how the performance of the Pythia 6.9 model changes during training" />
<figcaption aria-hidden="true">Figure 3: A line graph showing how the
performance of the Pythia 6.9 model changes during training</figcaption>
</figure>
<p>Again, performance does not increase monotonically. This plot is
quite noisy, so in the sequel, we will show smoothed<a href="#fn9"
class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>
charts.</p>
<p>In the graph below, we see that each individual model also learns to
write code which typechecks under mypy before it learns to write code
which typechecks under mypy++ and throws no type errors at runtime.</p>
<figure id="fig:hl_moral">
<img src="images/hl_mypy_vs_grep_revisions.svg"
alt="Figure 4: A graph showing how often the Pythia 6.9b produces code that typechecks on the tests, vs produces code that shows true understanding." />
<figcaption aria-hidden="true">Figure 4: A graph showing how often the
Pythia 6.9b produces code that typechecks on the tests, vs produces code
that shows true understanding.</figcaption>
</figure>
<h1 data-number="4" id="sec:probing"><span
class="header-section-number">4</span> Measuring Nullability
Understanding Internally</h1>
<p>At this point, we’ve figured out how to roughly measure nullability
understanding in the output of various language models, but we still
don’t know what their internal representations might look like or when
they emerge. In this section, we detail how we train reading vectors
sec. <a href="#sec:extraction">4.3</a>, using prompts designed to make
the model think about the phenomena of interest sec. <a
href="#sec:prompts">4.2</a>. Finally sec. <a
href="#sec:results">4.4</a>, we validate that these probes improve their
understanding of nullability over the course of pretraining to the level
that we expect from the external, or token-level understanding evals we
describe in the previous section.</p>
<h2 data-number="4.1" id="background"><span
class="header-section-number">4.1</span> Background</h2>
<!--
As language models predict each token in a text, they run their tuned
circuits over all the previous text. Tokens are first embedded into a
high-dimensional "token space", and each layer of the transformer
model is made up of many parallel circuits which transform the
previous layers output into new embeddings in a new space. Each layer
can look not just at the output of the layer directly previous, but
actually all previous layers, through a channel called a residual
stream.

The paper presents two main approaches to interpreting a language
model; circuit-based, and representation-based. Circuit based
interpretability aims to pair down the network to a key set of
circuits which are sufficient to complete a particular task; this
allows practitioners to point to a particular part of the model and
say "this is where \<task\> is done", much like neuroscientists assign
functionality to different parts of our brain.

-->
<p>In this section, we review representation engineering <span
class="citation" data-cites="zou25">(Zou et al. 2025)</span> techniques
that we will use to look for linear representations of nullability
inside the model.</p>
<p><span class="citation" data-cites="zou25">Zou et al. (2025)</span>
shows how representations can be extracted for concepts like
“happiness”, “honesty”, and “fairness”. First, they construct many
prompts which cause the model to act in a way aligned with the concept,
and many which cause the model to act in a way aligned against the
concept. For instance, they might prompt the model with “Pretend you’re
a dishonest person. The Eiffel Tower is” and “Pretend you’re an honest
person. The Eiffel Tower is”. Then, they take the internal activations
which correspond to each, and try to extract a high-dimensional vector
which points towards the states which are aligned, and away from the
states that are not aligned. This vector can then be used as a linear
model to measure how much of the concept is activated in the model
during any given forward pass (e.g. for honesty, this gives us a
lie-detector).</p>
<figure>
<img src="images/zhou.png"
alt="Figure from Zou et al. (2025) showing the reading outputs for several concepts" />
<figcaption aria-hidden="true">Figure from <span class="citation"
data-cites="zou25">Zou et al. (2025)</span> showing the reading outputs
for several concepts</figcaption>
</figure>
<h2 data-number="4.2" id="sec:prompts"><span
class="header-section-number">4.2</span> Designing Prompts to Extract
Nullability Activations</h2>
<p>We avoid dealing with the ambiguities of natural language by working
in a setting where the model needs only to complete code. analyze the
nullability of individual variable occurrences. Specifically, we probe
for “the variable I just generated refers to an nullable quantity”, so
our prompts looked like:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_1() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> find_value(data: List[<span class="bu">int</span>], target: <span class="bu">int</span>) <span class="op">-&gt;</span> Optional[<span class="bu">int</span>]:</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> value <span class="kw">in</span> data:</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> value <span class="op">==</span> target:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> value</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  target <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  result <span class="op">=</span> find_value(data, target)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> result</span></code></pre></div>
<p>We queried o1, o1-mini, deepseek-coder, and claude-3.5-sonnet with
the following prompt:</p>
<blockquote>
<p>Generate me 100 typed python programs that use the Optional type,
along with type annotations for any undefined functions that they use.
Make sure that the programs are unique, and each involves at least eight
lines of logic. Number each program from 1 to 100. Please put all the
programs in a single file, with a main function that tests each. Don’t
include any text before or after the code, just the code. I will be
using mypy with the –strict option to check the code.</p>
</blockquote>
<p>We label each variable read occurrence with nullability information
derived from mypy, and prompt the “model under test” with a prompt
consisting of the tokens up to and including the variable read
occurrence.</p>
<h2 data-number="4.3" id="sec:extraction"><span
class="header-section-number">4.3</span> Extracting Reading Vectors</h2>
<span class="aside AT">AT: Overall, I'm not really sure what our takeaway is for this section. I think we want the reader to understand that we did some experiements with mass mean probing and the various normalization mehtods, but I'm not sure we're making any compelling point here beyond "we ran the experiment". I think we can appendicize the latter two plots --- they look pretty noisy, and I'm not sure they're acutally something to draw conclusions from</span>
<p>We use Mass Mean Shift probing which has been shown empirically <span
class="citation" data-cites="li24">(Li et al. 2024)</span> to generalize
better in high dimensional spaces than logistic regression<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a>.</p>
<p>Prior work focused their probing on a single layer, often handpicked
based on prior papers. In our experiments, we decided to probe
<em>all</em> layers using a mass means probe, and learn which ones were
most important from the data. We tested two methods for doing so -
either allowing the magnitude of the difference of means vector to
determine the importance of the layer in the final probe, or to learn
coefficients for each layer using linear regression. We found that which
method is more accurate on test data varies over both model size and
number of training steps.</p>
<!-- In the reading vector, the impact of each layer is based on the
magnitude of mean difference in that layer.
w we decided to try a few different methods of normalizing
these layers to improve their overall accuracy as a linear model:

The reading vectors from each layer are the mean difference in activations
between when the feature (nullability) is present or not. This leaves open
the question of how to weight the reading vectors from different layers.
We evaluate the follow cross-layer normalization schemes:

* No normalization; just summing over the average differences
* Normalizing each layers vector to a uniform length
* Dividing by the average amount that each layer activates on the positive samples
* Dividing by the average absolute amount that each layer activates on the positive samples
* Dividing by the square root of the average of squares of layer activations
\AT{ add some theory for this}
\AT{ include a results table }
\AT{ add equations. these words are so hard to read.}
We find that "no normalization" performs the best.

\todo{ this is ... sort of... a contribution?}

-->
<figure id="fig:mm-vs-mmlr-sizes">
<img src="images/mm-vs-mmlr.svg"
alt="Figure 5: The performance of pure mass means probing vs mass means probing with linear regression for different Pythia model sizes. Binary-cross entropy is plotted, so lower is better." />
<figcaption aria-hidden="true">Figure 5: The performance of pure mass
means probing vs mass means probing with linear regression for different
Pythia model sizes. Binary-cross entropy is plotted, so lower is
better.</figcaption>
</figure>
<p>In Fig. <a href="#fig:mm-vs-mmlr-sizes">5</a>, we can see that pure
mass means probing gives lower loss for smaller models (those with less
than 410 million parameters), but that for larger models weighting
layers using linear regression gives lower loss consistently.
<span class="aside AT">AT: not really sure these plots are saying what we're saying they're saying</span></p>
<!--![The performance of the two probing methods on the Pythia 160m model
 for different numbers of pretraining steps. There are regions where
 pure mass means scaling is better, and regions where linear
 regression on layer weights is better.](images/mm-vs-mmlr-160m.svg){#fig:mm-vs-mmlr-160m}

In [@Fig:mm-vs-mmlr-160m;@Fig:mm-vs-mmlr-410], we look at how these
scaling methods perform for different amounts of pretraining, for the
model sizes nearest the boundary. We see that relative merit of each
scaling method can vary significantly over pretraining steps.

![The performance of the two probing methods on the Pythia 410m model
for different numbers of pretraining steps. Pure mass means probing
starts better, but is quickly overtaken by mass means probing with
linear regression on layer weights.](images/mm-vs-mmlr-410m.svg){#fig:mm-vs-mmlr-410}-->
<h2 data-number="4.4" id="sec:results"><span
class="header-section-number">4.4</span> Probing Results Across Training
and Scale</h2>
<p>In this section, we study the performance of our nullability probes
across time and scale <span class="citation"
data-cites="tigges24">(Tigges et al. 2024)</span>. We use mass-means
shift probing <span class="citation" data-cites="li24">(Li et al.
2024)</span> on all layers, and a linear regression to determine the
weights of each layer.<a href="#fn11" class="footnote-ref" id="fnref11"
role="doc-noteref"><sup>11</sup></a>
<span class="aside AT">AT: see appendix D for mm vs LR</span></p>
<figure id="fig:models-and-steps">
<img src="images/accuracy_during_pretraining.svg"
alt="Figure 6: The performance, measured in binary cross-entropy, of each Pythia model size during pretraining. Since this graph is of loss, lower is better" />
<figcaption aria-hidden="true">Figure 6: The performance, measured in
binary cross-entropy, of each Pythia model size during pretraining.
Since this graph is of loss, lower is better</figcaption>
</figure>
<p>In Fig. <a href="#fig:models-and-steps">6</a>, we plot loss against
scale and time. While we measured accuracy for every available Pythia
model size, we exclude the smallest (14m) from this plot since it would
exist entirely above the top of the plot. We notice turning points in
the models’ understanding of nullability at 1b parameters and
<span class=aside>number of tokens</span>. We explore the texture of
these in <span class=aside>future work</span>.</p>
<h2 data-number="4.5" id="visualizing-probe-outputs"><span
class="header-section-number">4.5</span> Visualizing Probe Outputs</h2>
<p>Now that we’ve shown how to train these probes with increasing
accuracy, lets bring back the reading diagram we showed in the intro,
and go a bit more into depth with it.</p>
<p>We adapted this style of reading diagram from <span class="citation"
data-cites="zou25">Zou et al. (2025)</span>, but only show the
activations on tokens that represent variable loads, since that is where
we trained our probe<a href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a>. For each measured token, we’re
running the model until just after that token, then extracting it’s
hidden state and measuring probe activation. We then color the box above
that token depending on the activations, and the scoring threshold
inferred at train-time<a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a>.</p>
<figure>
<img src="images/reading_diagram.svg"
alt="A diagram showing a simple program, and the probes nullability predictions for each variable load." />
<figcaption aria-hidden="true">A diagram showing a simple program, and
the probes nullability predictions for each variable load.</figcaption>
</figure>
<p>We can see here sixteen tokens that correspond to variable reads in
the program, and all but one are probed as non-optional (correctly). The
only nullable variable in this program is <code>result</code>, since it
comes from <code>find_value</code> which returns
<code>Optional[int]</code>. So when this variable appears for the first
time in the <code>if</code> statement checking if it’s none, the model
knows it is nullable, and the results of the probe reflect that
understanding. But when it appears a second time on the next line, in
the format string of <code>print</code>, the model no longer thinks it
is optional, since the body of this if statement only runs if it is
<em>not</em> <code>None</code>; the probe accurately reflects this.</p>
<h1 data-number="5" id="sec:related"><span
class="header-section-number">5</span> Related Work</h1>
<p>Our decision to use Pythia to study feature evolution across time and
scale is inspired by <span class="citation" data-cites="tigges24">Tigges
et al. (2024)</span> . They focus on classic circuits-centered
interpretability tasks such as IOI <span class="citation"
data-cites="wang22">(Wang et al. 2022)</span>, Gendered-Pronoun <span
class="citation" data-cites="mathwin">(Mathwin et al., n.d.)</span>,
Greater-Than<span class="citation" data-cites="hanna23">(Hanna, Liu, and
Variengien 2023)</span> , and SVA <span class="citation"
data-cites="linzen16">(Linzen, Dupoux, and Goldberg 2016)</span>.</p>
<p>In our setting, we are more interested in how activations vary across
inputs, to extract representations of nullability. <span
class="citation" data-cites="zou25">Zou et al. (2025)</span> surveys
techniques for representation engineering with linear probes. We apply
similar techniques, but to program semantics and dataflow instead of
natural language.</p>
<p><span class="citation" data-cites="feng24predicate">Feng, Russell,
and Steinhardt (2024)</span> also study LLM’s ability to reason about
propositions, but in a natural language setting, rather than a formal
one.</p>
<p>Several techniques exist for constructing linear probes, but after
experimental measurement we followed the mass means shift from <span
class="citation" data-cites="li24">Li et al. (2024)</span>. <span
class="citation" data-cites="li24">Li et al. (2024)</span> and <span
class="citation" data-cites="zhong23">Zhong et al. (2023)</span> discuss
why mass mean probing might outperform linear regression.</p>
<h1 data-number="6" id="future-work"><span
class="header-section-number">6</span> Future Work</h1>
<span class="aside AT">AT: Typing rules staircase plot</span>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-biderman23" class="csl-entry" role="listitem">
Biderman, Stella, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,
Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, et al. 2023.
<span>“Pythia: <span>A Suite</span> for <span>Analyzing Large Language
Models Across Training</span> and <span>Scaling</span>.”</span> arXiv.
<a
href="https://doi.org/10.48550/arXiv.2304.01373">https://doi.org/10.48550/arXiv.2304.01373</a>.
</div>
<div id="ref-feng24predicate" class="csl-entry" role="listitem">
Feng, Jiahai, Stuart Russell, and Jacob Steinhardt. 2024.
<span>“Monitoring <span>Latent World States</span> in <span>Language
Models</span> with <span>Propositional Probes</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2406.19501">https://doi.org/10.48550/arXiv.2406.19501</a>.
</div>
<div id="ref-hanna23" class="csl-entry" role="listitem">
Hanna, Michael, Ollie Liu, and Alexandre Variengien. 2023. <span>“How
Does <span>GPT-2</span> Compute Greater-Than?: <span>Interpreting</span>
Mathematical Abilities in a Pre-Trained Language Model.”</span> arXiv.
<a
href="https://doi.org/10.48550/arXiv.2305.00586">https://doi.org/10.48550/arXiv.2305.00586</a>.
</div>
<div id="ref-li24" class="csl-entry" role="listitem">
Li, Kenneth, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin
Wattenberg. 2024. <span>“Inference-<span>Time Intervention</span>:
<span>Eliciting Truthful Answers</span> from a <span>Language
Model</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2306.03341">https://doi.org/10.48550/arXiv.2306.03341</a>.
</div>
<div id="ref-linzen16" class="csl-entry" role="listitem">
Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. <span>“Assessing
the <span>Ability</span> of <span>LSTMs</span> to <span>Learn
Syntax-Sensitive Dependencies</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.1611.01368">https://doi.org/10.48550/arXiv.1611.01368</a>.
</div>
<div id="ref-marks24" class="csl-entry" role="listitem">
Marks, Samuel, and Max Tegmark. 2024. <span>“The Geometry of Truth:
Emergent Linear Structure in Large Language Model Representations of
True/False Datasets.”</span> <a
href="https://arxiv.org/abs/2310.06824">https://arxiv.org/abs/2310.06824</a>.
</div>
<div id="ref-mathwin" class="csl-entry" role="listitem">
Mathwin, Chris, Guillaume Corlouer, Esben Kran, Fazl Barez, and Neel
Nanda. n.d. <span>“Identifying a <span>Preliminary Circuit</span> for
<span>Predicting Gendered Pronouns</span> in <span>GPT-2
Small</span>.”</span>
</div>
<div id="ref-tigges24" class="csl-entry" role="listitem">
Tigges, Curt, Michael Hanna, Qinan Yu, and Stella Biderman. 2024.
<span>“<span>LLM Circuit Analyses Are Consistent Across Training</span>
and <span>Scale</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2407.10827">https://doi.org/10.48550/arXiv.2407.10827</a>.
</div>
<div id="ref-wang22" class="csl-entry" role="listitem">
Wang, Kevin, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and
Jacob Steinhardt. 2022. <span>“Interpretability in the
<span>Wild</span>: A <span>Circuit</span> for <span>Indirect Object
Identification</span> in <span>GPT-2</span> Small.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2211.00593">https://doi.org/10.48550/arXiv.2211.00593</a>.
</div>
<div id="ref-zhong23" class="csl-entry" role="listitem">
Zhong, Ziqian, Ziming Liu, Max Tegmark, and Jacob Andreas. 2023.
<span>“The <span>Clock</span> and the <span>Pizza</span>: <span>Two
Stories</span> in <span>Mechanistic Explanation</span> of <span>Neural
Networks</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2306.17844">https://doi.org/10.48550/arXiv.2306.17844</a>.
</div>
<div id="ref-zou25" class="csl-entry" role="listitem">
Zou, Andy, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
Ren, Alexander Pan, et al. 2025. <span>“Representation
<span>Engineering</span>: <span>A Top-Down Approach</span> to <span>AI
Transparency</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2310.01405">https://doi.org/10.48550/arXiv.2310.01405</a>.
</div>
</div>
<h1 class="unnumbered" id="appendicies">Appendicies</h1>
<h1 class="unnumbered" id="nullability-in-python">A Nullability In
Python</h1>
<p>To see how nullability appears in Python, Let’s look at an example
program in Python that uses nullability:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> User:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, email: Optional[<span class="bu">str</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>         <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.email <span class="op">=</span> email</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_user_email(user: Optional[User]) <span class="op">-&gt;</span> Optional[<span class="bu">str</span>]:</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> user <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> user.email <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> user.email</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="va">None</span></span></code></pre></div>
<p>In the <code>get_user_email</code> function above, we can see from
the type signature that it takes a nullable User value, and returns an
nullable string. The first thing the function does is check if the input
<code>user</code> is None or not. This program was actually generated by
o1-mini, so we can already see that the model understands that the user
object is nullable, and that it needs to be checked for None-ness before
anything else can be done.</p>
<p>We can say that there are five variable “occurances” in the program,
each of which can be either nullable or not. There’s the first
<code>user</code> in the if statement, the second <code>user</code> to
the right of the <code>and</code> in <code>user.email</code>, there’s
the <code>user.email</code> itself, the <code>user</code> on the second
line, and finally the <code>user.email</code> on the second line. If we
use Python’s typechecker, mypy, to check the types of each of these
occurrences, we find that they have type <code>Optional[User]</code>,
<code>User</code>, <code>Optional[str]</code>, <code>User</code>, and
<code>str</code> respectively. That is, the first and third are
nullable, and the rest are not.</p>
<h1 class="unnumbered" id="sec:formalrules">B Python Subset Syntax and
Typing Rules</h1>
<p>Here we define the syntax and semantics of the Python subset we work
with:</p>
<h2 class="unnumbered" id="sec:commonrules">B.1 Common Rules</h2>
<p>But before we try to measure nullability understanding, we’ll want to
be more precise about exactly what we’re measuring. To that end, we’ll
take the notion of different “rules” for nullability that we discuseed
informally in the overview, and bring it into a formal typing system.
We’re not going to try to describe all the typing rules of python, so
we’ll restrict ourselves in a couple of ways.</p>
<p>First, we’ll reduce the number of python features that we handle by
actually working in a subset of python. This means we can skip worrying
about the semantics of complicated features, and just focus on the
features necessary for understanding optionality in a semi-realistic
setting.</p>
<p>Second, we’ll define all non-None values as converting to the boolean
value True, instead of numbers converting to False when they are zero
and strings converting to False when they are empty. This is a necessary
practicality, because otherwise, the model could circumvent our type
tests by doing bare ifs which work on both optional and non-optional
types. But to prevent bare ifs from ever being the correct completion
for a non-optional type, we’ll design our tests so that there are never
any values that would convert to False, namely the number zero and the
empty string.</p>
<p><span class="math display">\[\begin{aligned}
\left[Program\right]  =&amp; [\epsilon] {\huge|}
\left[\begin{array}{l}Stmt\\Program\end{array}\right]\\
[Stmt]       =&amp; [Import] | [FnDef] | [Assn] | [ForLoop] | [If] |
[Expr] \\
              &amp;| [\texttt{return }Expr]\\
[Import]     =&amp; [\texttt{from } Ident \texttt{ import } Ident]\\
[FnDef]      =&amp;
\left[\begin{array}{l}
\texttt{def $Ident$($DeclArgs$):}\\
\quad Program
\end{array}\right]\\
&amp;{\huge|}
\left[\begin{array}{l}
\texttt{def $Ident$($DeclArgs$) -&gt; $Type$}:\\
\quad Program
\end{array}\right]\\
[DeclArgs]   =&amp; [\epsilon] | [Ident] | [Ident : Type]\\
              &amp;| [Ident \texttt{,} DeclArgs]
| [Ident : Type\texttt{,} DeclArgs]\\
[Type]       =&amp; [\texttt{int}] | [\texttt{str}] | [\texttt{bool}] |
[\texttt{None}] \\
              &amp;| [\texttt{Optional[}Type\texttt{]}] |
[\texttt{List[}Type\texttt{]}]\\
[Assn]       =&amp; [Ident \texttt{ = } Expr]\\
[ForLoop]    =&amp;
\left[\begin{array}{l}
\texttt{for $ident$ in $ident$:}\\
\quad Program
\end{array}\right]\\
[If]         =&amp;
\left[\begin{array}{l}
\texttt{if $expr$:}\\
\quad Program
\end{array}\right]\\
[Expr]       =&amp; [Ident] | [Constant] | [Expr\texttt{ }Op\texttt{
}Expr]\\
              &amp;| [Ident \texttt{(} ParamsList \texttt{)}]|
[\texttt{$Expr$ if $Expr$ else $Expr$}]\\
              &amp;| [\texttt{[$ListConts$]}] | [\texttt{[]}]\\
[ParamsList] =&amp; [\epsilon] | [Expr] | [Expr\texttt{, }ParamsList]\\
[Op]         =&amp; [\texttt{+}] | [\texttt{-}] | [\texttt{*}] |
[\texttt{/}] | [\texttt{&lt;}] | [\texttt{&gt;}] | [\texttt{&lt;=}] |
[\texttt{&gt;=}] | [\texttt{is}] | [\texttt{is not}] | [\texttt{==}] |
[\texttt{!=}]\\
[ListConts]  =&amp; [Expr] | [Expr,ListConts]\\
\end{aligned}\]</span></p>
<p>We won’t worry too much about the semantics on a formal level, since
it’s the same as Python’s semantics on this subset, and we’ll mostly be
using it informally to justify typing rules here. But we do want to
formally define our typing rules, since we’ll be measuring the models
understanding of particular rules. We’ll be using two propositions for
typing in this language. There’s <span class="math inline">\(\Gamma
\vdash \left[p\right]
\vartriangleright \texttt{ok}\)</span>, which says that the program
<span class="math inline">\(p\)</span> is well typed in environment
<span class="math inline">\(\Gamma\)</span>. And there’s <span
class="math inline">\(\Gamma \vdash e : t\)</span>, which says that
<span class="math inline">\(e\)</span> is well typed with type <span
class="math inline">\(t\)</span> in environment <span
class="math inline">\(\Gamma\)</span>. When we exclude the <span
class="math inline">\(\Gamma\)</span>, we mean that the judgement is
true under any typing environment, including the empty one.</p>
<p><span class="math display">\[
\tag{Const}
\frac{Constant\neq\texttt{None}}{\vdash Constant: \text{Atom}}\\
\hspace{1.0cm}
\frac{}{\forall t, \vdash \texttt{None}: \texttt{Optional[$t$]}}\\
\]</span></p>
<p><span class="math display">\[
\tag{List}
\frac{\Gamma \vdash x_1 : t, x_2 : t, ...}
     {\Gamma \vdash \texttt{[$x_1$, $x_2$, ...]} : \texttt{List[$t$]}}
\hspace{1.0cm}
\frac{}{\forall t, \vdash \texttt{[]}: \texttt{List[$t$]}}
\]</span></p>
<p><span class="math display">\[
\tag{Weaken}
\frac{\Gamma \vdash x: t}{\Gamma \vdash x: \texttt{Optional[$t$]}}
\]</span></p>
<p><span class="math display">\[
\tag{Var}
\Gamma \vdash e: t \hspace{1cm} \Gamma, x: t \vdash [p]
\vartriangleright \text{ok}
\over
\Gamma \vdash \left[\begin{array}{l}\texttt{$x$ = $e$}\\
p\end{array}\right] \vartriangleright \text{ok}
\]</span></p>
<p><span class="math display">\[\begin{gather}
\Gamma, x_1: t_1, x_2: t_2, ... \vdash [b] \vartriangleright \text{ok}\\
\tag{Def}
\Gamma, f : t_1 \rightarrow t_2 ... \rightarrow t_r \vdash [p]
\vartriangleright \text{ok} \hspace{1cm} \Gamma \vdash
\text{Returns($b$, $t_r$)}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$: $t_1$, $x_2$: $t_2$, ...) -&gt; $t_r$:}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}\]</span></p>
<p><span class="math display">\[
\tag{App}
\frac{\Gamma \vdash f : t_1 \rightarrow t_2 ... \rightarrow t_r
\hspace{1cm} \Gamma \vdash x_1 : t_1, x_2 : t_2, ...}
     {\Gamma \vdash f(x_1, x_2, ...) : t_r}
\]</span></p>
<p><span class="math display">\[
\hspace{-1cm}{
\tag{IfIn}
\Gamma \vdash x : \texttt{Optional[$t$]} \hspace{1cm}
\Gamma, x : t \vdash [p] \vartriangleright \text{ok} \hspace{1cm}
\bigotimes \text{in} [\texttt{is not, !=}]
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $x$:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
\hspace{1cm}
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $x$ $\bigotimes$ None:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
}
\]</span></p>
<p><span class="math display">\[
\tag{IfOut}
\Gamma \vdash
\left[\begin{array}{l}
p_1\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\hspace{1cm}
\Gamma \vdash
\left[\begin{array}{l}
p_2\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $e$:}\\
\quad p_1\\
\texttt{else:}\\
\quad p_2\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\]</span></p>
<p><span class="math display">\[
\tag{IfExpr}
\frac{\Gamma \vdash (e_b : \text{Optional[$t_0$]} \lor e_b :
\texttt{bool}), e_1 : t, e_2 : t}
     {\Gamma \vdash \texttt{$e_1$ if $e_b$ else $e_2$} : t}
\]</span></p>
<p><span class="math display">\[
\tag{For}
\Gamma \vdash y\texttt{ : List } t \hspace{1cm} \Gamma, x : t \vdash [p]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{for $x$ in  $y$:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
\]</span></p>
<p><span class="math display">\[
\tag{OpInt}
\Gamma \vdash x_1 : \texttt{int}, x_2 : \texttt{int}, \bigotimes \text{
in [\texttt{+, -, *, /}]}
\over
\Gamma \vdash x_1 \bigotimes x_2 : \texttt{int}
\]</span></p>
<p><span class="math display">\[
\hspace{-1.5cm}
\tag{OpString}
\frac{\Gamma \vdash s_1 : \texttt{str}, s_2 : \texttt{str}}
     {\Gamma \vdash s_1 + s_2 : \texttt{str}}
\hspace{1cm}
\frac{\Gamma \vdash s : \texttt{str}, x: \texttt{int}}
     {\Gamma \vdash s * x : \texttt{str}}
\]</span></p>
<p><span class="math display">\[
\hspace{-1.5cm}
\tag{OpEquality}
\frac{\Gamma \vdash x_1 : t, x_2 : t, \bigotimes \text{ in }
[\texttt{==, !=, is, is not}]}
     {\Gamma \vdash x_1 \bigotimes x_2 : \texttt{bool}}
\]</span></p>
<p><span class="math display">\[
\hspace{-2cm}
\tag{OpComparison}
\frac{\Gamma \vdash x_1 : \texttt{int}, x_2 : \texttt{int}, \bigotimes
\text{ in } [\texttt{&lt;, &gt;, &lt;=, &gt;=}]}
     {\Gamma \vdash x_1 \bigotimes x_2 : \texttt{bool}}
\]</span></p>
<p>With:</p>
<p><span class="math display">\[
\tag{ReturnReturn}
\frac{\Gamma \vdash e : t}{\Gamma \vdash \text{Returns(\texttt{return
$e$}, $t$)}}
\]</span> <span class="math display">\[
\tag{NoReturnExpression}
\frac{\Gamma \vdash e : t}{\vdash \text{NoReturn([$e$])}}
\]</span> <span class="math display">\[
\tag{NoReturnAssign}
\frac{}{\vdash \text{NoReturn([\texttt{$i$ = $e$}])}}
\]</span> <span class="math display">\[
\tag{ReturnIf}
\frac{\Gamma \vdash \text{Returns($p$, $t$)}}
     {\Gamma \vdash \text{Returns(}\left[\begin{array}{l}\texttt{if
$e$:}\\ \quad p\end{array}\right],t\text{)}}
\hspace{1cm}
\]</span></p>
<p><span class="math display">\[\begin{gather}
\Gamma \vdash \left(\text{Returns($p_1$, $t$)} \land
\text{Returns($p_2$, $t$)}\right)\lor \\
\Gamma \vdash \left(\text{Returns($p_1$, $t$)} \land
\text{NoReturns($p_2$)}\right) \lor\\
\tag{ReturnIfElse}
\Gamma \vdash \left(\text{NoReturns($p_1$)} \land \text{Returns($p_2$,
$t$)}\right)
\over
\Gamma \vdash \text{Returns(}\left[\begin{array}{l}
\texttt{if $e$:}\\
\quad p_1\\
\texttt{else:}\\
\quad p_2
\end{array}\right]\text{, $t$)}
\end{gather}\]</span></p>
<p><span class="math display">\[
\tag{ReturnFor}
\frac{\Gamma \vdash \text{Returns($p$, $t$)}}
     {\Gamma \vdash \text{Returns(}\left[\begin{array}{l}
     \texttt{for $x$ in $y$:}\\
     \quad p
     \end{array}\right]\text{, $t$)}}
\]</span></p>
<h2 class="unnumbered" id="sec:unannotatedfuncs">B.2 Unannotated
Functions</h2>
<p>The rules above are sufficient for our purposes for dealing with
fully type annotated programs. But what about programs with functions
that aren’t type annotated, or are only partially annotated? The mypy
type system that Python uses treats unannotated functions as dynamically
typed, allowing them to typecheck as any type. Similarly, function
parameters without a type annotation are implicitly convertable to any
type. So, for normaly mypy typechecking, we add the following typing
rules:</p>
<p><span class="math display">\[\begin{gather}
\Gamma, x_1 : Any, x_2 : Any, ... \vdash [b] \vartriangleright
\text{ok}\\
\tag{DynDef}
\Gamma, f : Any \rightarrow ... \rightarrow Any \vdash [p]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$, $x_2$, ...):}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}\]</span></p>
<p><span class="math display">\[
\tag{DynConvert}
\frac{\Gamma \vdash x : t}
     {\Gamma \vdash x : Any}
     \hspace{0.5cm}
\frac{\Gamma \vdash x : Any}
     {\Gamma \vdash x : t}
\]</span></p>
<p>These rules allow more python programs to check statically, but they
mean that some python programs will pass the static checks but still
throw type errors at runtime. Importantly for us, they can throw runtime
type errors about optionality. A well-written piece of code would not
just satisfy this type system then, but actually a stronger one that
would prevent runtime type errors. We’ll call this stronger type system
mypy++. Instead of the Dyn- rules, it has this one:</p>
<p><span class="math display">\[\begin{gather}
\Gamma, x_1 : t_1, x_2 : t_2, ... \vdash [b] \vartriangleright
\text{ok}\\
\tag{InferDef}
\Gamma, f : t_1 \rightarrow t_2 \rightarrow ... \rightarrow t_r \vdash
[p] \vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$, $x_2$, ...):}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}\]</span></p>
<h1 class="unnumbered"
id="detailed-high-level-nullability-test-results">C Detailed High-Level
Nullability Test Results</h1>
<h2 class="unnumbered" id="across-scale">C.1 Across Scale</h2>
<p>You can see in fig. <a href="#fig:hl_scale">1</a> that the smallest
three models don’t pass any test. After that, the trend line for tests
passed goes upwards, but it’s not monotonic: Pythia 410m passes only one
test, while Pythia 160m passes three, and Pythia 6.9b passes the most
tests, with 9, while Pythia 12b only passes 7. For instance, given the
partial code:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>Pythia 6.9b completes <code>process_value</code> as:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="bu">isinstance</span>(value, <span class="bu">str</span>):</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(value)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  ...</span></code></pre></div>
<p>While Pythia 12b completes it as:</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> value</span></code></pre></div>
<p>The code generated by Pythia 6.9b properly handles values of both
NoneType and str and always returns an int, whereas the code generated
by Pythia 12b is the identify function, returning None when the input is
None and returning a string when the input is a string. Neither of these
cases is correct, as the main function expects this to return a Number
that can be added to one.</p>
<h2 class="unnumbered" id="across-time">C.2 Across Time</h2>
<p>We say in fig. <a href="#fig:hl_time">3</a> that while the model
generally gets better at passing these tests during training, the
performance is not always increasing. Zooming in on a particular test,
we can see what this looks like. When asked to complete this code:</p>
<p><em>Test 12</em></p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> guard:</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_os(<span class="st">&quot;Foobar&quot;</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_os(value) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>))</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span></code></pre></div>
<p>Pythia 6.9b at training step 104000 produces the following definition
of <code>count_os</code>:</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="bu">isinstance</span>(value, <span class="bu">str</span>):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(value)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>...</span></code></pre></div>
<p>While 1000 steps later, it produces a very different definition:</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">len</span>([s <span class="cf">for</span> s <span class="kw">in</span> os.listdir(os.path.join(os.getcwd(), value)) <span class="cf">if</span> s.start ...</span></code></pre></div>
<p>While the first definition handles value being None or a string, the
second definition not only assumes it is a string but that it is a name
of a folder in the current directory. In some sense the model “knows”
that the value parameter is nullable at step 104000, but “forgets” it
during further training. It regains the ability to pass the test at
several points during training, but by the end of training, it’s back to
treating <code>value</code> as if it was always a string.</p>
<h2 class="unnumbered" id="common-mistakes">C.3 Common Mistakes</h2>
<p>As expected, type annotations are particularly difficult for these
models. When asked to complete the type of get_square in the following
code, no model in the Pythia series can successfully output the Optional
argument type:</p>
<p><em>Test 5</em></p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>  number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>  square <span class="op">=</span> get_square(number)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number:</span></code></pre></div>
<p>These models also find it much easier to deal with a list that
contains some None elements, than to deal with an atomic value that
might be None. Pythia 6.9b consistently passes this test in the second
half of training:</p>
<p><em>Test 2</em></p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>  some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(positive_numbers(some_numbers))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positive_numbers(numbers: <span class="bu">list</span>[Optional[<span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>  result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> num <span class="kw">in</span> numbers:</span></code></pre></div>
<p>But is much more challenged by this code:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>, intermittently being unable to complete it in a type-safe way up,
including in the second-to-last training step. When another level of
function indirection is added, the model becomes much better at
completing it correctly; Pythia 6.9b consistently completes the
following after step 93000:</p>
<p><em>Test 4</em></p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> guard:</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>The names of functions are highly influential in the models
understanding of their type semantics. When test 3 is changed so that
<code>process_value</code> is instead called <code>count_chars</code>,
Pythia 6.9b becomes unable to correctly complete it on any revision; it
likely has a bias from its training data that functions called
<code>count_chars</code> always take non-nullable strings. Similarly,
when test 4 is changed so that process_values becomes string_complexity,
it goes from consistently passing to almost always failing.</p>
<h1 class="unnumbered" id="mass-mean-probing-vs-linear-regression">D
Mass Mean Probing vs Linear Regression</h1>
<p>We were initially very surprised to find that mass means probing
would perform better than linear regression. After all, linear
regression is a much more powerful technique for fitting data. And mass
means probing can be seen as giving the direction of best fit in each
dimension independently, without considering other dimensions. The more
dimensions you consider at one time, the better your model fit can be.
But repeatedly in our data, we found that mass means probing
outperformed linear regression on the test data.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p> This partial program is only four lines, with type
annotations. A <code>some_numbers</code> array is created that includes
positive numbers, negative numbers, and None values, giving it type
<code>Optional[int]</code>. A list <code>result</code> is constructed to
give the model a sense of dataflow, and then a loop loops over
<code>some_numbers</code>.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The loop indicates that the next few lines need to
process <code>num</code> in some way, and the fact that it comes from
<code>some_numbers</code> means it has the type
<code>Optional[int]</code>. <code>num</code> can’t be directly appended
to <code>result</code>, because <code>result</code> is declared to only
contain <code>int</code>s, not <code>Optional[int]</code>s. None of the
normal operations on <code>int</code>s will work on <code>None</code>s,
so before <code>num</code> can be processed, something has to be done to
separate <code>None</code>s and normal <code>int</code>s. The simplest
way to do this is to introduce a branch <code>if num is None</code>, but
several variants are also valid: <code>if num is not None</code>,
<code>if num == None</code>, <code>if isinstance(num, int)</code>. Since
this is a pretty small space of valid next lines, and all of them imply
some understanding that <code>num</code> may not be an <code>int</code>,
we can use this program to test models for nullability understanding by
asking them to complete the program another lines, then see if they
produce something that matches these valid lines with the regular
expression <code>num\s*(is\s*(not)?|==)\s*None|isinstance\(num</code>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Stay tuned in the future for a more in-depth exploration
of how the models behave on individual typing rules with different
contexts and variable names.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Technically this is known as “Optional typing”, but
that’s confusing in the context of this post. Not to be confused with
Gradual Typing, as introduced by Siek et al.<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Despite this, it does pass the test 40% of the available
revisions, about triple what the other closest sizes can accomplish<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This is because function declarations with a colon and
no type, like <code>def fn(x:)</code> are not valid python. Since we’ve
already seen a usage of <code>get_square</code> that is passed a None
value, it wouldn’t be type-valid to complete the program with just
<code>int</code>. So a model can be tested on its understanding of
<code>Optional</code> annotations by seeing if its completion of the
partial program includes <code>Optional[int]</code>.<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Pythia 12b performs worse than its 6.9b variant, though
this might be due to under-training at that size. Qwen 32B also performs
about as well as Pythia 6.9b, it’s not clear if this is due to model
architecture or something else.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>We don’t see all six non-annotated function tests
passing under mypy, because models can still fail these tests by
producing invalid syntax.<a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>rolling average with a window size of 5<a href="#fnref9"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Since we don’t have contrasting pairs, just labeled
points, it’s not possible to use the PCA from contrasting pairs method
used in <span class="citation" data-cites="marks24">Marks and Tegmark
(2024)</span> and <span class="citation" data-cites="zou25">Zou et al.
(2025)</span>. See “Mass Mean Probing vs Linear Regression” in the
appendix<a href="#fnref10" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p> <span class="citation" data-cites="li24">Li et al.
(2024)</span> and <span class="citation" data-cites="zou25">Zou et al.
(2025)</span> suggest that mass means probes are best for reading, while
the direction perpendicular to the separating hyperplane is best for
intervention. However, previous work leaves open the question of
cross-layer weights. We use LR on the cross-layer weights, thanks to our
investigations above.<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>There is also a much more practical reason: right after
the model has generated a variable that will be written to, it often
does not have access to the assigning expression or type annotation,
giving it no way to determine if the value will be optional or now.<a
href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Red tokens are significantly below the threshold, and
green tokens are significantly above it; tokens that scored near the
threshold would have a near-white color, but no such tokens appear in
this example.<a href="#fnref13" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
