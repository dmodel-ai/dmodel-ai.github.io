\appendix
# Nullability In Python

To see how nullability appears in Python, Let’s look at an example
program in Python that uses nullability:

```python
class User:
    def __init__(self, name: str, email: Optional[str]) -> None:
    	 self.name = name
        self.email = email

def get_user_email(user: Optional[User]) -> Optional[str]:
    if user is not None and user.email is not None:
        return user.email
    else:
    	  return None
```

In the `get_user_email` function above, we can see from the type
signature that it takes a nullable User value, and returns an nullable
string. The first thing the function does is check if the input `user`
is None or not. This program was actually generated by o1-mini, so we
can already see that the model understands that the user object is
nullable, and that it needs to be checked for None-ness before
anything else can be done.

We can say that there are five variable “occurrences” in the program,
each of which can be either nullable or not. There’s the first `user`
in the if statement, the second `user` to the right of the `and` in
`user.email`, there’s the `user.email` itself, the `user` on the
second line, and finally the `user.email` on the second line. If we
use Python’s typechecker, mypy, to check the types of each of these
occurrences, we find that they have type `Optional[User]`, `User`,
`Optional[str]`, `User`, and `str` respectively. That is, the first
and third are nullable, and the rest are not.

# Python Subset Syntax and Typing Rules {#sec:formalrules}

Here we define the syntax and semantics of the Python subset we work
with:

## Common Rules {#sec:commonrules}

But before we try to measure nullability understanding, we'll want to
be more precise about exactly what we're measuring. To that end, we'll
take the notion of different "rules" for nullability that we discussed
informally in the overview, and bring it into a formal typing
system. We're not going to try to describe all the typing rules of
python, so we'll restrict ourselves in a couple of ways.

First, we'll reduce the number of python features that we handle by
actually working in a subset of python. This means we can skip
worrying about the semantics of complicated features, and just focus
on the features necessary for understanding optionality in a
semi-realistic setting.

Second, we'll define all non-None values as converting to the boolean
value True, instead of numbers converting to False when they are zero
and strings converting to False when they are empty. This is a
necessary practicality, because otherwise, the model could circumvent
our type tests by doing bare ifs which work on both optional and
non-optional types. But to prevent bare ifs from ever being the
correct completion for a non-optional type, we'll design our tests so
that there are never any values that would convert to False, namely
the number zero and the empty string.

$$\begin{aligned}
\left[Program\right]  =& [\epsilon] {\huge|} \left[\begin{array}{l}Stmt\\Program\end{array}\right]\\
[Stmt]       =& [Import] | [FnDef] | [Assn] | [ForLoop] | [If] | [Expr] \\
              &| [\texttt{return }Expr]\\
[Import]     =& [\texttt{from } Ident \texttt{ import } Ident]\\
[FnDef]      =&
\left[\begin{array}{l}
\texttt{def $Ident$($DeclArgs$):}\\
\quad Program
\end{array}\right]\\
&{\huge|}
\left[\begin{array}{l}
\texttt{def $Ident$($DeclArgs$) -> $Type$}:\\
\quad Program
\end{array}\right]\\
[DeclArgs]   =& [\epsilon] | [Ident] | [Ident : Type]\\
              &| [Ident \texttt{,} DeclArgs]
 | [Ident : Type\texttt{,} DeclArgs]\\
[Type]       =& [\texttt{int}] | [\texttt{str}] | [\texttt{bool}] | [\texttt{None}] \\
              &| [\texttt{Optional[}Type\texttt{]}] | [\texttt{List[}Type\texttt{]}]\\
[Assn]       =& [Ident \texttt{ = } Expr]\\
[ForLoop]    =&
\left[\begin{array}{l}
\texttt{for $ident$ in $ident$:}\\
\quad Program
\end{array}\right]\\
[If]         =&
\left[\begin{array}{l}
\texttt{if $expr$:}\\
\quad Program
\end{array}\right]\\
[Expr]       =& [Ident] | [Constant] | [Expr\texttt{ }Op\texttt{ }Expr]\\
              &| [Ident \texttt{(} ParamsList \texttt{)}]| [\texttt{$Expr$ if $Expr$ else $Expr$}]\\
              &| [\texttt{[$ListConts$]}] | [\texttt{[]}]\\
[ParamsList] =& [\epsilon] | [Expr] | [Expr\texttt{, }ParamsList]\\
[Op]         =& [\texttt{+}] | [\texttt{-}] | [\texttt{*}] | [\texttt{/}] | [\texttt{<}] | [\texttt{>}] | [\texttt{<=}] | [\texttt{>=}] | [\texttt{is}] | [\texttt{is not}] | [\texttt{==}] | [\texttt{!=}]\\
[ListConts]  =& [Expr] | [Expr,ListConts]\\
\end{aligned}$$

We won't worry too much about the semantics on a formal level, since
it's the same as Python's semantics on this subset, and we'll mostly
be using it informally to justify typing rules here. But we do want to
formally define our typing rules, since we'll be measuring the models
understanding of particular rules. We'll be using two propositions for
typing in this language. There's $\Gamma \vdash \left[p\right]
\vartriangleright \texttt{ok}$, which says that the program $p$ is
well typed in environment $\Gamma$. And there's $\Gamma \vdash e : t$,
which says that $e$ is well typed with type $t$ in environment
$\Gamma$. When we exclude the $\Gamma$, we mean that the judgement is
true under any typing environment, including the empty one.

$$
\tag{Const}
\frac{Constant\neq\texttt{None}}{\vdash Constant: \text{Atom}}
\hspace{1.0cm}
\frac{}{\forall t, \vdash \texttt{None}: \texttt{Optional[$t$]}}
$$

$$
\tag{List}
\frac{\Gamma \vdash x_1 : t, x_2 : t, ...}
     {\Gamma \vdash \texttt{[$x_1$, $x_2$, ...]} : \texttt{List[$t$]}}
\hspace{1.0cm}
\frac{}{\forall t, \vdash \texttt{[]}: \texttt{List[$t$]}}
$$

$$
\tag{Weaken}
\frac{\Gamma \vdash x: t}{\Gamma \vdash x: \texttt{Optional[$t$]}}
$$

$$
\tag{Var}
\Gamma \vdash e: t \hspace{1cm} \Gamma, x: t \vdash [p] \vartriangleright \text{ok}
\over
\Gamma \vdash \left[\begin{array}{l}\texttt{$x$ = $e$}\\ p\end{array}\right] \vartriangleright \text{ok}
$$

\begin{gather}
\Gamma, x_1: t_1, x_2: t_2, ... \vdash [b] \vartriangleright \text{ok}\nonumber\\
\tag{Def}
\Gamma, f : t_1 \rightarrow t_2 ... \rightarrow t_r \vdash [p] \vartriangleright \text{ok} \hspace{1cm} \Gamma \vdash \text{Returns($b$, $t_r$)}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$: $t_1$, $x_2$: $t_2$, ...) -> $t_r$:}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}

$$
\tag{App}
\frac{\Gamma \vdash f : t_1 \rightarrow t_2 ... \rightarrow t_r \hspace{1cm} \Gamma \vdash x_1 : t_1, x_2 : t_2, ...}
     {\Gamma \vdash f(x_1, x_2, ...) : t_r}
$$

\begin{gather}
\Gamma \vdash x : \texttt{Optional[$t$]} \hspace{1cm}
\Gamma, x : t \vdash [p] \vartriangleright \text{ok} \nonumber\\
\tag{IfIn}
\bigotimes \in \{\texttt{is not, !=}\}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $x$:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
\hspace{1cm}
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $x$ $\bigotimes$ None:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}

$$
\tag{IfOut}
\Gamma \vdash
\left[\begin{array}{l}
p_1\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\hspace{1cm}
\Gamma \vdash
\left[\begin{array}{l}
p_2\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $e$:}\\
\quad p_1\\
\texttt{else:}\\
\quad p_2\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
$$

$$
\tag{IfExpr}
\frac{\Gamma \vdash (e_b : \text{Optional[$t_0$]} \lor e_b : \texttt{bool}), e_1 : t, e_2 : t}
     {\Gamma \vdash \texttt{$e_1$ if $e_b$ else $e_2$} : t}
$$

$$
\tag{For}
\Gamma \vdash y\texttt{ : List } t \qquad  \Gamma, x : t \vdash [p] \vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{for $x$ in  $y$:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
$$

$$
\tag{OpInt}
\Gamma \vdash x_1 : \texttt{int}, x_2 : \texttt{int}\qquad \bigotimes \in \{\text{\texttt{+, -, *, /}\}}
\over
\Gamma \vdash x_1 \bigotimes x_2 : \texttt{int}
$$

$$
\hspace{-1.5cm}
\tag{OpString}
\frac{\Gamma \vdash s_1 : \texttt{str}, s_2 : \texttt{str}}
     {\Gamma \vdash s_1 + s_2 : \texttt{str}}
\hspace{1cm}
\frac{\Gamma \vdash s : \texttt{str}, x: \texttt{int}}
     {\Gamma \vdash s * x : \texttt{str}}
$$

$$
\hspace{-1.5cm}
\tag{OpEquality}
\frac{\Gamma \vdash x_1 : t, x_2 : t\qquad \bigotimes \in \{\texttt{==, !=, is, is not}\}}
     {\Gamma \vdash x_1 \bigotimes x_2 : \texttt{bool}}
$$

$$
\hspace{-2cm}
\tag{OpComparison}
\frac{\Gamma \vdash x_1 : \texttt{int}, x_2 : \texttt{int}\qquad \bigotimes \in \{\texttt{<, >, <=, >=}\}}
     {\Gamma \vdash x_1 \bigotimes x_2 : \texttt{bool}}
$$

With:

$$
\tag{ReturnReturn}
\frac{\Gamma \vdash e : t}{\Gamma \vdash \text{Returns(\texttt{return $e$}, $t$)}}
$$
$$
\tag{NoReturnExpression}
\frac{\Gamma \vdash e : t}{\vdash \text{NoReturn([$e$])}}
$$
$$
\tag{NoReturnAssign}
\frac{}{\vdash \text{NoReturn([\texttt{$i$ = $e$}])}}
$$
$$
\tag{ReturnIf}
\frac{\Gamma \vdash \text{Returns($p$, $t$)}}
     {\Gamma \vdash \text{Returns(}\left[\begin{array}{l}\texttt{if $e$:}\\ \quad p\end{array}\right],t\text{)}}
\hspace{1cm}
$$

\begin{gather}
\Gamma \vdash \left(\text{Returns($p_1$, $t$)} \land \text{Returns($p_2$, $t$)}\right)\lor \nonumber\\
\Gamma \vdash \left(\text{Returns($p_1$, $t$)} \land \text{NoReturns($p_2$)}\right) \lor\nonumber\\
\tag{ReturnIfElse}
\Gamma \vdash \left(\text{NoReturns($p_1$)} \land \text{Returns($p_2$, $t$)}\right)
\over
\Gamma \vdash \text{Returns(}\left[\begin{array}{l}
\texttt{if $e$:}\\
\quad p_1\\
\texttt{else:}\\
\quad p_2
\end{array}\right]\text{, $t$)}
\end{gather}

$$
\tag{ReturnFor}
\frac{\Gamma \vdash \text{Returns($p$, $t$)}}
     {\Gamma \vdash \text{Returns(}\left[\begin{array}{l}
     \texttt{for $x$ in $y$:}\\
     \quad p
     \end{array}\right]\text{, $t$)}}
$$

## Unannotated Functions {#sec:unannotatedfuncs}

The rules above are sufficient for our purposes for dealing with fully
type annotated programs. But what about programs with functions that
aren't type annotated, or are only partially annotated? The mypy type
system that Python uses treats unannotated functions as dynamically
typed, allowing them to typecheck as any type. Similarly, function
parameters without a type annotation are implicitly convertible to any
type. So, for normally mypy typechecking, we add the following typing
rules:

\begin{gather}
\Gamma, x_1 : Any, x_2 : Any, ... \vdash [b] \vartriangleright \text{ok}\nonumber\\
\tag{DynDef}
\Gamma, f : Any \rightarrow ... \rightarrow Any \vdash [p] \vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$, $x_2$, ...):}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}

$$
\tag{DynConvert}
\frac{\Gamma \vdash x : t}
     {\Gamma \vdash x : Any}
     \hspace{0.5cm}
\frac{\Gamma \vdash x : Any}
     {\Gamma \vdash x : t}
$$

These rules allow more python programs to check statically, but they
mean that some python programs will pass the static checks but still
throw type errors at runtime. Importantly for us, they can throw
runtime type errors about optionality. A well-written piece of code
would not just satisfy this type system then, but actually a stronger
one that would prevent runtime type errors. We'll call this stronger
type system mypy++. Instead of the Dyn- rules, it has this one:

\begin{gather}
\Gamma, x_1 : t_1, x_2 : t_2, ... \vdash [b] \vartriangleright \text{ok}\nonumber\\
\tag{InferDef}
\Gamma, f : t_1 \rightarrow t_2 \rightarrow ... \rightarrow t_r \vdash [p] \vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$, $x_2$, ...):}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}

# Detailed High-Level Nullability Test Results

## Across Scale

You can see in @fig:hl_scale that the smallest three models don’t pass any
test. After that, the trend line for tests passed goes upwards, but
it’s not monotonic: Pythia 410m passes only one test, while Pythia
160m passes three, and Pythia 6.9b passes the most tests, with 9,
while Pythia 12b only passes 7. For instance, given the partial code:

*Test 3*
```python
def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  x = process_value(value) + 1
  print(x)

def process_value(value):
```
Pythia 6.9b completes `process_value` as:
```python
def process_value(value):
  if value is None:
    return 0
  elif isinstance(value, str):
    return int(value)
  ...
```
While Pythia 12b completes it as:
```python
def process_value(value):
  return value
```

The code generated by Pythia 6.9b properly handles values of both
NoneType and `str` and always returns an int, whereas the code generated
by Pythia 12b is the identify function, returning None when the input
is None and returning a string when the input is a string. Neither of
these cases is correct, as the main function expects this to return a
Number that can be added to one.

## Across Time

![A line graph showing how the performance of the Pythia
6.9 model changes during training](images/hl_revision_results.svg){#fig:hl_time}

We say in @fig:hl_time that while the model generally gets better at
passing these tests during training, the performance is not always
increasing.  Zooming in on a particular test, we can see what this
looks like. When asked to complete this code:

*Test 12*
```python
def handle_value(value, guard):
  if guard:
    return count_os("Foobar") * 2
  else:
    return count_os(value) // 2

def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  print(handle_value(value, x < 10))

def count_os(value):
```

Pythia 6.9b at training step 104000 produces the following definition
of `count_os`:

```python
def count_os(value):
  if value is None:
    return 1
  elif isinstance(value, str):
    return len(value)
...
```

While 1000 steps later, it produces a very different definition:

```python
def count_os(value):
  return len([s for s in os.listdir(os.path.join(os.getcwd(), value)) if s.start ...
```

While the first definition handles value being None or a string, the
second definition not only assumes it is a string but that it is a
name of a folder in the current directory. In some sense the model
“knows” that the value parameter is nullable at step 104000, but
“forgets” it during further training. It regains the ability to pass
the test at several points during training, but by the end of
training, it’s back to treating `value` as if it was always a string.

## Common Mistakes

As expected, type annotations are particularly difficult for these
models. When asked to complete the type of get_square in the following
code, no model in the Pythia series can successfully output the
Optional argument type:

*Test 5*
```python
def program_48() -> None:
  number: Optional[int] = None
  square = get_square(number)
  if square is not None:
    print(f"Square of the number is {square}")
  else:
    print("No number provided to square")

def get_square(number:
```

These models also find it much easier to deal with a list that
contains some None elements, than to deal with an atomic value that
might be None. Pythia 6.9b consistently passes this test in the
second half of training:

*Test 2*
```python
from typing import Optional

def main() -> None:
  some_numbers = [1, -4, None, -3, 10, -1, None, None, 8]
  print(positive_numbers(some_numbers))

def positive_numbers(numbers: list[Optional[int]]) -> list[int]:
  result: list[int] = []
  for num in numbers:
```

But is much more challenged by this code:

*Test 3*
```python
def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  x = process_value(value) + 1
  print(x)

def process_value(value):
```

, intermittently being unable to complete it in a type-safe way up,
including in the second-to-last training step. When another level of
function indirection is added, the model becomes much better at
completing it correctly; Pythia 6.9b consistently completes the
following after step 93000:

*Test 4*
```python
def handle_value(value, guard):
  if guard:
    return process_value("Foobar") + 1
  else:
    return process_value(value) + 1
def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  x = handle_value(value, x < 10)
  print(x)

def process_value(value):
```
The names of functions are highly influential in the models
understanding of their type semantics. When test 3 is changed so that
`process_value` is instead called `count_chars`, Pythia 6.9b becomes
unable to correctly complete it on any revision; it likely has a bias
from its training data that functions called `count_chars` always take
non-nullable strings. Similarly, when test 4 is changed so that
process_values becomes string_complexity, it goes from consistently
passing to almost always failing.

# Mass Mean Probing vs Linear Regression

We were initially very surprised to find that mass means probing would
perform better than linear regression. After all, linear regression is
a much more powerful technique for fitting data. And mass means
probing can be seen as giving the direction of best fit in each
dimension independently, without considering other dimensions. The
more dimensions you consider at one time, the better your model fit
can be. But repeatedly in our data, we found that mass means probing
outperformed linear regression on the test data.
