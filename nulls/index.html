<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Alex Sanchez-Stern and Anish Tondwalkar" />
  <title>Understanding Models Understanding Nullability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <link rel="stylesheet" href="sidenote.css" />
  <script src="sidenotes.js"></script>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Understanding Models Understanding Nullability</h1>
<p class="author"><a href="https://www.alexsanchezstern.com">Alex
Sanchez-Stern</a> and <a href="https://ani.sh">Anish Tondwalkar</a></p>
<p class="date"><span class="math inline">d_{model}</span></p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Large language models have demonstrated an emergent ability to write
code, but this ability requires an internal representation of program
semantics that is little understood. Recent interpretability work has
demonstrated that it is possible to extract internal representations of
natural language concepts, raising the possibility that similar
techniques could be used to extract program semantics concepts. In this
work, we study how large language models represent the nullability of
program values. We measure how well models of various sizes complete
programs that use nullable values, and then extract an internal
representation of nullability.
</div>
</header>
<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>The last five years have shown us that large language models, like
ChatGPT, Claude, and DeepSeek, can effectively write programs in many
domains. This is an impressive capability, given that writing programs
involves having a formal understanding of program semantics. But though
we know that these large models understand programs to an extent, we
still don’t know many things about these models’ understanding. We don’t
know where they have deep understanding and where they use heuristic
reasoning, how they represents program knowledge, and what kinds of
situations will challenge their capabilities.</p>
<p>Fortunately, recent work in model interpretability and representation
engineering<span class="aside AT">AT: reframe. recent tools let us....</span>
has produced promising results which give hope towards understanding
more and more of the internal thought processes of LLMs. Here at <span
class="math inline">d_{model}</span> , we can think of no better place
to apply these new techniques than formal methods, where there are many
abstract properties that can be extracted with static analysis. The vast
work done in programming language theory over the past hundred years
provides many tools for scaling an understanding of the internal thought
processes of language models as they write
code.<span class="aside AT">AT: for examples, see cite, cite</span></p>
<p>In that spirit, we wanted to start with a simple property that comes
up in every programming languages, nullability. Nullable values are
represented differently across languages; as null pointers in C or C++,
with explicit Option types in Rust, and with special nil or None values
in dynamic languages like Javascript, Lisp, or Python. In every case,
understanding where values can be nullable is necessary for writing even
basic code, and misunderstanding where they are nullable can often be a
source of bugs.</p>
<p>Do our models understand when a value is nullable? They must, to be
able to write code that deals with nullable values, but we haven’t known
what form this knowledge takes, what situations are likely to confuse
the model. Until now.<span class=aside>big claim!</span></p>
<p>In this work:</p>
<ul>
<li><p>We introduce a microbenchmark of 15 programs that test basic
model understanding of the flow of nullability through a
program.</p></li>
<li><p>We find that models begin to understand nullability in a local
scope, satisfying many requirements of the python typechecker, before
they start to understand how nullability flows across the program.
(Sec. <a href="#sec:testing">2</a>)</p></li>
<li><p>We find that models develop an internal concept of nullability as
they scale up and are trained for longer. (Sec. <a
href="#sec:probing">3</a>)</p></li>
</ul>
<p>We end with a demo: after we train a probe that uses the determines
whether the model thinks a variable read corresponds to a nullable
variable, we can demonstrate that internal knowledge in a reading
diagram:</p>
<figure id="fig:reading1">
<img src="images/reading_diagram.svg" class="inlinefig"
alt="Figure 1: A diagram showing a simple program, and the probes nullability predictions for each variable load." />
<figcaption aria-hidden="true">Figure 1: A diagram showing a simple
program, and the probes nullability predictions for each variable
load.</figcaption>
</figure>
<h1 data-number="2" id="sec:testing"><span
class="header-section-number">2</span> Measuring Nullability
Understanding Externally</h1>
<p>We begin with a “skyline” estimate of model understanding of
nullability (a la <span class="citation" data-cites="feng24binding">Feng
and Steinhardt (2024)</span>). That is, we evaluate model nullability
understanding externally, at the token-level, (as opposite to
internally, at the activation level, using interpretability techniques).
In this section, we first explain the task of nullability understanding
(Sec. <a href="#sec:task">2.1</a>). Then we formally decompose the
reasoning steps required to reason about nullability both inside
(Sec. <a href="#sec:intra">2.2</a>) and across (Sec. <a
href="#sec:inter">2.3</a>) functions. Finally, we present the results of
our “skyline” analysis. (Sec. <a href="#sec:eval_results">2.5</a>).</p>
<h2 data-number="2.1" id="sec:task"><span
class="header-section-number">2.1</span>
<code>NullabilityEval</code></h2>
<p>To measure nullability understanding externally, we ask the model to
complete simple partial programs that each require an understanding of
nullability. We refer to this suite of programs as
<code>NullabilityEval</code>. All of the tests in this benchmark suite
are composed of three functions or less, where the largest function is
seven lines long.</p>
<p>In our experiments we focus on the Pythia model suite <span
class="citation" data-cites="biderman23">(Biderman et al. 2023)</span>,
as they have checkpoints available across training runs and various
scales. For measuring performance at larger sizes, we also include
Qwen2.5-Coder-32b<span class="aside AT">AT: cite</span>, Llama 3.1 405b
Instruct<span class="aside AT">AT: cite</span>, and DeepSeek-V3
(671b)<span class="aside AT">AT: cite</span>.</p>
<p>Each partial program is constructed such that there are a very
limited number of valid next lines in the program, and all of them
demonstrate some knowledge of the concept of nullability.</p>
<p>For example, our first test is:</p>
<p><em>Test 1</em><a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span></code></pre></div>
<p>We would like the mode to generate code that performs some operation
on the elements of <code>some_numbers</code>. The simplest way to do
that is to introduce a branch <code>if num is None</code>, but several
variants are also valid: <code>if num is not None</code>,
<code>if num == None</code>, <code>if isinstance(num, int)</code>. That
is, this example is constructed such that there is a small space of
valid next lines, and all of them require some understanding that
<code>num</code> may not be an <code>int</code>.<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<h2 data-number="2.2" id="sec:intra"><span
class="header-section-number">2.2</span> Understanding Typing Rules</h2>
<p>We find that Pythia models as small as 2.8b can successfully complete
Test 1, and that they learn to complete the test in the first third of
training. Consistent with observations that larger models are more
sample-efficient
<span class="aside AT">AT: who do I cite for this claim? Kaplan?</span>,
larger Pythia models learn to complete this test earlier, with Pythia
12b able to complete the test 20% of the way into training and Pythia
2.8b able to complete it 28% of the way into training.<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>These results indicate that these models understand nullability to
some extent, but how deep is this understanding? To quantify this, we
give a syntax and semantics of a minimalist subset of python that
captures nullability in Appendix <a href="#sec:commonrules">B.1</a>. We
can then classify each partial program by which program constructs and
rules determine the nullability of the target variable. For instance,
Test 1 uses the <span class="math inline">(List)</span>, <span
class="math inline">(Var)</span>, and <span
class="math inline">(For)</span> rules.</p>
<p>So, do Pythia models 2.8b and up understand the semantics of these
three rules? As it turns out, not exactly. LLM’s pick up on a lot of
information relationships in their training data that have statistical
correlation, without it necessarily being causal. What this means in
practice is that the models use things like variable names, whitespace,
statement orderings, and constant values to guess at certain programming
concepts.</p>
<p>For example, while many of the Pythia models can complete:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span></code></pre></div>
<p>with a proper None check, changing the variable names and the
constants into:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>foo <span class="op">=</span> [<span class="dv">60</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">33</span>]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>bar: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> corejoice <span class="kw">in</span> foo:</span></code></pre></div>
<p>causes all the Pythia models to fail the test.</p>
<p>To test the reliance on these features more robustly, we generated
100 random variable namings and constant values for the above program,
as well as for small program that test other typing rules. We also
balanced these tests with programs that made use of the same language
constructs, but where the loop variable was <em>not</em> optional. We
found that when lists and for loops were involved, Pythia 2.8b was only
able to correctly label 57% of the programs, only slightly better than
chance. Even Pythia 12b could only correctly label 68% of the tests.
These results show that the Pythia models rely heavily on these
non-semantic features when reasoning about for loops.</p>
<p>Fortunately, many other typing rules do not exhibit such a strong
reliance on variable naming and constants. In the same setting of
generated programs, we found that Pythia 2.8b was able to correctly
label programs using the Abs, Var, App, and If_Out a much greater
proportion of the time (99%, 93%, 86%, and 98% respectively). Stay tuned
in the future (Sec. <a href="#sec:future">4</a>) for a more in-depth
exploration of how the models behave on individual typing rules when we
increase the variability of our program generation.</p>
<h2 data-number="2.3" id="sec:inter"><span
class="header-section-number">2.3</span> Interprocedural Analysis</h2>
<p>We can further challenge the model by adding layers of procedural
indirection between the source and sink of nullable values, thereby
testing the model’s <em>interprocedural</em> understanding. First, we
demonstrate how to write such tests, and the difficulties of writing
tests that may be too easy (Sec. <a href="#sec:inter_test">2.3.1</a>).
Then, we present a harder problem (Sec. <a
href="#sec:unannot">2.3.2</a>) and introduce a stronger type system
<code>mypy++</code> to formalized the needed reasoning (Sec. <a
href="#sec:mypypp">2.3.3</a>).</p>
<h3 data-number="2.3.1" id="sec:inter_test"><span
class="header-section-number">2.3.1</span> A simple test</h3>
<p>Here’s a simple test for interprocedural analyses:</p>
<p><em>Test 2</em></p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(positive_numbers(some_numbers))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positive_numbers(numbers: <span class="bu">list</span>[Optional[<span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> numbers:</span></code></pre></div>
<p>In this test, we’ve taken the same logic and spread it across
<code>main</code> and another function, <code>positive_numbers</code>.
Ideally, the model would have to think a little harder to understand
that <code>some_numbers</code> is flowing from <code>main</code> through
the function call into the body of <code>positive_numbers</code>,
causing the for loop body to need a <code>None</code> check. In
practice, however, we find this test is actually <em>easier</em> for the
model to pass than Test 1, with models as small as Pythia 1b passing it,
and Pythia 12b learning to pass it 13% of the way into training.</p>
<p>Because of the type annotations on the <code>positive_numbers</code>
function, the model doesn’t need to pay attention to <code>main</code>
at all. It can just look at <code>positive_numbers</code>, and use the
type annotation to know that <code>numbers</code> contains
<code>Optional[int]</code>s. Then, using the For rule it can reason that
<code>num</code> is nullable, so it must be checked for None before
proceeding. Looking at the type annotation turns out to be easier for
the model than scanning through a list to determine if there are None
and non-None values, resulting in an easier test overall.</p>
<h3 data-number="2.3.2" id="sec:unannot"><span
class="header-section-number">2.3.2</span> Unannotated Functions</h3>
<p>So how would we <em>actually</em> test for interprocedural
nullability understanding in the model? Well, type annotations on Python
functions aren’t required<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>, so we can instead provide the model
with an <strong>unannotated</strong> function, and see if it still
understands the flow of nullability. Here’s a test that does that:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>Our base set of typing rules (listed as “Common Rules”) don’t handle
unannotated functions though, so we’re going to have to add some more,
and here we’re faced with a choice. The typing rules for normal Python
say that functions without return type annotations return the Any type,
and arguments without a type annotation have the type Any. In fact,
normal mypy will not check unannotated functions at <em>all</em>, even
for internal consistency; the <code>--check-untyped-defs</code> option
will add some checking back, but the types of arguments and return type
will still be Any. In Python, a value of any type can be converted to an
Any, and an Any can be converted to any value type.</p>
<p>This means that it would be technically type safe to do anything in
the body of <code>process_value</code>, including just returning the
argument, without a static type error. All Pythia models with at least
410 million parameters are able to make use of this extra flexibility to
write code for Test 3 that typechecks under mypy. But at runtime, code
that exploits it would still fail.</p>
<h3 data-number="2.3.3" id="sec:mypypp"><span
class="header-section-number">2.3.3</span> A stricter type system for
Python: mypy++</h3>
<p>If we want code that does not <code>TypeError</code> at runtime, we
can strengthen our type checker by requiring that there be some valid,
non-<code>Any</code>, type for the function that typechecks at the call
site and in the function body. This type need not appear in the source
code, but is required to exist. We’ll call this augmented type system
mypy++.</p>
<p>In Appendix <a href="#sec:unannotatedfuncs">B.2</a>, we formalize the
unannotated function rules for mypy vs mypy++.</p>
<p>There’s no consistent threshold of size at which Pythia models can
pass Test 3. Pythia 1b, 2.8b, and 6.9b pass the test in their final
revisions, but Pythia 410m, 1.4b, and 12b don’t. The models with at
least 1 billion parameters all have points in training where they can
pass the test, but only intermittently. Even 6.9b, the best performing
size on this test, fails the test in its second-to-last available
revision<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>. You can see how this evolves over
scale in fig. <a href="#fig:hl_mypy">3</a> and time in fig. <a
href="#fig:hl_moral">4</a>. See Sec. <a href="#sec:results">3.5</a> for
further discussion of performance over time.</p>
<p>What the models <em>can</em> do well, however, is learn to pass these
tests in the mypy type system (as opposed to mypy++). In that system,
where they don’t need to reason globally about the functions but only
locally, this test is one of the easiest for the models to complete.<a
href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a></p>
<p>Since this test suite is meant to be informative beyond the sizes of
the Pythia models, we also add another layer of indirection to add more
difficulty, in this test:</p>
<p><em>Test 4</em></p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> guard:</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>With two layers of indirection, we start to hit the limits of the
capabilities of even frontier models. Llama 405b is unable to
successfully pass this test, as are smaller models like Qwen Coder 32b,
while DeepSeek V3 (671b parameters) is able to pass it. However, Pythia
6.9b is still able to pass this test pretty consistently.</p>
<h2 data-number="2.4" id="generating-type-annotations"><span
class="header-section-number">2.4</span> Generating Type
Annotations</h2>
<p>Finally, we can test how well the models write write type annotations
for functions. Here, the trailling colon makes the type expression the
only valid completion.</p>
<p><em>Test 5</em>:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  square <span class="op">=</span> get_square(number)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number:</span></code></pre></div>
<p>None of the Pythia models pass this test. Qwen Coder 32b is also
incapable of passing this test, but both Llama 405b and DeepSeek V3 pass
it.</p>
<p>We would indeed expect that writing type annotations is more
difficult than merely implicitly reasoning about the types of python
programs, as only a small fraction of python programs in the wild are
thus annotated.</p>
<h2 data-number="2.5" id="sec:eval_results"><span
class="header-section-number">2.5</span> External Test Results Across
Training and Scale</h2>
<p>We wrote three variations of each of these tests, resulting in 15
tests total. Below, you can see the number of passing tests for each
model.
<span class="aside AT">AT: should we be plotting lpr? instead of raw pass rate? but log 15 is pretty small...</span></p>
<figure id="fig:hl_scale">
<img src="images/hl_model_results.svg"
alt="Figure 2: A bar graph showing how several sizes of model perform on the high-level nullability tests" />
<figcaption aria-hidden="true">Figure 2: A bar graph showing how several
sizes of model perform on the high-level nullability tests</figcaption>
</figure>
<p>In Fig. <a href="#fig:hl_scale">2</a>, we can see the number of
passing tests for each model. We can see that, generally speaking,
models get better with scale: Pythia-2.8b parameters can pass about half
the tests, but we need the much larger and more parameter efficient
Llama-405b to pass all of the tests. This matches our expectations that
eval scores should scale logarithmically, indicating that these tests
are well distributed.</p>
<figure id="fig:hl_mypy">
<img src="images/hl_mypy_vs_grep_models.svg"
alt="Figure 3: A bar plot showing how the Pythia models perform in mypy vs mypy++" />
<figcaption aria-hidden="true">Figure 3: A bar plot showing how the
Pythia models perform in mypy vs mypy++</figcaption>
</figure>
<span class=aside>let's merge these figures</span>
<p>In Fig. <a href="#fig:hl_mypy">3</a>, we see the test result for the
pythia models using the mypy and mypy++ type systems. As we expected,
the mypy results (red bar) are always above the mypy++ results (blue
bar), as mypy++ is a stricter type system. There are six tests in the
dataset involving non-annotated functions, and using the weaker mypy
typesystem causes up to five more tests to pass than using mypy++<a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a></p>
<p>Next, we want to understand the training dynamics at play here.
Below, we can see how Pythia 6.9b performs on the tests during training
from step 2 to 143000: <a href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a></p>
<figure id="fig:hl_moral">
<img src="images/hl_mypy_vs_grep_revisions.svg"
alt="Figure 4: A plot showing how often the Pythia 6.9b produces code that typechecks on the tests, vs produces code that does not go wrong." />
<figcaption aria-hidden="true">Figure 4: A plot showing how often the
Pythia 6.9b produces code that typechecks on the tests, vs produces code
that does not go wrong.</figcaption>
</figure>
<p>We see that each individual model learns to write code which
typechecks under mypy before it learns to write code which typechecks
under mypy++ and throws no type errors at runtime.</p>
<h1 data-number="3" id="sec:probing"><span
class="header-section-number">3</span> Measuring Nullability
Understanding Internally</h1>
<p>At this point, we’ve figured out how to roughly measure nullability
understanding in the output of various language models, but we still
don’t know what their internal representations might look like or when
they emerge. Next, we detail how we train reading vectors (Sec. <a
href="#sec:extraction">3.3</a>), using prompts designed to make the
model think about the phenomena of interest (Sec. <a
href="#sec:prompts">3.2</a>). Finally, in Sec. <a
href="#sec:results">3.5</a>, we validate that these probes improve their
understanding of nullability over the course of pretraining to the level
that we expect from the external, or token-level understanding evals we
describe in the previous section.</p>
<h2 data-number="3.1" id="background"><span
class="header-section-number">3.1</span> Background</h2>
<p>In this section, we review representation engineering <span
class="citation" data-cites="zou25">(Zou et al. 2025)</span> techniques
that we will use to look for linear representations of nullability
inside the model.</p>
<p><span class="citation" data-cites="zou25">Zou et al. (2025)</span>
shows how representations can be extracted for concepts like
“happiness”, “honesty”, and “fairness”. First, they construct many
prompts which cause the model to act in a way aligned with the concept,
and many which cause the model to act in a way aligned against the
concept. For instance, they might prompt the model with “Pretend you’re
a dishonest person. The Eiffel Tower is” and “Pretend you’re an honest
person. The Eiffel Tower is”. Then, they take the internal activations
which correspond to each, and try to extract a high-dimensional vector
which points towards the states which are aligned, and away from the
states that are not aligned. This vector can then be used as a linear
model to measure how much of the concept is activated in the model
during any given forward pass (e.g. for honesty, this gives us a
lie-detector).
<span class="aside AT">AT:  formatting citekey in figcap</span></p>
<figure>
<img src="images/zhou.png"
alt="Figure from Zou et al. (2025) showing the reading outputs for several concepts" />
<figcaption aria-hidden="true">Figure from <span class="citation"
data-cites="zou25">Zou et al. (2025)</span> showing the reading outputs
for several concepts</figcaption>
</figure>
<h2 data-number="3.2" id="sec:prompts"><span
class="header-section-number">3.2</span> Designing Prompts to Extract
Nullability Activations</h2>
<p>We avoid dealing with the ambiguities of natural language by working
in a setting where the model needs only to complete code. analyze the
nullability of individual variable occurrences. Specifically, we probe
for “the variable I just generated refers to an nullable quantity”, so
our prompts looked like:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_1() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> find_value(data: List[<span class="bu">int</span>], target: <span class="bu">int</span>) <span class="op">-&gt;</span> Optional[<span class="bu">int</span>]:</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> value <span class="kw">in</span> data:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> value <span class="op">==</span> target:</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> value</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  target <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  result <span class="op">=</span> find_value(data, target)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> result</span></code></pre></div>
<p>We queried o1, o1-mini, deepseek-coder, and claude-3.5-sonnet with
the following prompt:</p>
<blockquote>
<p>Generate me 100 typed python programs that use the Optional type,
along with type annotations for any undefined functions that they use.
Make sure that the programs are unique, and each involves at least eight
lines of logic. Number each program from 1 to 100. Please put all the
programs in a single file, with a main function that tests each. Don’t
include any text before or after the code, just the code. I will be
using mypy with the –strict option to check the code.</p>
</blockquote>
<p>We label each variable read occurrence with nullability information
derived from mypy, and prompt the “model under test” with a prompt
consisting of the tokens up to and including the variable read
occurrence.</p>
<h2 data-number="3.3" id="sec:extraction"><span
class="header-section-number">3.3</span> Extracting Reading Vectors</h2>
<p>Prior work focused their probing on a single layer, often handpicked
based on prior papers. We probe all layers instead. We use Mass Mean
Shift probing for each layer, because it’s been shown empirically <span
class="citation" data-cites="li24">(Li et al. 2024)</span> to generalize
better in high dimensional spaces than logistic regression<a href="#fn9"
class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a>.</p>
<p>We then tested two methods for determining the relative importance of
the different layers — either allowing the magnitude of the difference
of means vector to determine the importance of the layer in the final
probe, or to learn coefficients for each layer using linear regression.
We found that which method is more accurate on test data varies over
both model size and number of training steps.
<span class="aside AT">AT: should we mention this in the intro/contributions/or a "key results" section like tlide?</span></p>
<figure id="fig:mm-vs-mmlr-sizes">
<img src="images/mm-vs-mmlr.svg"
alt="Figure 5: The performance of pure mass means shift vs mass means shift with linear regression for different Pythia model sizes. Lower is better." />
<figcaption aria-hidden="true">Figure 5: The performance of pure mass
means shift vs mass means shift with linear regression for different
Pythia model sizes. Lower is better.</figcaption>
</figure>
<p>In Fig. <a href="#fig:mm-vs-mmlr-sizes">5</a>, we can see that pure
mass means probing
<span class="aside AT">AT: add a y axis label for BCE. This should probably be a line plot.</span>
gives lower loss for smaller models (those with less than 410 million
parameters), but that for larger models weighting layers using linear
regression gives lower loss consistently.</p>
<h2 data-number="3.4" id="visualizing-probe-outputs"><span
class="header-section-number">3.4</span> Visualizing Probe Outputs</h2>
<p>Let us return to the reading diagram from the introduction,
reproduced below.</p>
<p>This diagram is adapted from the style of reading diagram from <span
class="citation" data-cites="zou25">Zou et al. (2025)</span>, but only
show the activations on tokens that represent variable loads<a
href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a>. For each position of interest, we
prompt the model with the partial program that consists of all tokens up
to (preceeding) and including that position. We then probe the model at
the following token. We color the box above that position based on the
output of the probe, and a scoring threshold inferred at train-time<a
href="#fn11" class="footnote-ref" id="fnref11"
role="doc-noteref"><sup>11</sup></a>.</p>
<figure id="fig:reading2">
<img src="images/reading_diagram.svg" class="inlinefig"
alt="Figure 6: A diagram showing a simple program, and the probes nullability predictions for each variable load." />
<figcaption aria-hidden="true">Figure 6: A diagram showing a simple
program, and the probes nullability predictions for each variable
load.</figcaption>
</figure>
<p>In this program, there are sixteen tokens that correspond to variable
loads, and (correctly) all but one are marked as non-optional. The only
nullable variable in this program is <code>result</code>, since it comes
from <code>find_value</code> which returns <code>Optional[int]</code>.<a
href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a></p>
<h2 data-number="3.5" id="sec:results"><span
class="header-section-number">3.5</span> Probing Results Across Training
and Scale</h2>
<p>In this section, we study the performance of our nullability probes
across time and scale <span class="citation"
data-cites="tigges24">(Tigges et al. 2024)</span>. We use mass-means
shift probing <span class="citation" data-cites="li24">(Li et al.
2024)</span> on all layers, and a linear regression to determine the
weights of each layer.<a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a>
<span class="aside AT">AT: see appendix D for mm vs LR</span></p>
<figure id="fig:models-and-steps">
<img src="images/accuracy_during_pretraining.svg"
alt="Figure 7: The performance (probe test loss) of each Pythia model size during pretraining. Lower is better." />
<figcaption aria-hidden="true">Figure 7: The performance (probe test
loss) of each Pythia model size during pretraining. Lower is
better.</figcaption>
</figure>
<p>In fig. <a href="#fig:models-and-steps">7</a>, we plot loss against
scale and time. While we measured accuracy for every available Pythia
model size, we exclude the smallest (14m) from this plot since it would
exist entirely above the top of the plot.</p>
<p>One thing that is interesting to note is that models up to 1b reach a
minimum loss before loss for this task climbing again. Charitably, this
may be because the features beyond this point become more complex — less
linear, or the represented features themselves represent more subtle
concepts. Cynically, this reflects that models—small models in
particular—do not uniformly improve at this task over training, as we
observed in Sec. <a href="#sec:mypypp">2.3.3</a>.</p>
<p>Our suspicion is that this pattern would continue even for the larger
models if we continued to overtrain them for longer.
<span class=aside>position the min loss stuff in terms of scaling laws or something?</span></p>
<h1 data-number="4" id="sec:future"><span
class="header-section-number">4</span> Future Work</h1>
<p>We hope to get a better understanding of this phenomenon by studying
the decomposed nullability reasoning as described in Sec. <a
href="#sec:intra">2.2</a> and Appendix <a
href="#sec:commonrules">B.1</a>.</p>
<h1 data-number="5" id="sec:related"><span
class="header-section-number">5</span> Related Work</h1>
<p>Our decision to use Pythia to study feature evolution across time and
scale is inspired by <span class="citation" data-cites="tigges24">Tigges
et al. (2024)</span> . They focus on classic circuits-centered
interpretability tasks such as IOI <span class="citation"
data-cites="wang22">(Wang et al. 2022)</span>, Gendered-Pronoun <span
class="citation" data-cites="mathwin">(Mathwin et al., n.d.)</span>,
Greater-Than <span class="citation" data-cites="hanna23">(Hanna, Liu,
and Variengien 2023)</span> , and SVA <span class="citation"
data-cites="linzen16">(Linzen, Dupoux, and Goldberg 2016)</span>.</p>
<p>In our setting, we are more interested in how activations vary across
inputs, to extract representations of nullability. <span
class="citation" data-cites="zou25">Zou et al. (2025)</span> surveys
techniques for representation engineering with linear probes. We apply
similar techniques, but to program semantics and dataflow instead of
natural language.</p>
<p><span class="citation" data-cites="feng24predicate">Feng, Russell,
and Steinhardt (2024)</span> also study LLM’s ability to reason about
propositions, but in a natural language setting, rather than a formal
one.</p>
<p>Several techniques exist for constructing linear probes, but after
experimental measurement we followed the mass means shift from <span
class="citation" data-cites="li24">Li et al. (2024)</span>. <span
class="citation" data-cites="li24">Li et al. (2024)</span> and <span
class="citation" data-cites="zhong23">Zhong et al. (2023)</span> discuss
why mass mean probing might outperform linear regression.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-biderman23" class="csl-entry" role="listitem">
Biderman, Stella, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,
Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, et al. 2023.
<span>“Pythia: <span>A Suite</span> for <span>Analyzing Large Language
Models Across Training</span> and <span>Scaling</span>.”</span> arXiv.
<a
href="https://doi.org/10.48550/arXiv.2304.01373">https://doi.org/10.48550/arXiv.2304.01373</a>.
</div>
<div id="ref-feng24predicate" class="csl-entry" role="listitem">
Feng, Jiahai, Stuart Russell, and Jacob Steinhardt. 2024.
<span>“Monitoring <span>Latent World States</span> in <span>Language
Models</span> with <span>Propositional Probes</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2406.19501">https://doi.org/10.48550/arXiv.2406.19501</a>.
</div>
<div id="ref-feng24binding" class="csl-entry" role="listitem">
Feng, Jiahai, and Jacob Steinhardt. 2024. <span>“How Do <span>Language
Models Bind Entities</span> in <span>Context</span>?”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2310.17191">https://doi.org/10.48550/arXiv.2310.17191</a>.
</div>
<div id="ref-hanna23" class="csl-entry" role="listitem">
Hanna, Michael, Ollie Liu, and Alexandre Variengien. 2023. <span>“How
Does <span>GPT-2</span> Compute Greater-Than?: <span>Interpreting</span>
Mathematical Abilities in a Pre-Trained Language Model.”</span> arXiv.
<a
href="https://doi.org/10.48550/arXiv.2305.00586">https://doi.org/10.48550/arXiv.2305.00586</a>.
</div>
<div id="ref-li24" class="csl-entry" role="listitem">
Li, Kenneth, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin
Wattenberg. 2024. <span>“Inference-<span>Time Intervention</span>:
<span>Eliciting Truthful Answers</span> from a <span>Language
Model</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2306.03341">https://doi.org/10.48550/arXiv.2306.03341</a>.
</div>
<div id="ref-linzen16" class="csl-entry" role="listitem">
Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. <span>“Assessing
the <span>Ability</span> of <span>LSTMs</span> to <span>Learn
Syntax-Sensitive Dependencies</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.1611.01368">https://doi.org/10.48550/arXiv.1611.01368</a>.
</div>
<div id="ref-marks24" class="csl-entry" role="listitem">
Marks, Samuel, and Max Tegmark. 2024. <span>“The Geometry of Truth:
Emergent Linear Structure in Large Language Model Representations of
True/False Datasets.”</span> <a
href="https://arxiv.org/abs/2310.06824">https://arxiv.org/abs/2310.06824</a>.
</div>
<div id="ref-mathwin" class="csl-entry" role="listitem">
Mathwin, Chris, Guillaume Corlouer, Esben Kran, Fazl Barez, and Neel
Nanda. n.d. <span>“Identifying a <span>Preliminary Circuit</span> for
<span>Predicting Gendered Pronouns</span> in <span>GPT-2
Small</span>.”</span>
</div>
<div id="ref-tigges24" class="csl-entry" role="listitem">
Tigges, Curt, Michael Hanna, Qinan Yu, and Stella Biderman. 2024.
<span>“<span>LLM Circuit Analyses Are Consistent Across Training</span>
and <span>Scale</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2407.10827">https://doi.org/10.48550/arXiv.2407.10827</a>.
</div>
<div id="ref-wang22" class="csl-entry" role="listitem">
Wang, Kevin, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and
Jacob Steinhardt. 2022. <span>“Interpretability in the
<span>Wild</span>: A <span>Circuit</span> for <span>Indirect Object
Identification</span> in <span>GPT-2</span> Small.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2211.00593">https://doi.org/10.48550/arXiv.2211.00593</a>.
</div>
<div id="ref-zhong23" class="csl-entry" role="listitem">
Zhong, Ziqian, Ziming Liu, Max Tegmark, and Jacob Andreas. 2023.
<span>“The <span>Clock</span> and the <span>Pizza</span>: <span>Two
Stories</span> in <span>Mechanistic Explanation</span> of <span>Neural
Networks</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2306.17844">https://doi.org/10.48550/arXiv.2306.17844</a>.
</div>
<div id="ref-zou25" class="csl-entry" role="listitem">
Zou, Andy, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
Ren, Alexander Pan, et al. 2025. <span>“Representation
<span>Engineering</span>: <span>A Top-Down Approach</span> to <span>AI
Transparency</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2310.01405">https://doi.org/10.48550/arXiv.2310.01405</a>.
</div>
</div>
<h1 class="unnumbered" id="appendicies">Appendicies</h1>
<h1 class="unnumbered" id="nullability-in-python">A Nullability In
Python</h1>
<p>To see how nullability appears in Python, Let’s look at an example
program in Python that uses nullability:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> User:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, email: Optional[<span class="bu">str</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>         <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.email <span class="op">=</span> email</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_user_email(user: Optional[User]) <span class="op">-&gt;</span> Optional[<span class="bu">str</span>]:</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> user <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> user.email <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> user.email</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="va">None</span></span></code></pre></div>
<p>In the <code>get_user_email</code> function above, we can see from
the type signature that it takes a nullable User value, and returns an
nullable string. The first thing the function does is check if the input
<code>user</code> is None or not. This program was actually generated by
o1-mini, so we can already see that the model understands that the user
object is nullable, and that it needs to be checked for None-ness before
anything else can be done.</p>
<p>We can say that there are five variable “occurances” in the program,
each of which can be either nullable or not. There’s the first
<code>user</code> in the if statement, the second <code>user</code> to
the right of the <code>and</code> in <code>user.email</code>, there’s
the <code>user.email</code> itself, the <code>user</code> on the second
line, and finally the <code>user.email</code> on the second line. If we
use Python’s typechecker, mypy, to check the types of each of these
occurrences, we find that they have type <code>Optional[User]</code>,
<code>User</code>, <code>Optional[str]</code>, <code>User</code>, and
<code>str</code> respectively. That is, the first and third are
nullable, and the rest are not.</p>
<h1 class="unnumbered" id="sec:formalrules">B Python Subset Syntax and
Typing Rules</h1>
<p>Here we define the syntax and semantics of the Python subset we work
with:</p>
<h2 class="unnumbered" id="sec:commonrules">B.1 Common Rules</h2>
<p>But before we try to measure nullability understanding, we’ll want to
be more precise about exactly what we’re measuring. To that end, we’ll
take the notion of different “rules” for nullability that we discuseed
informally in the overview, and bring it into a formal typing system.
We’re not going to try to describe all the typing rules of python, so
we’ll restrict ourselves in a couple of ways.</p>
<p>First, we’ll reduce the number of python features that we handle by
actually working in a subset of python. This means we can skip worrying
about the semantics of complicated features, and just focus on the
features necessary for understanding optionality in a semi-realistic
setting.</p>
<p>Second, we’ll define all non-None values as converting to the boolean
value True, instead of numbers converting to False when they are zero
and strings converting to False when they are empty. This is a necessary
practicality, because otherwise, the model could circumvent our type
tests by doing bare ifs which work on both optional and non-optional
types. But to prevent bare ifs from ever being the correct completion
for a non-optional type, we’ll design our tests so that there are never
any values that would convert to False, namely the number zero and the
empty string.</p>
<p><span class="math display">\begin{aligned}
\left[Program\right]  =&amp; [\epsilon] {\huge|}
\left[\begin{array}{l}Stmt\\Program\end{array}\right]\\
[Stmt]       =&amp; [Import] | [FnDef] | [Assn] | [ForLoop] | [If] |
[Expr] \\
              &amp;| [\texttt{return }Expr]\\
[Import]     =&amp; [\texttt{from } Ident \texttt{ import } Ident]\\
[FnDef]      =&amp;
\left[\begin{array}{l}
\texttt{def $Ident$($DeclArgs$):}\\
\quad Program
\end{array}\right]\\
&amp;{\huge|}
\left[\begin{array}{l}
\texttt{def $Ident$($DeclArgs$) -&gt; $Type$}:\\
\quad Program
\end{array}\right]\\
[DeclArgs]   =&amp; [\epsilon] | [Ident] | [Ident : Type]\\
              &amp;| [Ident \texttt{,} DeclArgs]
| [Ident : Type\texttt{,} DeclArgs]\\
[Type]       =&amp; [\texttt{int}] | [\texttt{str}] | [\texttt{bool}] |
[\texttt{None}] \\
              &amp;| [\texttt{Optional[}Type\texttt{]}] |
[\texttt{List[}Type\texttt{]}]\\
[Assn]       =&amp; [Ident \texttt{ = } Expr]\\
[ForLoop]    =&amp;
\left[\begin{array}{l}
\texttt{for $ident$ in $ident$:}\\
\quad Program
\end{array}\right]\\
[If]         =&amp;
\left[\begin{array}{l}
\texttt{if $expr$:}\\
\quad Program
\end{array}\right]\\
[Expr]       =&amp; [Ident] | [Constant] | [Expr\texttt{ }Op\texttt{
}Expr]\\
              &amp;| [Ident \texttt{(} ParamsList \texttt{)}]|
[\texttt{$Expr$ if $Expr$ else $Expr$}]\\
              &amp;| [\texttt{[$ListConts$]}] | [\texttt{[]}]\\
[ParamsList] =&amp; [\epsilon] | [Expr] | [Expr\texttt{, }ParamsList]\\
[Op]         =&amp; [\texttt{+}] | [\texttt{-}] | [\texttt{*}] |
[\texttt{/}] | [\texttt{&lt;}] | [\texttt{&gt;}] | [\texttt{&lt;=}] |
[\texttt{&gt;=}] | [\texttt{is}] | [\texttt{is not}] | [\texttt{==}] |
[\texttt{!=}]\\
[ListConts]  =&amp; [Expr] | [Expr,ListConts]\\
\end{aligned}</span></p>
<p>We won’t worry too much about the semantics on a formal level, since
it’s the same as Python’s semantics on this subset, and we’ll mostly be
using it informally to justify typing rules here. But we do want to
formally define our typing rules, since we’ll be measuring the models
understanding of particular rules. We’ll be using two propositions for
typing in this language. There’s <span class="math inline">\Gamma \vdash
\left[p\right]
\vartriangleright \texttt{ok}</span>, which says that the program <span
class="math inline">p</span> is well typed in environment <span
class="math inline">\Gamma</span>. And there’s <span
class="math inline">\Gamma \vdash e : t</span>, which says that <span
class="math inline">e</span> is well typed with type <span
class="math inline">t</span> in environment <span
class="math inline">\Gamma</span>. When we exclude the <span
class="math inline">\Gamma</span>, we mean that the judgement is true
under any typing environment, including the empty one.</p>
<p><span class="math display">
\tag{Const}
\frac{Constant\neq\texttt{None}}{\vdash Constant: \text{Atom}}
\hspace{1.0cm}
\frac{}{\forall t, \vdash \texttt{None}: \texttt{Optional[$t$]}}
</span></p>
<p><span class="math display">
\tag{List}
\frac{\Gamma \vdash x_1 : t, x_2 : t, ...}
     {\Gamma \vdash \texttt{[$x_1$, $x_2$, ...]} : \texttt{List[$t$]}}
\hspace{1.0cm}
\frac{}{\forall t, \vdash \texttt{[]}: \texttt{List[$t$]}}
</span></p>
<p><span class="math display">
\tag{Weaken}
\frac{\Gamma \vdash x: t}{\Gamma \vdash x: \texttt{Optional[$t$]}}
</span></p>
<p><span class="math display">
\tag{Var}
\Gamma \vdash e: t \hspace{1cm} \Gamma, x: t \vdash [p]
\vartriangleright \text{ok}
\over
\Gamma \vdash \left[\begin{array}{l}\texttt{$x$ = $e$}\\
p\end{array}\right] \vartriangleright \text{ok}
</span></p>
<p><span class="math display">\begin{gather}
\Gamma, x_1: t_1, x_2: t_2, ... \vdash [b] \vartriangleright
\text{ok}\nonumber\\
\tag{Def}
\Gamma, f : t_1 \rightarrow t_2 ... \rightarrow t_r \vdash [p]
\vartriangleright \text{ok} \hspace{1cm} \Gamma \vdash
\text{Returns($b$, $t_r$)}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$: $t_1$, $x_2$: $t_2$, ...) -&gt; $t_r$:}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}</span></p>
<p><span class="math display">
\tag{App}
\frac{\Gamma \vdash f : t_1 \rightarrow t_2 ... \rightarrow t_r
\hspace{1cm} \Gamma \vdash x_1 : t_1, x_2 : t_2, ...}
     {\Gamma \vdash f(x_1, x_2, ...) : t_r}
</span></p>
<p><span class="math display">\begin{gather}
\Gamma \vdash x : \texttt{Optional[$t$]} \hspace{1cm}
\Gamma, x : t \vdash [p] \vartriangleright \text{ok} \nonumber\\
\tag{IfIn}
\bigotimes \text{in} [\texttt{is not, !=}]
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $x$:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
\hspace{1cm}
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $x$ $\bigotimes$ None:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}</span></p>
<p><span class="math display">
\tag{IfOut}
\Gamma \vdash
\left[\begin{array}{l}
p_1\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\hspace{1cm}
\Gamma \vdash
\left[\begin{array}{l}
p_2\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{if $e$:}\\
\quad p_1\\
\texttt{else:}\\
\quad p_2\\
p_3
\end{array}\right]
\vartriangleright \text{ok}
</span></p>
<p><span class="math display">
\tag{IfExpr}
\frac{\Gamma \vdash (e_b : \text{Optional[$t_0$]} \lor e_b :
\texttt{bool}), e_1 : t, e_2 : t}
     {\Gamma \vdash \texttt{$e_1$ if $e_b$ else $e_2$} : t}
</span></p>
<p><span class="math display">
\tag{For}
\Gamma \vdash y\texttt{ : List } t \hspace{1cm} \Gamma, x : t \vdash [p]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{for $x$ in  $y$:}\\
\quad p
\end{array}\right]
\vartriangleright \text{ok}
</span></p>
<p><span class="math display">
\tag{OpInt}
\Gamma \vdash x_1 : \texttt{int}, x_2 : \texttt{int}, \bigotimes \text{
in [\texttt{+, -, *, /}]}
\over
\Gamma \vdash x_1 \bigotimes x_2 : \texttt{int}
</span></p>
<p><span class="math display">
\hspace{-1.5cm}
\tag{OpString}
\frac{\Gamma \vdash s_1 : \texttt{str}, s_2 : \texttt{str}}
     {\Gamma \vdash s_1 + s_2 : \texttt{str}}
\hspace{1cm}
\frac{\Gamma \vdash s : \texttt{str}, x: \texttt{int}}
     {\Gamma \vdash s * x : \texttt{str}}
</span></p>
<p><span class="math display">
\hspace{-1.5cm}
\tag{OpEquality}
\frac{\Gamma \vdash x_1 : t, x_2 : t, \bigotimes \text{ in }
[\texttt{==, !=, is, is not}]}
     {\Gamma \vdash x_1 \bigotimes x_2 : \texttt{bool}}
</span></p>
<p><span class="math display">
\hspace{-2cm}
\tag{OpComparison}
\frac{\Gamma \vdash x_1 : \texttt{int}, x_2 : \texttt{int}, \bigotimes
\text{ in } [\texttt{&lt;, &gt;, &lt;=, &gt;=}]}
     {\Gamma \vdash x_1 \bigotimes x_2 : \texttt{bool}}
</span></p>
<p>With:</p>
<p><span class="math display">
\tag{ReturnReturn}
\frac{\Gamma \vdash e : t}{\Gamma \vdash \text{Returns(\texttt{return
$e$}, $t$)}}
</span> <span class="math display">
\tag{NoReturnExpression}
\frac{\Gamma \vdash e : t}{\vdash \text{NoReturn([$e$])}}
</span> <span class="math display">
\tag{NoReturnAssign}
\frac{}{\vdash \text{NoReturn([\texttt{$i$ = $e$}])}}
</span> <span class="math display">
\tag{ReturnIf}
\frac{\Gamma \vdash \text{Returns($p$, $t$)}}
     {\Gamma \vdash \text{Returns(}\left[\begin{array}{l}\texttt{if
$e$:}\\ \quad p\end{array}\right],t\text{)}}
\hspace{1cm}
</span></p>
<p><span class="math display">\begin{gather}
\Gamma \vdash \left(\text{Returns($p_1$, $t$)} \land
\text{Returns($p_2$, $t$)}\right)\lor \nonumber\\
\Gamma \vdash \left(\text{Returns($p_1$, $t$)} \land
\text{NoReturns($p_2$)}\right) \lor\nonumber\\
\tag{ReturnIfElse}
\Gamma \vdash \left(\text{NoReturns($p_1$)} \land \text{Returns($p_2$,
$t$)}\right)
\over
\Gamma \vdash \text{Returns(}\left[\begin{array}{l}
\texttt{if $e$:}\\
\quad p_1\\
\texttt{else:}\\
\quad p_2
\end{array}\right]\text{, $t$)}
\end{gather}</span></p>
<p><span class="math display">
\tag{ReturnFor}
\frac{\Gamma \vdash \text{Returns($p$, $t$)}}
     {\Gamma \vdash \text{Returns(}\left[\begin{array}{l}
     \texttt{for $x$ in $y$:}\\
     \quad p
     \end{array}\right]\text{, $t$)}}
</span></p>
<h2 class="unnumbered" id="sec:unannotatedfuncs">B.2 Unannotated
Functions</h2>
<p>The rules above are sufficient for our purposes for dealing with
fully type annotated programs. But what about programs with functions
that aren’t type annotated, or are only partially annotated? The mypy
type system that Python uses treats unannotated functions as dynamically
typed, allowing them to typecheck as any type. Similarly, function
parameters without a type annotation are implicitly convertable to any
type. So, for normaly mypy typechecking, we add the following typing
rules:</p>
<p><span class="math display">\begin{gather}
\Gamma, x_1 : Any, x_2 : Any, ... \vdash [b] \vartriangleright
\text{ok}\nonumber\\
\tag{DynDef}
\Gamma, f : Any \rightarrow ... \rightarrow Any \vdash [p]
\vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$, $x_2$, ...):}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}</span></p>
<p><span class="math display">
\tag{DynConvert}
\frac{\Gamma \vdash x : t}
     {\Gamma \vdash x : Any}
     \hspace{0.5cm}
\frac{\Gamma \vdash x : Any}
     {\Gamma \vdash x : t}
</span></p>
<p>These rules allow more python programs to check statically, but they
mean that some python programs will pass the static checks but still
throw type errors at runtime. Importantly for us, they can throw runtime
type errors about optionality. A well-written piece of code would not
just satisfy this type system then, but actually a stronger one that
would prevent runtime type errors. We’ll call this stronger type system
mypy++. Instead of the Dyn- rules, it has this one:</p>
<p><span class="math display">\begin{gather}
\Gamma, x_1 : t_1, x_2 : t_2, ... \vdash [b] \vartriangleright
\text{ok}\nonumber\\
\tag{InferDef}
\Gamma, f : t_1 \rightarrow t_2 \rightarrow ... \rightarrow t_r \vdash
[p] \vartriangleright \text{ok}
\over
\Gamma \vdash
\left[\begin{array}{l}
\texttt{def $f$($x_1$, $x_2$, ...):}\\
\quad b\\
p
\end{array}\right]
\vartriangleright \text{ok}
\end{gather}</span></p>
<h1 class="unnumbered"
id="detailed-high-level-nullability-test-results">C Detailed High-Level
Nullability Test Results</h1>
<h2 class="unnumbered" id="across-scale">C.1 Across Scale</h2>
<p>You can see in fig. <a href="#fig:hl_scale">2</a> that the smallest
three models don’t pass any test. After that, the trend line for tests
passed goes upwards, but it’s not monotonic: Pythia 410m passes only one
test, while Pythia 160m passes three, and Pythia 6.9b passes the most
tests, with 9, while Pythia 12b only passes 7. For instance, given the
partial code:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>Pythia 6.9b completes <code>process_value</code> as:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="bu">isinstance</span>(value, <span class="bu">str</span>):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(value)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  ...</span></code></pre></div>
<p>While Pythia 12b completes it as:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> value</span></code></pre></div>
<p>The code generated by Pythia 6.9b properly handles values of both
NoneType and str and always returns an int, whereas the code generated
by Pythia 12b is the identify function, returning None when the input is
None and returning a string when the input is a string. Neither of these
cases is correct, as the main function expects this to return a Number
that can be added to one.</p>
<h2 class="unnumbered" id="across-time">C.2 Across Time</h2>
<figure id="fig:hl_time">
<img src="images/hl_revision_results.svg"
alt="Figure 8: A line graph showing how the performance of the Pythia 6.9 model changes during training" />
<figcaption aria-hidden="true">Figure 8: A line graph showing how the
performance of the Pythia 6.9 model changes during training</figcaption>
</figure>
<p>We say in fig. <a href="#fig:hl_time">8</a> that while the model
generally gets better at passing these tests during training, the
performance is not always increasing. Zooming in on a particular test,
we can see what this looks like. When asked to complete this code:</p>
<p><em>Test 12</em></p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> guard:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_os(<span class="st">&quot;Foobar&quot;</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_os(value) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>))</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span></code></pre></div>
<p>Pythia 6.9b at training step 104000 produces the following definition
of <code>count_os</code>:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="bu">isinstance</span>(value, <span class="bu">str</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(value)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>...</span></code></pre></div>
<p>While 1000 steps later, it produces a very different definition:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">len</span>([s <span class="cf">for</span> s <span class="kw">in</span> os.listdir(os.path.join(os.getcwd(), value)) <span class="cf">if</span> s.start ...</span></code></pre></div>
<p>While the first definition handles value being None or a string, the
second definition not only assumes it is a string but that it is a name
of a folder in the current directory. In some sense the model “knows”
that the value parameter is nullable at step 104000, but “forgets” it
during further training. It regains the ability to pass the test at
several points during training, but by the end of training, it’s back to
treating <code>value</code> as if it was always a string.</p>
<h2 class="unnumbered" id="common-mistakes">C.3 Common Mistakes</h2>
<p>As expected, type annotations are particularly difficult for these
models. When asked to complete the type of get_square in the following
code, no model in the Pythia series can successfully output the Optional
argument type:</p>
<p><em>Test 5</em></p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  square <span class="op">=</span> get_square(number)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number:</span></code></pre></div>
<p>These models also find it much easier to deal with a list that
contains some None elements, than to deal with an atomic value that
might be None. Pythia 6.9b consistently passes this test in the second
half of training:</p>
<p><em>Test 2</em></p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(positive_numbers(some_numbers))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positive_numbers(numbers: <span class="bu">list</span>[Optional[<span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> num <span class="kw">in</span> numbers:</span></code></pre></div>
<p>But is much more challenged by this code:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>, intermittently being unable to complete it in a type-safe way up,
including in the second-to-last training step. When another level of
function indirection is added, the model becomes much better at
completing it correctly; Pythia 6.9b consistently completes the
following after step 93000:</p>
<p><em>Test 4</em></p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> guard:</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>The names of functions are highly influential in the models
understanding of their type semantics. When test 3 is changed so that
<code>process_value</code> is instead called <code>count_chars</code>,
Pythia 6.9b becomes unable to correctly complete it on any revision; it
likely has a bias from its training data that functions called
<code>count_chars</code> always take non-nullable strings. Similarly,
when test 4 is changed so that process_values becomes string_complexity,
it goes from consistently passing to almost always failing.</p>
<h1 class="unnumbered" id="mass-mean-probing-vs-linear-regression">D
Mass Mean Probing vs Linear Regression</h1>
<p>We were initially very surprised to find that mass means probing
would perform better than linear regression. After all, linear
regression is a much more powerful technique for fitting data. And mass
means probing can be seen as giving the direction of best fit in each
dimension independently, without considering other dimensions. The more
dimensions you consider at one time, the better your model fit can be.
But repeatedly in our data, we found that mass means probing
outperformed linear regression on the test data.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The loop indicates that the next few lines need to
process <code>num</code> in some way, and the fact that it comes from
<code>some_numbers</code> means it has the type
<code>Optional[int]</code>. <code>num</code> can’t be directly appended
to <code>result</code>, because <code>result</code> is declared to only
contain <code>int</code>s, not <code>Optional[int]</code>s. None of the
normal operations on <code>int</code>s will work on <code>None</code>s,
so before <code>num</code> can be processed, something has to be done to
separate <code>None</code>s and normal <code>int</code>s.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In particular, we can use this program to test models
for nullability understanding by asking them to complete the program’s
next lines, then see if those lines matches the regular expression
<code>num\s*(is\s*(not)?|==)\s*None|isinstance\(num</code>.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Note that these results differ substantially from those
of <span class="citation" data-cites="tigges24">Tigges et al.
(2024)</span>, who find that for <em>circuit</em> analyses (rather than
representational analyses like ours), circuit parts are learned at
roughly the same point during training across scale.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Technically this is known as “Optional typing”, but
that’s confusing in the context of this post. Not to be confused with
Gradual Typing, as introduced by Siek et al.<a href="#fnref4"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Despite this, it does pass the test 40% of the available
revisions, about triple what the other closest sizes can accomplish<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This indicates that if you were to train a model using
typechecking as reinforcement feedback, you would want to use mypy++ and
not mypy.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>We don’t see all six non-annotated function tests
passing under mypy, because models can still fail these tests by
producing invalid syntax.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>smoothed with a rolling average with a window size of
5<a href="#fnref8" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Since we don’t have contrasting pairs, just labeled
points, it’s not possible to use the PCA from contrasting pairs method
used in <span class="citation" data-cites="marks24">Marks and Tegmark
(2024)</span> and <span class="citation" data-cites="zou25">Zou et al.
(2025)</span>. See “Mass Mean Probing vs Linear Regression” in the
appendix<a href="#fnref9" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>This is, of course, where we trained our probes, but
there is also a practical reason: right after the model has generated a
variable that will be written to, it often does not have access to the
assigning expression or type annotation, giving it no way to determine
if the value will be optional or now.<a href="#fnref10"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Red tokens are significantly below the threshold, and
green tokens are significantly above it; tokens that scored near the
threshold would have a near-white color, but no such tokens appear in
this example.<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>When this variable appears for the first time, it is in
the <code>if</code> statement that checks if it’s <code>None</code>.
Then, the model knows it is nullable, and the results of the probe
reflect that understanding. But when it appears a second time on the
next line, in the format string of <code>print</code>, the body of this
if statement only runs if it is <em>not</em> <code>None</code>. The
model understand this as well, and the probe accurately reflects this.<a
href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p> <span class="citation" data-cites="li24">Li et al.
(2024)</span> and <span class="citation" data-cites="zou25">Zou et al.
(2025)</span> suggest that mass means probes are best for reading, while
the direction perpendicular to the separating hyperplane is best for
intervention. However, previous work leaves open the question of
cross-layer weights. We use LR on the cross-layer weights, thanks to our
investigations above.<a href="#fnref13" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
