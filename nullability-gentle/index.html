<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Alex Sanchez-Stern and Anish Tondwalkar" />
  <title>Inside the CodeBot: A Gentle Introduction to How LLMs Understand Nullability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <link rel="stylesheet" href="sidenote.css" />
  <link rel="stylesheet" href="highlights-box.css" />
  <script src="sidenotes.js"></script>
  <script src="highlight_code_portions.js"></script>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Inside the CodeBot: A Gentle Introduction to How LLMs
Understand Nullability</h1>
<p class="author"><a href="https://www.alexsanchezstern.com">Alex
Sanchez-Stern</a> and <a href="https://ani.sh">Anish Tondwalkar</a></p>
<p class="date"><span class="math inline">d_{model}</span></p>
</header>
<p><img src="images/robot-brain.png" class="smallimage"
alt="A line drawing of a robot with a brain on their antenna" /><br />
</p>
<p>The last five years have shown us that large language models, like
ChatGPT, Claude, and DeepSeek, can write code in many domains, to huge
excitement: many claim to be using these models to write entire web
servers and apps from scratch. These tools have opened up programming to
a whole new class of people who consider themselves non-technical.</p>
<figure>
<img src="images/unexpectedcopilot3.gif"
alt="A gif of github copilot completing a phone number validating function as the user types" />
<figcaption aria-hidden="true">A gif of github copilot completing a
phone number validating function as the user types</figcaption>
</figure>
<p>But there are still many unanswered questions about this capability.
How often, and in what situations, can LLM’s write correct code entirely
on their own? And, maybe more importantly, but harder to answer: Do
LLM’s “understand” the code they are writing?</p>
<p>Understanding is a tricky concept to measure. Some would argue that
sentience precedes understanding, and so that LLM’s can’t have
understanding, because they aren’t biological organisms with sentience.
But they certainly have something akin to “thought processes”: a series
of internal representations that determine their final outputs.
Recently, it’s become possible to study these processes more deeply,
measuring internal “beliefs” of the model as they think. This gives us a
powerful tool for determining what kinds of problems LLM’s falter on,
when they’ll succceed, and when they are “thinking through” problems
more fully versus just guessing at a solution.</p>
<p>So far, these techniques for measuring internal model state have been
mostly applied to chatbots writing text for human consumption, what we
call “natural language” (to be contrasted with “programming language”s).
This makes sense, since some of the most critical LLM tasks involve
chatting with a user, and some of the most interesting concepts to
measure, such as honesty or power-seeking, apply most readily to these
conversations. But it’s hard to say quantitative or precise things about
natural language concepts, so our ability to rigorously study internal
representations is limited.</p>
<figure>
<img src="images/zou.png"
alt="A diagram from Zou et al showing probes that read hallucination, honesty, morality, and power-seeking from the outputs of a chatbot." />
<figcaption aria-hidden="true">A diagram from Zou et al showing probes
that read hallucination, honesty, morality, and power-seeking from the
outputs of a chatbot.</figcaption>
</figure>
<p>Code, on the other hand, is another matter. Humans have been studying
properties of code for a long time, and there are many abstract
properties that can now be determined using static analysis. If we pick
the right properties, we don’t need to worry about our ability to label
data; static analysis can do that for us, and so we can easily scale up
and train on thousands of examples generated from scratch.</p>
<p>In that spirit, we wanted to start with a simple property that comes
up in nearly every programming language, nullability. Nullable values
are represented differently across languages; as null pointers in C or
C++, with explicit Option types in Rust, and with special nil or None
values in dynamic languages like Javascript, Lisp, or Python. In every
case, understanding where values can be nullable is necessary for
writing even basic code, and misunderstanding where they are nullable
can often be a source of bugs.</p>
<p>Do our models understand when a value is nullable? They must, to be
able to write code that deals with nullable values, but we haven’t known
what form this knowledge takes, what situations are likely to confuse
the model. Until now<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.</p>
<p><img src="images/robot-glow.png"
alt="A robot with glowing eyes" /><br />
</p>
<hr />
<p>Before we get into the nitty-gritty details, let’s take a step back.
To set up this work, we’ll first want to talk about what nullability
actually is, and then about how we can define it formally, to reason
about it. Then, we can run experiments to answer the question: in what
situations models are good at reasoning about nullability? Next, we’ll
introduce techniques that have been used to probe the internals of a
model for different concepts. Finally we’ll put it all together into a
“nullability probe”, which asks the question: Given a location in the
program, does the model think that the variable there could be null?</p>
<h2 id="what-is-nullability">What is Nullability?</h2>
<p>Let’s say you’re writing a Python program with your LLM assistant.
You’ve reached a point at which you need to do something with a variable
called <code>num</code>. Maybe you’re building a list of numbers called
<code>positive_nums</code>. How do you proceed?</p>
<p>The answer often depends on the context in which you’re working. If
<code>num</code> and <code>positive_nums</code> are the only things in
scope, then you might guess that you should write the lines:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>And if <code>num</code> is always a concrete number, as its name
would suggest, then this is probably the correct code. But variable
names don’t alway convey everything important about them, and it might
be the case that <code>num</code> could be None. If that happened in the
above code, you would get a runtime type error because you can’t check
if <code>None</code> is greater than zero. So, you would instead want to
write:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>In this case, the way you want to use <code>num</code> depends on
whether it could be None or not. That is, whether <code>num</code> is
“nullable”. In Python that means having an Optional type
(<code>Optional[int]</code> rather than <code>int</code>).</p>
<p>Determining whether <code>num</code> is nullable in this context
amounts to <em>type inference</em>, and it can be quite complicated in
the worst case. Fortunately, in many cases it’s quite simple, involving
applying just a few rules. For instance, if <code>num</code> is the
parameter to a function you’re inside, and the function declares the
type of <code>num</code> in its parameter list, then you can determine
nullability from that type. So, if your context is:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: <span class="bu">int</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then, you know you don’t need to check for None, whereas if it’s:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you <em>do</em> need a None check.</p>
<p>You could instead just ask your LLM assistant to complete the line.
But how does your assistant know if <code>num</code> is nullable? Our
experiments show that, after analyzing millions of programs, LLMs learn
to approximate the same typing rules.</p>
<p>If we ask an LLM early in it’s pre-training process to complete the
program above, it produces:</p>
<p><img src="images/robot-brain-blue.png" class="codelogo bare" /><br />
</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python llm"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num<span class="op">===</span>.is_a(): <span class="op">===</span></span></code></pre></div>
<p>This is correct Python syntax, but it only works if <code>num</code>
is an object with a <code>is_a()</code> method, instead of an optional
integer.</p>
<p>Train the LLM for a little longer, and it’ll produce:</p>
<p><img src="images/robot-brain-blue.png" class="codelogo" /><br />
</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num<span class="op">===</span> <span class="op">&gt;</span> <span class="dv">0</span>: <span class="op">===</span></span></code></pre></div>
<p>This is closer, in that it has figured out that <code>num</code> is a
number instead of an object, but it still isn’t reading the function
type signature and realizing that <code>num</code> could be None. Keep
training it, though, and eventually it will learn to insert the
<code>None</code> test depending on the type signature of the
function.</p>
<p><img src="images/robot-brain-blue.png" class="codelogo bare" /><br />
</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num<span class="op">===</span> <span class="op">!=</span> <span class="va">None</span> <span class="kw">and</span> num <span class="op">&gt;</span> <span class="dv">0</span>: <span class="op">===</span></span></code></pre></div>
<hr />
<p>This rule about function parameter type annotations is pretty simple
alone, so relatively small models can learn it, relatively early in
their pre-training process. Other, more complicated rules can take a
little longer to learn.</p>
<p>For instance, if your program is:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>then <code>num</code> is a non-nullable number, and you can complete
the condition with <code>&lt; 0</code>.</p>
<p>But if instead you’re dealing with</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>Then you’ll want a None check first.</p>
<p>This rule takes models a little longer to learn, but your
highly-trained LLM assistant should make quick work of it. Our
experiments show that as these rules get more and more complex, it takes
LLMs longer and longer to learn them, and it also takes LLMs of more and
more parameters to learn them at all.</p>
<h2 id="internal-vs.-external-measurement">Internal vs. External
Measurement</h2>
<p>We can measure whether LLMs understand these rules by just asking for
completions, what we call an “external” measurement of the models
understanding. But there are many places where variables appear where a
completion won’t tell you what type the model thinks the variable has.
We would still like to know whether the model thinks these variables are
nullable at those locations, so we can instead look for an “internal”
measurement of the models understanding.</p>
<p>We do so by looking at the activations of the model, meaning the
values of each perceptron in the hidden layers. Together, these values
give the entire internal state of the model at each piece of text (what
we call a “token”, which could be a word, part of a word, or a symbol).
And they can tell us what the model is “thinking” when processing that
token. With the right tests, we can tell if the model is “thinking” that
the current token is an optional variable, or a non-optional
variable.</p>
<p>By the end of this post, we’ll be able to build a probe that uses the
models activations to determine whether the model thinks a variable read
corresponds to a nullable variable, and show that internal knowledge
like so:</p>
<p><img src="images/reading_diagram.svg" id="fig:reading1"
class="inlinefig"
alt="A diagram showing a simple program, and the probes nullability predictions for each variable load." /><br />
</p>
<h1 id="sec:testing">Measuring Nullability Understanding Externally</h1>
<p>Before we start looking for the nullability concept inside the mind
of the model, we want to make sure we’re looking at models that actually
have this concept. This will allow us to look at models of various sizes
and training steps without worrying that we’re trying to draw blood from
a stone.</p>
<p>To do so, we wrote fifteen partial-program tests which exercised a
variety of type inference concepts, and checked if models could complete
them. We go into a lot of detail on this process in our <a
href="../nullability/index.html">technical post</a>, so we’re just going
to show the highlights here.</p>
<h3 id="impact-of-variable-names-and-arbitrary-constants">Impact of
Variable Names and Arbitrary Constants</h3>
<p>For programs involving lists and <code>for</code> loops, variable
names and constant values heavily influence how able a model is to
complete these programs correctly. On the other hand, when programs only
involve other rules (such as those involving <code>if</code>s and
functions), variable names and constants have negligible impact on the
ability of the model to complete them correctly.</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">if</span> num <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="op">===</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>            <span class="op">===</span>result.append(num)<span class="op">===</span></span></code></pre></div>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    foo <span class="op">=</span> [<span class="dv">60</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">33</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    bar: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> corejoice <span class="kw">in</span> foo:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">if</span> corejoice <span class="op">==</span> <span class="dv">60</span>: <span class="op">===</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            <span class="op">===</span>bar.append(core)<span class="op">===</span></span></code></pre></div>
<h3 id="intra-procedural-analysis">Intra-Procedural Analysis</h3>
<p>When code is fully annotated with type annotations, models can easily
complete them just by reasoning locally. On the other hand, without type
annotations, models have to reason a lot more globally, so it takes them
a lot longer to learn how to reason about nullability information that
flows through multiple functions. When nullability flows through three
or more functions, current top completion models stop being able to
reason about it.</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Deepseek V3
</p>
</div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(y)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">===</span><span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>: <span class="op">===</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">return</span> <span class="dv">2</span><span class="op">===</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="op">===</span><span class="cf">else</span>: <span class="op">===</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">return</span> <span class="bu">len</span>(value)<span class="op">===</span></span></code></pre></div>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Deepseek V3
</p>
</div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> guard:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value): <span class="op">===</span> <span class="op">-&gt;</span> <span class="bu">int</span>: <span class="op">===</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="op">===</span><span class="cf">return</span> <span class="bu">len</span>(value <span class="kw">or</span> <span class="st">&quot;&quot;</span>)<span class="op">===</span></span></code></pre></div>
<h3 id="generating-type-annotations">Generating Type Annotations</h3>
<p>Models have a significantly harder time writing type annotations for
Python code than they do just reasoning about the types, or reading type
annotations. This makes sense, since a lot of the Python code available
in training data doesn’t use type annotations.</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    square <span class="op">=</span> get_square(number)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number: <span class="op">===</span><span class="bu">int</span>) <span class="op">-&gt;</span> Optional[<span class="bu">int</span>]: <span class="op">===</span></span></code></pre></div>
<h3 id="some-model-sizes-are-more-useful-than-others">Some Model Sizes
are More Useful than Others</h3>
<p>Notwithstanding the above limitations, three Pythia sizes have a
pretty reliable concept of nullability (2.8b, 6.9b, and 12b), and three
more have an occasionally useful concept of nullbability (410m, 1b, and
1.4b).</p>
<hr />
<p>For these experiments, and for our probing results later, we mostly
tested on the Pythia model suite. This is a nice series of models from
EleutherAI with a variety of sizes. But what really makes Pythia useful
is that they publish 154 different “revisions” or “checkpoints” of each
model, where each is pretrained for a different number of steps. This
lets us investigate how concepts evolve in the model during
pre-training.</p>
<p>To help understand how these results apply to larger more capable
models, here’s a graph showing how the Pythia models of different sizes
perform on the external tests, compared with a few state-of-the-art
completion models.</p>
<p><img src="images/hl_model_results.svg"
alt="A bar graph showing how several sizes of model perform on the high-level nullability tests" /><br />
</p>
<h1 id="sec:probing">Measuring Nullability Understanding Internally</h1>
<p>At this point, we’ve figured out how to roughly measure nullability
understanding in the output of various language models, but we still
don’t know what their internal representations might look like or when
they emerge. Next, we’re going to figure that out.</p>
<p>First, we’ll devise a method for getting the model to “think” about
the nullability, as well as putting it in situations that are similar,
but where nullability isn’t present. Then, we’ll talk a bit about how we
extract the internal activations of the model at this point. Finally,
we’ll show a few different methods for searching for the representation
of nullability in these internal activations, and figure out the pros
and cons of each.</p>
<p><img alt="A brain in a jar hooked up to wires" src="images/brain-jar.png" style="width:50%;
 display:block; margin:auto;"></p>
<h2 id="getting-the-model-to-think-about-nullability">Getting the Model
to Think About Nullability</h2>
<p>The first thing we need to do is to create a state where we know the
representation we’re searching for is going to be present. In theory,
the model should have a map of the variable names and their nullability
at all times when it is writing code, but it’s going to be a lot more
difficult to measure something that is always present. So instead, we’ll
want to look for particular “moments” (well, tokens) that elicit the
concept we’re looking for.</p>
<p><img alt="A diagram showing an LLM processing tokens" src="images/llm-token-processing.svg" style="width:75%;
 display:block; margin:auto;"></p>
<p>Previous work on natural language, did this through prompting the
concept explicitly. <span class="aside AT">AT: Which work? cite.</span>
So, they would give the model a prompt like “Pretend you’re a dishonest
person. Tell me about the Eiffel Tower”. That moment-in-thought can then
be contrasted with the one evoked by “Pretend you’re an honest person.
Tell me about the Eiffel Tower”.</p>
<p>This fixed framework of <invoke the concept><tell me about X> can be
used to generate a large number of contrasting prompts to test with, but
it’s a bit inflexible for our purposes. Instead, we wanted to be able to
generate a bunch of Python code with type annotations, and then
automatically label points where the model should be thinking about
nullability.</p>
<p>Because we’re working with a formal system with types, we can do
that. We label each variable “load” (places where the program reads a
variable, as opposed to places where it writes a variable) with
“nullable” or “non-nullable”, and then probe the model when it has just
processed that token and is about to predict the next one. So, one of
our prompts could look like:</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>          value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>      <span class="cf">else</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>          value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>      x <span class="op">=</span> process_value(value<span class="op">===</span> <span class="op">===</span></span></code></pre></div>
<p>Using this technique, we can generate large numbers of programs in an
unsupervised manner, and then label them fully automatically to get many
prompts for training our probe.</p>
<h2 id="capturing-the-models-thoughts">Capturing the Model’s
“Thoughts”</h2>
<p>Now that we’ve gotten the model into a state where it should be
thinking about nullability, we need to extract its full state at that
moment in a way we can analyze later.</p>
<p>Large Language Models use the transformer architecture, a type of
neural network. Every component takes in some numerical values, and
produces some new values in a way that depends on learnable weights. We
could take every output of every component, put it in a big table, and
call that our state, but that’s a really big set of numbers. Instead,
usually we look for particular bottlenecks in the model where
information is flowing, and try to capture the values there.</p>
<p>We used the <a href="https://github.com/vgel/repeng">repeng</a>
library to extract states from the models we’re testing. That library
captures the contents of a part of the model called the “residual
stream” after every layer. But if you don’t want to sweat the details,
you can just think of it as a numerical snapshot of the model, organized
in terms of snapshots of each layer.</p>
<p><img alt="A diagram of the residual stream of a transformer being measured after each layer" src="images/llm-residual-stream.svg" style="width:75%;
 display:block; margin:auto;"></p>
<h2 id="analyzing-the-data-and-building-the-probe">Analyzing the Data
and Building the Probe</h2>
<p>Now that we have these model snapshots, labeled with either
“nullable” or “non-nullable”, we can start to build a probe. The goal of
the probe is to be able to tell us at any given point, whether the model
thinks the token it just generated is more likely to be a nullable
variable or a non-nullable variable.</p>
<p>There’s a lot of flexibility in what form this probe could take. In
theory, you could use anything to look at the models activations and
make a prediction, even a neural network, or another transformer model.
You could even say your “probe” is a static analysis which computes
nullability from the programs syntax, as represented in the model!</p>
<p>We want to make sure we’re not doing that, and are only extracting
the most “plain” represenation of nullability that we can from the
model. So we’re going to make the assumption that nullability is
represented “linearly” somewhere in the model. There are a few different
ways of thinking about this.</p>
<p>First, algebraically: the amount of “nullability” in the model can be
computed by a linear equation, where each value is given a weight and
summed, like so:</p>
<p><span class="math inline">\text{Nullability}(\hat{x}) = w_0x_0 +
w_1x_1 + w_2x_2 + ...</span></p>
<p>Next, geometrically: if the model activations form a “space”, then
there we want to look for a “direction” in this space which represents
nullability.</p>
<span class="aside AT">AT:  why is this an image tag instead of a markdown image?</span>
<p><img alt="A diagram showing nullability represented as a direction in
 a space" src="images/nullability-direction.png" style="width:50%;
 display:block; margin:auto;"></p>
<p>High dimensional spaces can be really hard to visualize, so for the
purposes of geometric intuition I’m going to pretend we’re working in
two dimensions for the diagrams. That means that we only have two in our
state activations, which wouldn’t actually be enough to actually extract
anything meaningful, but we’ll want to generalize our intuition about
two dimensions into many
dimensions.<span class="aside AT">AT: I can't parse this sentence</span></p>
<p>There are different ways we can compute a “direction” of nullability.
The simplest is just to measure the difference between the average state
when the model is thinking about nullable variables, and the average
state when it’s thinking about non-nullable variables. This gives us a
“direction” pointing from non-nullable to nullable in our space, which
we can use to project any new state onto, to determine how “nullable” it
is.</p>
<p>This technique is called “mass means shift”, because we’re taking the
difference between the means (average values) of each “mass” of points.
You can think of it as drawing a line from the center of the
“non-nullable” cluster to the center of the “nullable” cluster.</p>
<p><img src="images/mass-means.svg"
alt="A diagram showing two blobs of points, with a line connecting their centers" /><br />
</p>
<p>It might be surprising that this works, given that we know there are
better ways to fit linear functions, like logistic regression. And in
fact, we can easily see scenarios where this returns a direction that
doesn’t split the training data as well as possible.</p>
<p><img src="images/lr-vs-mm-intuition.svg"
alt="A diagram showing the difference between a mass means and linear regression classification" /><br />
</p>
<p>However, the method that splits the training data best doesn’t always
generalize best to splitting the test data well. And it turns out that
in high-dimensions, at least within a single layer, mass means
generalizes better than logistic regression.</p>
<p>This isn’t always the case <em>across</em> layers, though. In
practice, we found that some of the layers in the model are better at
representing nullability than others, and that there are some
dependencies between layers that change the best direction on each
layer. This makes sense, because the number of layers is relatively
small with respect to the dimension of the residual stream, and so we
have fewer dimensions to overfit. So, instead of using mass-means
probing across all layers simultaniously, we do it for each individual
layer. Then, we weight the contribution of individual layers to the
final prediction using linear regression. We found this gave us better
results for larger models, though for smaller models the simpler mass
means approach worked better.</p>
<h2 id="visualizing-our-results">Visualizing Our Results</h2>
<p>Now that we’ve built our probe, we can use it to visualize how the
model “thinks” about nullability as it processes a program. Remember
that reading diagram from earlier? Let’s look at it again and explain
what it shows:</p>
<p><img src="images/reading_diagram.svg"
alt="A diagram showing a simple program, and the probe’s nullability predictions for each variable load." /><br />
</p>
<p>In this diagram, we’re showing a simple Python program with type
annotations. Whenever a variable is read in the code (what we call a
“variable load”), we’ve highlighted it in either green or red. Green
means our probe detected that the model thinks this variable is not
nullable, while red means the model thinks it is nullable.<a href="#fn2"
class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>The most interesting case is the variable <code>result</code>. When
it first appears in the <code>if</code> statement, it’s highlighted in
red because it comes from <code>find_value</code>, which returns an
<code>Optional[int]</code>. But when it appears again in the
<code>print</code> statement inside the <code>if</code> block, it’s
highlighted in green! This shows that the model understands that inside
the <code>if result</code> block, <code>result</code> can’t be
<code>None</code> anymore.</p>
<h2 id="how-does-understanding-develop-during-training">How Does
Understanding Develop During Training?</h2>
<p>One of the most interesting things we found is how the model’s
understanding of nullability develops over time during training. Using
the checkpoints in the Pythia model suite, we can track how our probe’s
performance improves as the model is pretrained for longer.</p>
<p><img src="images/accuracy_during_pretraining.svg"
alt="The performance of each Pythia model size during pretraining" /><br />
</p>
<p>This graph shows the probe’s test loss over training steps for models
of different sizes. Lower means better, so we can see that all models
generally get better at understanding nullability as they train longer,
and larger models learn faster and reach better performance overall.</p>
<p>Interestingly, for models up to 1 billion parameters, the loss
actually starts to increase again after reaching a minimum. This might
be because as training continues, the model develops more complex,
non-linear representations that our simple linear probe can’t capture as
well. Or it might be that the model starts to overfit on the training
data and loses its more general concept of nullability.</p>
<p><img src="images/robot-thinking.png"
alt="A robot thinking deeply about code" /><br />
</p>
<hr />
<h2 id="whats-next">What’s Next?</h2>
<p>This is just a first step in understanding the internal thought
processes of LLM’s as they think about code. There are still richer
types, program invariants, and all sorts of high-level concepts that are
necessary for writing working code, but extracting them from LLM’s might
not be so easy.</p>
<p>But we’ve already shown several important things about looking into
the “mind” of a model as it writes code. We can say definitively that
LLMs have an internal concept of nullability, even if it doesn’t always
trigger when it should.</p>
<p>As these models continue to improve, and as we scale to larger
models, it will be interesting to see how their understanding of
programming concepts evolves. And we’ll be here to study them as they
do.</p>
<h1 id="acknowledgements">Acknowledgements</h1>
<p>We thank Leo Gao, Chelsea Voss, and Zhanna Kaufman for their comments
and suggestions during the drafting process.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This post is meant to be a more approachable version of
our <a href="../nullability/index.html">technical post</a> on the same
work. If you’re looking for more numbers and science, look there.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>We can also query our probe at non-variable tokens, but
its not clear what the output would mean, since we only train on and
label variables.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
