@misc{baez24,
  title = {What Is {{Entropy}}?},
  author = {Baez, John C.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.09232},
  eprint = {2409.09232},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.09232},
  urldate = {2025-02-07},
  abstract = {This short book is an elementary course on entropy, leading up to a calculation of the entropy of hydrogen gas at standard temperature and pressure. Topics covered include information, Shannon entropy and Gibbs entropy, the principle of maximum entropy, the Boltzmann distribution, temperature and coolness, the relation between entropy, expected energy and temperature, the equipartition theorem, the partition function, the relation between expected energy, free energy and entropy, the entropy of a classical harmonic oscillator, the entropy of a classical particle in a box, and the entropy of a classical ideal gas.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Mathematical Physics},
  file = {/home/atondwal/Zotero/storage/F7UHG6F4/Baez - 2024 - What is Entropy.pdf}
}

@misc{biderman23,
  title = {Pythia: {{A Suite}} for {{Analyzing Large Language Models Across Training}} and {{Scaling}}},
  shorttitle = {Pythia},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and van der Wal, Oskar},
  year = {2023},
  month = may,
  number = {arXiv:2304.01373},
  eprint = {2304.01373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.01373},
  urldate = {2025-02-17},
  abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at {\textbackslash}url\{https://github.com/EleutherAI/pythia\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/atondwal/Zotero/storage/ZFXTZYPX/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Models Across Training and Scaling.pdf;/home/atondwal/Zotero/storage/VFRU5R73/2304.html}
}

@misc{chen23,
  title = {Dynamical versus {{Bayesian Phase Transitions}} in a {{Toy Model}} of {{Superposition}}},
  author = {Chen, Zhongtian and Lau, Edmund and Mendel, Jake and Wei, Susan and Murfet, Daniel},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06301},
  eprint = {2310.06301},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06301},
  urldate = {2025-02-07},
  abstract = {We investigate phase transitions in a Toy Model of Superposition (TMS) (Elhage et al., 2022) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular k-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these k-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same k-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/N8CHVX66/Chen et al. - 2023 - Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition.pdf}
}

@misc{davies23,
  title = {Unifying {{Grokking}} and {{Double Descent}}},
  author = {Davies, Xander and Langosco, Lauro and Krueger, David},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06173},
  eprint = {2303.06173},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06173},
  urldate = {2025-02-07},
  abstract = {A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied grokking, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar double descent. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/ES9RRJJ2/Davies et al. - 2023 - Unifying Grokking and Double Descent.pdf}
}

@misc{everett24,
  title = {Scaling {{Exponents Across Parameterizations}} and {{Optimizers}}},
  author = {Everett, Katie and Xiao, Lechao and Wortsman, Mitchell and Alemi, Alexander A. and Novak, Roman and Liu, Peter J. and Gur, Izzeddin and {Sohl-Dickstein}, Jascha and Kaelbling, Leslie Pack and Lee, Jaehoon and Pennington, Jeffrey},
  year = {2024},
  month = jul,
  number = {arXiv:2407.05872},
  eprint = {2407.05872},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.05872},
  urldate = {2025-02-07},
  abstract = {Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes tens of thousands of models trained with all combinations of three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 26.8B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose Adamatan2, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/VLA7CPBX/Everett et al. - 2024 - Scaling Exponents Across Parameterizations and Optimizers.pdf}
}

@misc{feng24binding,
  title = {How Do {{Language Models Bind Entities}} in {{Context}}?},
  author = {Feng, Jiahai and Steinhardt, Jacob},
  year = {2024},
  month = may,
  number = {arXiv:2310.17191},
  eprint = {2310.17191},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17191},
  urldate = {2025-02-07},
  abstract = {To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a ``green square'' and a ``blue circle'', LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {slug:binding},
  file = {/home/atondwal/Zotero/storage/W5VQGY8I/Feng and Steinhardt - 2024 - How do Language Models Bind Entities in Context.pdf}
}

@misc{feng24predicate,
  title = {Monitoring {{Latent World States}} in {{Language Models}} with {{Propositional Probes}}},
  author = {Feng, Jiahai and Russell, Stuart and Steinhardt, Jacob},
  year = {2024},
  month = dec,
  number = {arXiv:2406.19501},
  eprint = {2406.19501},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.19501},
  urldate = {2025-02-07},
  abstract = {Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input contexts in a latent world model, and we seek to extract these latent world states as logical propositions. For example, given the input context ``Greg is a nurse. Laura is a physicist.'', we aim to decode the propositions WorksAs(Greg, nurse) and WorksAs(Laura, physicist) from the model's internal activations. To do so we introduce propositional probes, which compositionally extract lexical concepts from token activations and bind them into propositions. Key to this is identifying a binding subspace in which bound tokens have high similarity (Greg {$\leftrightarrow$} nurse) but unbound ones do not (Greg ̸{$\leftrightarrow$} physicist). Despite only being trained on linguistically simple English templates, we find that propositional probes generalize to inputs written as short stories and translated to Spanish. Moreover, in three settings where LMs respond unfaithfully to the input context---prompt injections, backdoor attacks, and gender bias--- the decoded propositions remain faithful. This suggests that LMs often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {slug:predicate},
  file = {/home/atondwal/Zotero/storage/2TNNDJ6B/Feng et al. - 2024 - Monitoring Latent World States in Language Models with Propositional Probes.pdf}
}

@misc{frankle19,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  number = {arXiv:1803.03635},
  eprint = {1803.03635},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.03635},
  urldate = {2025-02-07},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/atondwal/Zotero/storage/WISI2RDU/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks.pdf}
}

@misc{frigg08,
  title = {A {{Field Guide}} to {{Recent Work}} on the {{Foundations}} of {{Statistical Mechanics}}},
  author = {Frigg, Roman},
  year = {2008},
  month = apr,
  number = {arXiv:0804.0399},
  eprint = {0804.0399},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0804.0399},
  urldate = {2025-02-07},
  abstract = {This is an extensive review of recent work on the foundations of statistical mechanics. Subject matters discussed include: interpretation of probability, typicality, recurrence, reversibility, ergodicity, mixing, coarse graining, past hypothesis, reductionism, phase average, thermodynamic limit, interventionism, entropy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics},
  file = {/home/atondwal/Zotero/storage/ENYKIZ6N/Frigg - 2008 - A Field Guide to Recent Work on the Foundations of Statistical Mechanics.pdf}
}

@misc{gorton24,
  title = {Group {{Crosscoders}} for {{Mechanistic Analysis}} of {{Symmetry}}},
  author = {Gorton, Liv},
  year = {2024},
  month = nov,
  number = {arXiv:2410.24184},
  eprint = {2410.24184},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.24184},
  urldate = {2025-02-07},
  abstract = {We introduce group crosscoders, an extension of crosscoders that systematically discover and analyse symmetrical features in neural networks. While neural networks often develop equivariant representations without explicit architectural constraints, understanding these emergent symmetries has traditionally relied on manual analysis. Group crosscoders automate this process by performing dictionary learning across transformed versions of inputs under a symmetry group. Applied to InceptionV1's mixed3b layer using the dihedral group D32, our method reveals several key insights: First, it naturally clusters features into interpretable families that correspond to previously hypothesised feature types, providing more precise separation than standard sparse autoencoders. Second, our transform block analysis enables the automatic characterisation of feature symmetries, revealing how different geometric features (such as curves versus lines) exhibit distinct patterns of invariance and equivariance. These results demonstrate that group crosscoders can provide systematic insights into how neural networks represent symmetry, offering a promising new tool for mechanistic interpretability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/LJ3PLWY5/Gorton - 2024 - Group Crosscoders for Mechanistic Analysis of Symmetry.pdf}
}

@misc{hanna23,
  title = {How Does {{GPT-2}} Compute Greater-than?: {{Interpreting}} Mathematical Abilities in a Pre-Trained Language Model},
  shorttitle = {How Does {{GPT-2}} Compute Greater-Than?},
  author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  year = {2023},
  month = nov,
  number = {arXiv:2305.00586},
  eprint = {2305.00586},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.00586},
  urldate = {2025-03-06},
  abstract = {Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years {$>$} 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we find related tasks that activate our circuit. Our results suggest that GPT-2 small computes greater-than using a complex but general mechanism that activates across diverse contexts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/IKT74TKA/Hanna et al. - 2023 - How does GPT-2 compute greater-than Interpreting mathematical abilities in a pre-trained language.pdf;/home/atondwal/Zotero/storage/Q9CCTKKT/2305.html}
}

@misc{huh24,
  title = {The {{Platonic Representation Hypothesis}}},
  author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  year = {2024},
  month = jul,
  number = {arXiv:2405.07987},
  eprint = {2405.07987},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.07987},
  urldate = {2025-02-07},
  abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/atondwal/Zotero/storage/CH37FDRJ/Huh et al. - 2024 - The Platonic Representation Hypothesis.pdf}
}

@misc{kamath23,
  title = {Finding {{Inductive Loop Invariants}} Using {{Large Language Models}}},
  author = {Kamath, Adharsh and Senthilnathan, Aditya and Chakraborty, Saikat and Deligiannis, Pantazis and Lahiri, Shuvendu K. and Lal, Akash and Rastogi, Aseem and Roy, Subhajit and Sharma, Rahul},
  year = {2023},
  month = nov,
  number = {arXiv:2311.07948},
  eprint = {2311.07948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.07948},
  urldate = {2025-02-07},
  abstract = {Loop invariants are fundamental to reasoning about programs with loops. They establish properties about a given loop's behavior. When they additionally are inductive, they become useful for the task of formal verification that seeks to establish strong mathematical guarantees about program's runtime behavior. The inductiveness ensures that the invariants can be checked locally without consulting the entire program, thus are indispensable artifacts in a formal proof of correctness. Finding inductive loop invariants is an undecidable problem, and despite a long history of research towards practical solutions, it remains far from a solved problem. This paper investigates the capabilities of the Large Language Models (LLMs) in offering a new solution towards this old, yet important problem. To that end, we first curate a dataset of verification problems on programs with loops. Next, we design a prompt for exploiting LLMs, obtaining inductive loop invariants, that are checked for correctness using sound symbolic tools. Finally, we explore the effectiveness of using an efficient combination of a symbolic tool and an LLM on our dataset and compare it against a purely symbolic baseline. Our results demonstrate that LLMs can help improve the state-of-the-art in automated program verification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/atondwal/Zotero/storage/35YJZS69/Kamath et al. - 2023 - Finding Inductive Loop Invariants using Large Language Models.pdf}
}

@inproceedings{li21,
  title = {Implicit {{Representations}} of {{Meaning}} in {{Neural Language Models}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
  year = {2021},
  pages = {1813--1827},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.143},
  urldate = {2025-02-07},
  langid = {english},
  file = {/home/atondwal/Zotero/storage/VZKLSKMQ/Li et al. - 2021 - Implicit Representations of Meaning in Neural Language Models.pdf}
}

@misc{li24,
  title = {Inference-{{Time Intervention}}: {{Eliciting Truthful Answers}} from a {{Language Model}}},
  shorttitle = {Inference-{{Time Intervention}}},
  author = {Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  year = {2024},
  month = jun,
  number = {arXiv:2306.03341},
  eprint = {2306.03341},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03341},
  urldate = {2025-02-17},
  abstract = {We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5\% to 65.1\%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/KE6CGKIJ/Li et al. - 2024 - Inference-Time Intervention Eliciting Truthful Answers from a Language Model.pdf;/home/atondwal/Zotero/storage/YGXVKHNZ/2306.html}
}

@misc{linzen16,
  title = {Assessing the {{Ability}} of {{LSTMs}} to {{Learn Syntax-Sensitive Dependencies}}},
  author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  year = {2016},
  month = nov,
  number = {arXiv:1611.01368},
  eprint = {1611.01368},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.01368},
  urldate = {2025-03-06},
  abstract = {The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1\% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/atondwal/Zotero/storage/FY2EKZWV/Linzen et al. - 2016 - Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies.pdf;/home/atondwal/Zotero/storage/W9QTQ6T3/1611.html}
}

@misc{liu24,
  title = {A {{Survey}} of {{Lottery Ticket Hypothesis}}},
  author = {Liu, Bohan and Zhang, Zijie and He, Peixiong and Wang, Zhensen and Xiao, Yang and Ye, Ruimeng and Zhou, Yang and Ku, Wei-Shinn and Hui, Bo},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04861},
  eprint = {2403.04861},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.04861},
  urldate = {2025-02-07},
  abstract = {The Lottery Ticket Hypothesis (LTH) states that a dense neural network model contains a highly sparse subnetwork (i.e., winning tickets) that can achieve even better performance than the original model when trained in isolation. While LTH has been proved both empirically and theoretically in many works, there still are some open issues, such as efficiency and scalability, to be addressed. Also, the lack of open-source frameworks and consensual experimental setting poses a challenge to future research on LTH. We, for the first time, examine previous research and studies on LTH from different perspectives. We also discuss issues in existing works and list potential directions for further exploration. This survey aims to provide an in-depth look at the state of LTH and develop a duly maintained platform to conduct experiments and compare with the most updated baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/atondwal/Zotero/storage/F7RKL9G3/Liu et al. - 2024 - A Survey of Lottery Ticket Hypothesis.pdf}
}

@article{mathwin,
  title = {Identifying a {{Preliminary Circuit}} for {{Predicting Gendered Pronouns}} in {{GPT-2 Small}}},
  author = {Mathwin, Chris and Corlouer, Guillaume and Kran, Esben and Barez, Fazl and Nanda, Neel},
  abstract = {We identify the broad structure of a circuit that is associated with correctly predicting a gendered pronoun given the subject of a rhetorical question. Progress towards identifying this circuit is achieved through a variety of existing tools, namely Conmy's Automatic Circuit Discovery and Nanda's Exploratory Analysis tools.},
  langid = {english},
  file = {/home/atondwal/Zotero/storage/3BMCV4Q8/Mathwin et al. - Identifying a Preliminary Circuit for Predicting Gendered Pronouns in GPT-2 Small.pdf}
}

@misc{mcinnes20,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  number = {arXiv:1802.03426},
  eprint = {1802.03426},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.03426},
  urldate = {2025-02-07},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that is applicable to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/atondwal/Zotero/storage/ALP3XJ85/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf}
}

@misc{nguyen24,
  title = {Differential Learning Kinetics Govern the Transition from Memorization to Generalization during In-Context Learning},
  author = {Nguyen, Alex and Reddy, Gautam},
  year = {2024},
  month = dec,
  number = {arXiv:2412.00104},
  eprint = {2412.00104},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.00104},
  urldate = {2025-02-07},
  abstract = {Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative rates at which these subcircuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition},
  file = {/home/atondwal/Zotero/storage/I33FSWGD/Nguyen and Reddy - 2024 - Differential learning kinetics govern the transition from memorization to generalization during in-c.pdf}
}

@misc{singh24,
  title = {Beyond {{Human Data}}: {{Scaling Self-Training}} for {{Problem-Solving}} with {{Language Models}}},
  shorttitle = {Beyond {{Human Data}}},
  author = {Singh, Avi and {Co-Reyes}, John D. and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Garcia, Xavier and Liu, Peter J. and Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi, Aaron and Kumar, Abhishek and Alemi, Alex and Rizkowsky, Alex and Nova, Azade and Adlam, Ben and Bohnet, Bernd and Elsayed, Gamaleldin and Sedghi, Hanie and Mordatch, Igor and Simpson, Isabelle and Gur, Izzeddin and Snoek, Jasper and Pennington, Jeffrey and Hron, Jiri and Kenealy, Kathleen and Swersky, Kevin and Mahajan, Kshiteej and Culp, Laura and Xiao, Lechao and Bileschi, Maxwell L. and Constant, Noah and Novak, Roman and Liu, Rosanne and Warkentin, Tris and Qian, Yundi and Bansal, Yamini and Dyer, Ethan and Neyshabur, Behnam and {Sohl-Dickstein}, Jascha and Fiedel, Noah},
  year = {2024},
  month = apr,
  number = {arXiv:2312.06585},
  eprint = {2312.06585},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.06585},
  urldate = {2025-02-07},
  abstract = {Fine-tuning language models (LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST{$EM$}, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST{$EM$} scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can reduce dependence on human-generated data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/MIMS727R/Singh et al. - 2024 - Beyond Human Data Scaling Self-Training for Problem-Solving with Language Models.pdf}
}

@article{strobl24,
  title = {What {{Formal Languages Can Transformers Express}}? {{A Survey}}},
  shorttitle = {What {{Formal Languages Can Transformers Express}}?},
  author = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
  year = {2024},
  month = may,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  eprint = {2311.00208},
  primaryclass = {cs},
  pages = {543--561},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00663},
  urldate = {2025-02-07},
  abstract = {As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/INAXJNTY/Strobl et al. - 2024 - What Formal Languages Can Transformers Express A Survey.pdf}
}

@misc{tigges24,
  title = {{{LLM Circuit Analyses Are Consistent Across Training}} and {{Scale}}},
  author = {Tigges, Curt and Hanna, Michael and Yu, Qinan and Biderman, Stella},
  year = {2024},
  month = nov,
  number = {arXiv:2407.10827},
  eprint = {2407.10827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.10827},
  urldate = {2025-02-07},
  abstract = {Most currently deployed LLMs undergo continuous training or additional finetuning. By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings. Existing studies of mechanisms over time focus on encoder-only or toy models, which differ significantly from most deployed models. In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters. We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale. Moreover, although such components may be implemented by different attention heads over time, the overarching algorithm that they implement remains. Surprisingly, both these algorithms and the types of components involved therein tend to replicate across model scale. Finally, we find that circuit size correlates with model size and can fluctuate considerably over time even when the same algorithm is implemented. These results suggest that circuit analyses conducted on small models at the end of pre-training can provide insights that still apply after additional training and over model scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/LX4QIYNG/Tigges et al. - 2024 - LLM Circuit Analyses Are Consistent Across Training and Scale.pdf}
}

@misc{vig20,
  title = {Causal {{Mediation Analysis}} for {{Interpreting Neural NLP}}: {{The Case}} of {{Gender Bias}}},
  shorttitle = {Causal {{Mediation Analysis}} for {{Interpreting Neural NLP}}},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Sakenis, Simas and Huang, Jason and Singer, Yaron and Shieber, Stuart},
  year = {2020},
  month = nov,
  number = {arXiv:2004.12265},
  eprint = {2004.12265},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.12265},
  urldate = {2025-03-06},
  abstract = {Common methods for interpreting neural models in natural language processing typically examine either their structure or their behavior, but not both. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. It enables us to analyze the mechanisms by which information flows from input to output through various model components, known as mediators. We apply this methodology to analyze gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are (i) sparse, concentrated in a small part of the network; (ii) synergistic, amplified or repressed by different components; and (iii) decomposable into effects flowing directly from the input and indirectly through the mediators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/atondwal/Zotero/storage/PHPQTJIT/Vig et al. - 2020 - Causal Mediation Analysis for Interpreting Neural NLP The Case of Gender Bias.pdf;/home/atondwal/Zotero/storage/32ZAJZLR/2004.html}
}

@misc{wang22,
  title = {Interpretability in the {{Wild}}: A {{Circuit}} for {{Indirect Object Identification}} in {{GPT-2}} Small},
  shorttitle = {Interpretability in the {{Wild}}},
  author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  year = {2022},
  month = nov,
  number = {arXiv:2211.00593},
  eprint = {2211.00593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.00593},
  urldate = {2025-03-06},
  abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/A2TNGWAI/Wang et al. - 2022 - Interpretability in the Wild a Circuit for Indirect Object Identification in GPT-2 small.pdf;/home/atondwal/Zotero/storage/VK4U3B62/2211.html}
}

@misc{xin24,
  title = {{{DeepSeek-Prover-V1}}.5: {{Harnessing Proof Assistant Feedback}} for {{Reinforcement Learning}} and {{Monte-Carlo Tree Search}}},
  shorttitle = {{{DeepSeek-Prover-V1}}.5},
  author = {Xin, Huajian and Ren, Z. Z. and Song, Junxiao and Shao, Zhihong and Zhao, Wanjia and Wang, Haocheng and Liu, Bo and Zhang, Liyue and Lu, Xuan and Du, Qiushi and Gao, Wenjun and Zhu, Qihao and Yang, Dejian and Gou, Zhibin and Wu, Z. F. and Luo, Fuli and Ruan, Chong},
  year = {2024},
  month = aug,
  number = {arXiv:2408.08152},
  eprint = {2408.08152},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.08152},
  urldate = {2025-02-07},
  abstract = {We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeekProver-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark (63.5\%) and the undergraduate level ProofNet benchmark (25.3\%).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/AHGR4LD3/Xin et al. - 2024 - DeepSeek-Prover-V1.5 Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo.pdf}
}

@misc{xu25,
  title = {Tracking the {{Feature Dynamics}} in {{LLM Training}}: {{A Mechanistic Study}}},
  shorttitle = {Tracking the {{Feature Dynamics}} in {{LLM Training}}},
  author = {Xu, Yang and Wang, Yi and Wang, Hao},
  year = {2025},
  month = feb,
  number = {arXiv:2412.17626},
  eprint = {2412.17626},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.17626},
  urldate = {2025-02-07},
  abstract = {Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we: (1) introduce SAE-Track, a method to efficiently obtain a continual series of SAEs; (2) formulate the process of feature formation and conduct a mechanistic analysis; and (3) analyze and visualize feature drift during training. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/ZQ5YJYRU/Xu et al. - 2025 - Tracking the Feature Dynamics in LLM Training A Mechanistic Study.pdf}
}

@misc{zhong23,
  title = {The {{Clock}} and the {{Pizza}}: {{Two Stories}} in {{Mechanistic Explanation}} of {{Neural Networks}}},
  shorttitle = {The {{Clock}} and the {{Pizza}}},
  author = {Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  year = {2023},
  month = nov,
  number = {arXiv:2306.17844},
  eprint = {2306.17844},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.17844},
  urldate = {2025-02-07},
  abstract = {Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/B6XKL4CJ/Zhong et al. - 2023 - The Clock and the Pizza Two Stories in Mechanistic Explanation of Neural Networks.pdf}
}

@misc{zhou23,
  title = {Instruction-{{Following Evaluation}} for {{Large Language Models}}},
  author = {Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  year = {2023},
  month = nov,
  number = {arXiv:2311.07911},
  eprint = {2311.07911},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.07911},
  urldate = {2025-02-07},
  abstract = {One core capability of Large Language Models (LLMs) is to follow natural language instructions. However, the evaluation of such abilities is not standardized: Human evaluations are expensive, slow, and not objectively reproducible, while LLM-based auto-evaluation is potentially biased or limited by the ability of the evaluator LLM. To overcome these issues, we introduce Instruction-Following Eval (IFEval) for large language models. IFEval is a straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set of "verifiable instructions" such as "write in more than 400 words" and "mention the keyword of AI at least 3 times". We identified 25 types of those verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We show evaluation results of two widely available LLMs on the market. Our code and data can be found at https://github.com/google-research/google-research/tree/master/instruction\_following\_eval},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/QGR96KFN/Zhou et al. - 2023 - Instruction-Following Evaluation for Large Language Models.pdf}
}

@misc{zou25,
  title = {Representation {{Engineering}}: {{A Top-Down Approach}} to {{AI Transparency}}},
  shorttitle = {Representation {{Engineering}}},
  author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
  year = {2025},
  month = mar,
  number = {arXiv:2310.01405},
  eprint = {2310.01405},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01405},
  urldate = {2025-03-05},
  abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/atondwal/Zotero/storage/3K53KHH4/Zou et al. - 2025 - Representation Engineering A Top-Down Approach to AI Transparency.pdf;/home/atondwal/Zotero/storage/M3KZJT37/2310.html}
}
@misc{marks24,
      title={The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
      author={Samuel Marks and Max Tegmark},
      year={2024},
      eprint={2310.06824},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.06824},
}