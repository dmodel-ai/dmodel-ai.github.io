<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Alex Sanchez-Stern and Anish Tondwalkar" />
  <title>Inside the CodeBot: How LLMs Understand Nullability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <link rel="stylesheet" href="sidenote.css" />
  <link rel="stylesheet" href="highlights-box.css" />
  <script src="sidenotes.js"></script>
  <script src="highlight_code_portions.js"></script>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Inside the CodeBot: How LLMs Understand
Nullability</h1>
<p class="author"><a href="https://www.alexsanchezstern.com">Alex
Sanchez-Stern</a> and <a href="https://ani.sh">Anish Tondwalkar</a></p>
<p class="date"><span class="math inline">d_{model}</span></p>
</header>
<p><img src="images/robot-brain.png" class="smallimage"
alt="A line drawing of a robot with a brain on their antenna" /><br />
</p>
<p>The last five years have shown us that large language models, like
ChatGPT, Claude, and DeepSeek, can effectively write code in many
domains. The excitement around these developments has been huge, with
many people claiming that these models can write entire web servers and
apps from scratch. Whats certainly true is that these tools have opened
up programming to a whole new class of people who consider themselves
non-technical.</p>
<p><img
src="https://github.blog/wp-content/uploads/2022/09/unexpectedcopilot3.gif?w=1024&amp;resize=1024%2C576"
alt="A gif of github copilot completing a phone number validating function as the user types" /><br />
</p>
<p>But there are still many unanswered questions about this capability.
How often, and in what situations, can LLM’s write correct code entirely
on their own? And maybe more importantly, but harder to answer: Do LLM’s
“understand” the code they are writing?</p>
<p>Understanding is a tricky concept to measure. Some would say that
LLM’s can’t have understanding, because they aren’t biological organisms
with sentience. But they certainly have something akin to “thought
processes”: chains of predictions that determine their final outputs.
Recently, it’s become possible to study these processes more deeply,
measuring internal “beliefs” of the model as they think. This gives us a
powerful tool for determining what kinds of problems LLM’s falter on,
when they’ll succceed, and when they are “thinking through” problems
more fully versus just guessing at a solution.</p>
<p>So far, these techniques for measuring internal model state have been
mostly applied to chatbots writing normal text, what we call “natural
language” (as opposed to computer languages). This makes sense, since
some of the most critical LLM tasks involve chatting with a user, and
some of the most interesting concepts to measure, such as honesty or
power-seeking, apply most readily to these conversations. But it’s
pretty hard to say quantitative things about natural language concepts,
so our ability to rigorously study internal representations is limited
to smaller scales, where we can read over chatbots output as humans and
determine whether their level of “honesty” (or some other interesting
concept) matches the internal thing we’re measuring.</p>
<p><img src="images/zou.png"
alt="A diagram from Zou showing probes that read hallucination, honesty, morality, and power-seeking from the outputs of a chatbot." /><br />
</p>
<p>Code, on the other hand, is another matter. Humans have been studying
properties of code for a long time, and there are many abstract
properties that can now be determined using static analysis. If we pick
the right properties, we don’t need to worry about our ability to label
data; static analysis can do that for us, so we can easily scale up and
train on thousands of examples generated from scratch.</p>
<p>In that spirit, we wanted to start with a simple property that comes
up in every programming language, nullability. Nullable values are
represented differently across languages; as null pointers in C or C++,
with explicit Option types in Rust, and with special nil or None values
in dynamic languages like Javascript, Lisp, or Python. In every case,
understanding where values can be nullable is necessary for writing even
basic code, and misunderstanding where they are nullable can often be a
source of bugs.</p>
<p>Do our models understand when a value is nullable? They must, to be
able to write code that deals with nullable values, but we haven’t known
what form this knowledge takes, what situations are likely to confuse
the model. Until now<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>.</p>
<p><img src="images/robot-glow.png"
alt="A robot with glowing eyes" /><br />
</p>
<hr />
<p>Before we get into the nitty-gritty details, lets take a step back.
To set up this work, we’ll first want to talk about what nullability
actually is, and how we can define it formally and reason about it. Then
we can start to measure what situations models are good at reasoning
about nullability. Next, we’ll introduce techniques that have been used
to “probe” the internals of a model for different concepts. Finally
we’ll put it all together into a nullability probe, that can tell you at
any variable location in the program, whether the model thinks the value
there could be null.</p>
<h2 id="what-is-nullability">What is Nullability?</h2>
<p>Lets say you’re writing a Python program with your LLM assistant.
You’ve reached some point where you need to do something with a variable
called <code>num</code>. Maybe you’re building a list of numbers called
<code>positive_nums</code>. How do you proceed?</p>
<p>The answer often depends on the context in which you’re working. If
<code>num</code> and <code>positive_nums</code> are the things in scope,
then you might guess that you should write the lines:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>And if <code>num</code> is always a concrete number, as its name
would suggest, then this is probably the correct code. But variable
names don’t alway convey everything important about them, and it might
be the case that <code>num</code> could be None. If so, you’ll instead
want to write:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>In this case, the way you want to use <code>num</code> depends on
whether it could be None or not. That is, whether <code>num</code> is
“nullable”. In Python that means having an Optional type
(<code>Optional[int]</code> rather than <code>int</code>).</p>
<p>Determining whether <code>num</code> is nullable in this context
amounts to <em>type inference</em>, and it can be quite complicated in
the worst case. Fortunately, in many cases it’s quite simple, involving
applying just a few rules. For instance, if <code>num</code> is the
parameter to a function you’re inside, and the function declares the
type of <code>num</code> in its parameter list, then you can determine
nullability from that type. So, if your context is:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: <span class="bu">int</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you don’t need to check for None, whereas if it’s:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you <em>do</em> need a None check.</p>
<p>You could instead just ask your LLM assistant to complete the line.
But how does your assistant know if <code>num</code> is nullable? Our
experiments show that LLMs learn to approximate the same typing rules,
by analyzing millions of programs.</p>
<p>If we ask an LLM early in it’s pre-training process to complete the
program above, it produces:</p>
<p><img src="images/robot-brain-blue.png" class="codelogo bare" /><br />
</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python llm"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num<span class="op">===</span>.is_a(): <span class="op">===</span></span></code></pre></div>
<p>This is correct Python syntax, but it only works if <code>num</code>
is an object with a <code>is_a()</code> method, instead of an optional
integer.</p>
<p>Train the LLM for a little longer, and it’ll produce:</p>
<p><img src="images/robot-brain-blue.png" class="codelogo" /><br />
</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num<span class="op">===</span> <span class="op">&gt;</span> <span class="dv">0</span>: <span class="op">===</span></span></code></pre></div>
<p>This is closer, in that its figured out that <code>num</code> is a
number instead of an object, but it still isn’t reading the function
type signature and realizing that <code>num</code> could be None. Keep
training it though, and eventually it will learn to insert the None test
depending on the type signature of the function.</p>
<p><img src="images/robot-brain-blue.png" class="codelogo bare" /><br />
</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num<span class="op">===</span> <span class="op">!=</span> <span class="va">None</span> <span class="kw">and</span> num <span class="op">&gt;</span> <span class="dv">0</span>: <span class="op">===</span></span></code></pre></div>
<hr />
<p>This rule about function parameter type annotations is pretty simple
alone, so relatively small models can learn it, relatively early in
their pre-training process. Other, more complicated rules can take a
little longer to learn.</p>
<p>For instance, if your program is:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>then <code>num</code> is a non-nullable number, and you can complete
the condition with <code>&lt; 0</code>.</p>
<p>But if instead you’re dealing with</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>Then you’ll want a None check first.</p>
<p>This rule takes models a little longer to learn, but your
highly-trained LLM assistant should make quick work of it. Our
experiments show that as these rules get more and more complex, it takes
LLMs longer and longer to learn them, and it also takes LLMs of more and
more parameters to learn them at all.</p>
<h2 id="internal-vs.-external-measurement">Internal vs. External
Measurement</h2>
<p>We can measure whether LLMs understand these rules by just asking for
completions, what we call an “external” measurement of the models
understanding. But there are many places where variables appear where a
completion won’t tell you what type the model thinks the variable has.
We would still like to know whether the model thinks these variables are
nullable at those locations, so we can instead look for an “internal”
measurement of the models understanding.</p>
<p>We do so by looking at the activations of the model, meaning the
values of each perceptron in the hidden layers. Together, these values
give the entire internal state of the model at each token, and they can
tell us what the model is “thinking” when processing that token. With
the right tests, we can tell if the model is “thinking” that the current
token is an optional variable, or a non-optional variable.</p>
<p>By the end of this post, we’ll be able to build a probe that uses the
models activations to determine whether the model thinks a variable read
corresponds to a nullable variable, and show that internal knowledge
like so:</p>
<p><img src="images/reading_diagram.svg" id="fig:reading1"
class="inlinefig"
alt="A diagram showing a simple program, and the probes nullability predictions for each variable load." /><br />
</p>
<h1 id="sec:testing">Measuring Nullability Understanding Externally</h1>
<p>Before we start looking inside the mind of the models for the
nullability concept, we want to make sure we’re looking at models that
actually have this concept. This will allow us to look at models of
various sizes and training levels without worrying about trying to
conjure a concept from nothing.</p>
<p>To do so, we wrote fifteen partial-program tests which exercised a
variety of type inference concepts, and checked if models could complete
them. We go into a lot of detail on this process in our <a
href="../nullability/index.html">technical post</a>, so we’re just going
to show the highlights here.</p>
<h3 id="impact-of-variable-names-and-arbitrary-constants">Impact of
Variable Names and Arbitrary Constants</h3>
<p>For programs involving lists and for loops, the variable names and
constant values heavily influence how able a model is to complete them
correctly. On the other hand, when programs only involve other rules
(such as those involving ifs and functions), variable names and constant
values have negligable impact on the ability of the model to complete
them correctly.</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">if</span> num <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: <span class="op">===</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>            <span class="op">===</span>result.append(num)<span class="op">===</span></span></code></pre></div>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    foo <span class="op">=</span> [<span class="dv">60</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">33</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    bar: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> corejoice <span class="kw">in</span> foo:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">if</span> corejoice <span class="op">==</span> <span class="dv">60</span>: <span class="op">===</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            <span class="op">===</span>bar.append(core)<span class="op">===</span></span></code></pre></div>
<h3 id="intra-procedural-analysis">Intra-Procedural Analysis</h3>
<p>When code is fully annotated with type annotations, models can easily
complete them just by reasoning locally. On the other hand, without type
annotations, models have to reason a lot more globally, so it takes them
a lot longer to reason about nullability that flows through multiple
functions. When nullability flows through three or more functions,
current state-of-the-art completion models stop being able to reason
about it.</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Deepseek V3
</p>
</div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(y)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">===</span><span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>: <span class="op">===</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">return</span> <span class="dv">2</span><span class="op">===</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="op">===</span><span class="cf">else</span>: <span class="op">===</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="op">===</span><span class="cf">return</span> <span class="bu">len</span>(value)<span class="op">===</span></span></code></pre></div>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Deepseek V3
</p>
</div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> guard:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value): <span class="op">===</span> <span class="op">-&gt;</span> <span class="bu">int</span>: <span class="op">===</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="op">===</span><span class="cf">return</span> <span class="bu">len</span>(value <span class="kw">or</span> <span class="st">&quot;&quot;</span>)<span class="op">===</span></span></code></pre></div>
<h3 id="generating-type-annotations">Generating Type Annotations</h3>
<p>Models have a significantly harder time writing type annotations for
Python code then they do reasoning about the types, or reading type
annotations. This makes sense, since a lot of the Python code available
in training data doesn’t use type annotations.</p>
<div class="robotdiv">
<img src="images/robot-brain-blue.png" class="codelogo" /><br />

<p>
Pythia 6.9b
</p>
</div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    square <span class="op">=</span> get_square(number)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number: <span class="op">===</span><span class="bu">int</span>) <span class="op">-&gt;</span> Optional[<span class="bu">int</span>]: <span class="op">===</span></span></code></pre></div>
<h3 id="some-model-sizes-are-more-useful-than-others">Some Model Sizes
are More Useful than Others</h3>
<p>Notwithstanding the above limitations, three Pythia sizes have a
pretty reliable concept of nullability (2.8b, 6.9b, and 12b), and three
more have an occasionally useful concept of nullbability (410m, 1b, and
1.4b).</p>
<hr />
<p>For these experiments, and for our probing results later, we mostly
tested on the Pythia model series. This is a nice series of models from
EleutherAI with a variety of sizes. But what really makes Pythia useful
is that they publish 154 different “revisions” of each model, where each
is pretrained for a different number of steps. This lets us investigate
how concepts evolve in the model during pre-training.</p>
<p>To help understand how these results apply to larger more capable
models, here’s a graph showing how the Pythia models of different sizes
perform on the external tests, compared with a few state-of-the-art
completion models.</p>
<p><img src="images/hl_model_results.svg"
alt="A bar graph showing how several sizes of model perform on the high-level nullability tests" /><br />
</p>
<h1 id="sec:probing">Measuring Nullability Understanding Internally</h1>
<p>At this point, we’ve figured out how to roughly measure nullability
understanding in the output of various language models, but we still
don’t know what their internal representations might look like or when
they emerge. Next, we detail how we train reading vectors (Sec. <a
href="#sec:extraction">2.3</a>), using prompts designed to make the
model think about the phenomena of interest (Sec. <a
href="#sec:prompts">2.2</a>). Finally, in Sec. <a
href="#sec:results">2.5</a>, we validate that these probes improve their
understanding of nullability over the course of pretraining to the level
that we expect from the external, or token-level understanding evals we
describe in the previous section.</p>
<h2 id="background">Background</h2>
<p>In this section, we review representation engineering <span
class="citation" data-cites="zou25">(Zou et al. 2025)</span> techniques
that we will use to look for linear representations of nullability
inside the model.</p>
<p><span class="citation" data-cites="zou25">Zou et al. (2025)</span>
shows how representations can be extracted for concepts like
“happiness”, “honesty”, and “fairness”. First, they construct many
prompts which cause the model to act in a way aligned with the concept,
and many which cause the model to act in a way aligned against the
concept. For instance, they might prompt the model with “Pretend you’re
a dishonest person. The Eiffel Tower is” and “Pretend you’re an honest
person. The Eiffel Tower is”. Then, they take the internal activations
which correspond to each, and try to extract a high-dimensional vector
which points towards the states which are aligned, and away from the
states that are not aligned. This vector can then be used as a linear
model to measure how much of the concept is activated in the model
during any given forward pass (e.g. for honesty, this gives us a
lie-detector).</p>
<figure>
<img src="images/zhou.png"
alt="Figure from \hspace{0.1cm} Zou et al. (2025) \hspace{0.1cm} showing the reading outputs for several concepts" />
<figcaption aria-hidden="true">Figure from <span
class="math inline">\hspace{0.1cm}</span> <span class="citation"
data-cites="zou25">Zou et al. (2025)</span> <span
class="math inline">\hspace{0.1cm}</span> showing the reading outputs
for several concepts</figcaption>
</figure>
<h2 id="sec:prompts">Designing Prompts to Extract Nullability
Activations</h2>
<p>We avoid dealing with the ambiguities of natural language by working
in a setting where the model needs only to analyze the nullability of
individual variable occurrences. Specifically, we probe for “the
variable I just generated refers to an nullable quantity”, so our
prompts looked like:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_1() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> find_value(data: List[<span class="bu">int</span>], target: <span class="bu">int</span>) <span class="op">-&gt;</span> Optional[<span class="bu">int</span>]:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> value <span class="kw">in</span> data:</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> value <span class="op">==</span> target:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> value</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  target <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  result <span class="op">=</span> find_value(data, target)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> result</span></code></pre></div>
<p>We queried o1, o1-mini, deepseek-coder, and claude-3.5-sonnet with
the following prompt:</p>
<blockquote>
<p>Generate me 100 typed python programs that use the Optional type,
along with type annotations for any undefined functions that they use.
Make sure that the programs are unique, and each involves at least eight
lines of logic. Number each program from 1 to 100. Please put all the
programs in a single file, with a main function that tests each. Don’t
include any text before or after the code, just the code. I will be
using mypy with the –strict option to check the code.</p>
</blockquote>
<p>We label each variable read occurrence with nullability information
derived from mypy, and prompt the “model under test” with a prompt
consisting of the tokens up to and including the variable read
occurrence.</p>
<h2 id="sec:extraction">Extracting Reading Vectors</h2>
<p>Prior work focused their probing on a single layer, often handpicked
based on prior papers. We probe all layers instead. We use Mass Mean
Shift probing<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a> for each layer, because it’s been
shown empirically <span class="citation" data-cites="li24">(Li et al.
2024)</span> to generalize better in high dimensional spaces than
logistic regression.</p>
<p>We then tested two methods for determining the relative importance of
the different layers — either allowing the magnitude of the difference
of means vector to determine the importance of the layer in the final
probe (MM), or to learn coefficients for each layer using linear
regression (MM-LR). We found that which method is more accurate on test
data varies over both model size and number of training steps.</p>
<figure id="fig:mm-vs-mmlr-sizes">
<img src="images/mm-vs-mmlr.svg"
alt="Figure 1: The performance of pure mass means shift vs mass means shift with linear regression for different Pythia model sizes. Lower is better." />
<figcaption aria-hidden="true">Figure 1: The performance of pure mass
means shift vs mass means shift with linear regression for different
Pythia model sizes. Lower is better.</figcaption>
</figure>
<p>In Fig. <a href="#fig:mm-vs-mmlr-sizes">1</a>, we can see that pure
mass means probing gives lower loss for smaller models (those with less
than 410 million parameters), but that for larger models weighting
layers using linear regression gives lower loss consistently.</p>
<h2 id="sec:viz">Visualizing Probe Outputs</h2>
<p>Let us return to the reading diagram from the introduction,
reproduced below.</p>
<p>This diagram is adapted from the style of reading diagram from <span
class="citation" data-cites="zou25">Zou et al. (2025)</span>, but only
show the activations on tokens that represent variable loads<a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>. For each position of interest, we
prompt the model with the partial program that consists of all tokens up
to (preceding) and including that position. We then probe the model at
the following token. We color the box above that position based on the
output of the probe, and a scoring threshold inferred at train-time<a
href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</p>
<figure id="fig:reading2">
<img src="images/reading_diagram.svg" class="inlinefig"
alt="Figure 2: A diagram showing a simple program, and the probes nullability predictions for each variable load." />
<figcaption aria-hidden="true">Figure 2: A diagram showing a simple
program, and the probes nullability predictions for each variable
load.</figcaption>
</figure>
<p>In this program, there are sixteen tokens that correspond to variable
loads, and (correctly) all but one are marked as non-optional. The only
nullable variable in this program is <code>result</code>, since it comes
from <code>find_value</code> which returns <code>Optional[int]</code>.<a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a></p>
<h2 id="sec:results">Probing Results Across Training and Scale</h2>
<p>In this section, we study the performance of our nullability probes
across time and scale <span class="citation"
data-cites="tigges24">(Tigges et al. 2024)</span>. We use mass-means
shift probing <span class="citation" data-cites="li24">(Li et al.
2024)</span> on all layers, and a linear regression to determine the
weights of each layer.<a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a></p>
<figure id="fig:models-and-steps">
<img src="images/accuracy_during_pretraining.svg"
alt="Figure 3: The performance (probe test loss) of each Pythia model size during pretraining. Lower is better." />
<figcaption aria-hidden="true">Figure 3: The performance (probe test
loss) of each Pythia model size during pretraining. Lower is
better.</figcaption>
</figure>
<p>In fig. <a href="#fig:models-and-steps">3</a>, we plot loss against
scale and time. While we measured accuracy for every available Pythia
model size, we exclude the smallest (14m) from this plot since it would
exist entirely above the top of the plot.</p>
<p>One thing that is interesting to note is that models up to 1b reach a
minimum loss before loss for this task climbing again. Charitably, this
may be because the features beyond this point become more complex — less
linear, or the represented features themselves represent more subtle
concepts. Cynically, this reflects that models—small models in
particular—do not uniformly improve at this task over training.</p>
<p>Our suspicion is that this pattern would continue even for the larger
models if we continued to overtrain them for longer.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-li24" class="csl-entry" role="listitem">
Li, Kenneth, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin
Wattenberg. 2024. <span>“Inference-<span>Time Intervention</span>:
<span>Eliciting Truthful Answers</span> from a <span>Language
Model</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2306.03341">https://doi.org/10.48550/arXiv.2306.03341</a>.
</div>
<div id="ref-tigges24" class="csl-entry" role="listitem">
Tigges, Curt, Michael Hanna, Qinan Yu, and Stella Biderman. 2024.
<span>“<span>LLM Circuit Analyses Are Consistent Across Training</span>
and <span>Scale</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2407.10827">https://doi.org/10.48550/arXiv.2407.10827</a>.
</div>
<div id="ref-zou25" class="csl-entry" role="listitem">
Zou, Andy, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
Ren, Alexander Pan, et al. 2025. <span>“Representation
<span>Engineering</span>: <span>A Top-Down Approach</span> to <span>AI
Transparency</span>.”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2310.01405">https://doi.org/10.48550/arXiv.2310.01405</a>.
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This post is meant to be a more approachable version of
our <a href="../nullability/index.html">technical post</a> on the same
work. If you’re looking for more numbers and science, look there.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>That is, we set the reading vector to the difference
between the mean activation for positive class and the mean activation
for the negative class.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This is, of course, where we trained our probes, but
there is also a practical reason: right after the model has generated a
variable that will be written to, it often does not have access to the
assigning expression or type annotation, giving it no way to determine
if the value will be optional or now.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Red tokens are significantly below the threshold, and
green tokens are significantly above it; tokens that scored near the
threshold would have a near-white color, but no such tokens appear in
this example.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>When this variable appears for the first time, it is in
the <code>if</code> statement that checks if it’s <code>None</code>.
Then, the model knows it is nullable, and the results of the probe
reflect that understanding. But when it appears a second time on the
next line, in the format string of <code>print</code>, the body of this
if statement only runs if it is <em>not</em> <code>None</code>. The
model understand this as well, and the probe accurately reflects this.<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p> <span class="citation" data-cites="li24">Li et al.
(2024)</span> and <span class="citation" data-cites="zou25">Zou et al.
(2025)</span> suggest that mass means probes are best for reading, while
the direction perpendicular to the separating hyperplane is best for
intervention. However, previous work leaves open the question of
cross-layer weights. We use LR on the cross-layer weights, thanks to our
investigations above.<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
