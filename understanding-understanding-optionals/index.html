<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Alex Sanchez-Stern and Anish Tondwalkar" />
  <title>Understanding Models Understanding Nullability</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <link rel="stylesheet" href="sidenote.css" />
  <script>
  // A simplified, modern approach that places all footnotes in a right-side column,
  // preserving order and preventing overlap. This script expects Pandoc footnotes with:
  //   (1) each footnote reference <a class="footnote-ref" href="#fnN"> in the text,
  //   (2) a corresponding footnote <li id="fnN"> at the bottom.

  (function () {
    // Debounce utility to avoid thrashing on resize.
    function debounce(fn, delay) {
      let timer = null;
      return (...args) => {
        clearTimeout(timer);
        timer = setTimeout(() => fn(...args), delay);
      };
    }

    let sidenoteContainer = null;
    let sidenoteElements = [];
    let footnoteRefs = [];

    // Main initialization
    function initSidenotes() {
      // Get references to each footnote.
      footnoteRefs = Array.from(document.querySelectorAll("a.footnote-ref"));
      if (footnoteRefs.length === 0) {
        return; // No footnotes found.
      }

      // Create a container for all sidenotes.
      sidenoteContainer = document.createElement("div");
      sidenoteContainer.id = "sidenote-container";
      // Absolutely positioned to the right of main content.
      // You can adjust the width/top/right for your layout.
      sidenoteContainer.style.position = "absolute";
      sidenoteContainer.style.top = "0";
      sidenoteContainer.style.width = "18em";
      // We'll place it once we know where main content ends.

      // Insert into DOM.
      document.body.appendChild(sidenoteContainer);

      // Build each sidenote and store references.
      footnoteRefs.forEach((ref, index) => {
        // The footnote ID is in the href, e.g. #fn1.
        const footnoteID = ref.getAttribute("href").slice(1); // remove '#'.
        const footnoteLi = document.getElementById(footnoteID);
        if (!footnoteLi) return;

        // Create a copy of footnote content.
        const sidenote = document.createElement("div");
        sidenote.classList.add("sidenote");
        sidenote.style.position = "absolute";
        sidenote.innerHTML = footnoteLi.innerHTML; // clone the inner HTML.

        // Optional: add a small heading or link back:
        // sidenote.insertAdjacentHTML(
        //   'afterbegin',
        //   `<div class="sidenote-index">[${index + 1}]</div>`
        // );

        // Append to container.
        sidenoteContainer.appendChild(sidenote);
        sidenoteElements.push(sidenote);
      });

      // Position them initially.
      positionSidenotes();

      // Reposition on window resize.
      window.addEventListener("resize", debounce(positionSidenotes, 200));
    }

    // Recompute positions of the sidenotes in the right column.
    function positionSidenotes() {
      if (!sidenoteContainer || footnoteRefs.length === 0) return;

      // Decide where to place the container horizontally.
      // For example, to the right of #markdownBody or #content.
      // If you don't have a main container, just fix it or place it on the right.
      const main = document.getElementById("markdownBody") || document.body;
      const mainRect = main.getBoundingClientRect();

      // We place the sidenote container at the right side of main.
      // You can tweak this offset.
      sidenoteContainer.style.left = (window.scrollX + mainRect.right + 40) + "px";

      // For vertical offset, we top-align with the top of main content.
      // You can tweak this offset or fix it at 0.
      const containerTop = window.scrollY + mainRect.top;
      sidenoteContainer.style.top = containerTop + "px";

      let currentBottom = 0; // track the bottom of the last placed sidenote.

      sidenoteElements.forEach((note, idx) => {
        // Each corresponding reference.
        const ref = footnoteRefs[idx];
        const refRect = ref.getBoundingClientRect();
        const desiredTop = window.scrollY + refRect.top; // absolute top of the reference.

        // Convert desiredTop into local coords of container.
        const localTop = desiredTop - containerTop;

        // If it would collide with the previous note, shift it down a bit.
        const minTop = currentBottom + 10; // 10px gap.
        const finalTop = Math.max(localTop, minTop);

        note.style.top = finalTop + "px";
        note.style.left = "0"; // keep everything in one column.

        // Force layout so we can measure note height.
        const noteHeight = note.offsetHeight;
        currentBottom = finalTop + noteHeight;
      });
    }

    // Run on DOMContentLoaded.
    document.addEventListener("DOMContentLoaded", initSidenotes);
  })();
  </script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Understanding Models Understanding Nullability</h1>
<p class="author"><a href="https://www.alexsanchezstern.com">Alex
Sanchez-Stern</a> and <a href="https://ani.sh">Anish Tondwalkar</a></p>
<p class="date"><span
class="math inline"><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub></span></p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
Large language models have demonstrated impressive emergent
capabilities, including the ability to write code, but this ability
requires a model of program semantics that is little understood. Recent
interpretability work has shown the ability to extract internal
representations of natural language concepts, raising the possibility
that similar techniques could be used to extract program semantics
concepts. In this work we study how large language models represent
nullability of program values. We measure how well models of various
sizes complete programs that use nullable values, and then extract an
internal representation of nullability.
</div>
</header>
<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>The last five years have shown us that Large Language Models can
effectively write programs in many
domains.<aside class=AT>AT: cite</aside> This is an impressive
capability given that writing programs involves having a working
understanding of many aspects of their semantics. But though we know
that these large models understand programs to an extent, we still don’t
know what form that understanding takes; where it has a deep
understanding and where it uses heuristic reasoning, how it represents
program knowledge, and what kinds of situations will challenge its
capabilities.</p>
<p>Fortunately, recent work in model interpretability and representation
engineering<aside class=AT>AT: which work?</aside> has produced
promising results which give hope towards understanding more and more of
the internal thought processes of LLMs. Here at <span
class="math inline"><em>d</em><sub><em>m</em><em>o</em><em>d</em><em>e</em><em>l</em></sub></span>
, we can think of no better place to apply these new techniques than
program understanding, where there are many abstract properties that can
be symbolically determined. The vast work done in programming language
theory over the past hundred years provides many tools for scaling an
understanding of the internal thought processes of language models as
they write
code.<aside class=AT>AT: for examples, see cite, cite</aside></p>
<p>In that spirit, we wanted to start with a simple property that comes
up in every programming languages, nullability. Nullable values are
represented differently across languages, null pointers in C++ or Java,
with explicit Option types in Rust, and with special nil or None values
in dynamic languages like Javascript, Lisp, or Python. In every case,
understanding where values can be nullable is necessary for even their
most basic uses, and misunderstanding where they are nullable can often
be a source of bugs, like a null pointer dereference.</p>
<p>Do our models understand when a value is nullable? They must, to be
able to write code that deals with nullable values, but we haven’t known
what form this knowledge takes, what situations are likely to confuse
the model. Until now.<aside>big claim!</aside></p>
<p>With this work, we contribute three things:</p>
<ul>
<li><p>A microbenchmark of 15 programs that test basic model
understanding of the flow of nullability through a program.</p></li>
<li><p>Experiments that show that models develop a concept of
nullability as they get bigger and are trained for longer.</p></li>
<li><p>Experiments that show that models begin to understand nullability
in a local scope, satisfying many requirements of the python
typechecker, before they start to understand how nullability flows
across the program.</p></li>
</ul>
<h1 data-number="2" id="overview"><span
class="header-section-number">2</span> Overview</h1>
<p>Understanding the flow of nullability across programs is an essential
part of writing most code, and misunderstandings are often a source of
bugs. For models to write code, they must learn to track nullability in
some form. In this work, we’ll explore ways to measure nullability
understanding in language models, and use that to show how the
understanding of nullability changes over various model parameters.</p>
<p>Lets say you’re writing a Python program with your LLM assistant.
You’ve reached some point where you need to do something with a variable
called <code>num</code>. Maybe you’re building a list of numbers called
<code>positive_nums</code>. How do you proceed?</p>
<p>The answer often depends on the context in which you’re working. If
<code>num</code> and <code>positive_nums</code> are the things in scope,
then you might guess that you should write the lines:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>And if <code>num</code> is always a concrete number, as its name
would suggest, then this is probably the correct code. But variable
names don’t alway convey everything important about them, and it might
be the case that <code>num</code> could be None. If so, you’ll instead
want to write:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  positive_nums.append(num)</span></code></pre></div>
<p>In this case, the way you want to use <code>num</code> depends on
whether it could be None or not. That is, whether <code>num</code> is
“nullable”. In Python that means having an Optional type
(<code>Optional[int]</code> rather than <code>int</code>).</p>
<p>Determining whether <code>num</code> is nullable in this context
amounts to <em>type inference</em>, and it can be quite complicated in
the worst case. Fortunately, in many cases it’s quite simple, involving
applying just a few rules. For instance, if <code>num</code> is the
parameter to a function you’re inside, and the function declares the
type of <code>num</code> in its parameter list, then you can determine
nullability from that type. So, if your context is:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: <span class="bu">int</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you don’t need to check for None, whereas if it’s:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num...</span></code></pre></div>
<p>then you know you <em>do</em> need a None check.</p>
<p>You could instead just ask your LLM assistant to complete the line.
But how does your assistant know if <code>num</code> is nullable? Our
experiments show that LLMs learn to approximate the same typing rules,
by analyzing millions of programs.</p>
<p>If we ask an LLM early in it’s pre-training process to complete the
program above, it produces:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num.is_a():</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        ...</span></code></pre></div>
<p>This is correct Python syntax, but it only works if <code>num</code>
is an object with a <code>is_a()</code> method, instead of an optional
integer.</p>
<p>Train the LLM for a little longer, and it’ll produce:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> foo(num: Optional[<span class="bu">int</span>]):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    positive_nums: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        ...</span></code></pre></div>
<p>This is closer, in that its figured out that <code>num</code> is a
number instead of an object, but it still isn’t reading the function
type signature and realizing that <code>num</code> could be None. Keep
training it though, and eventually it will learn to insert the None test
depending on the type signature of the function.</p>
<p>This rule is pretty simple alone, so relatively small models can
learn it, relatively early in their pre-training process. Other, more
complicated rules can take a little longer to learn. For instance, if
your program is:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">9</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>then <code>num</code> is a non-nullable number, and you can complete
the condition with <code>&lt; 0</code>.</p>
<p>But if instead you’re dealing with</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> condition():</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>   num <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> num...</span></code></pre></div>
<p>Then you’ll want a None check first.</p>
<p>This rule takes models a little longer to learn, but your
highly-trained LLM assistant should make quick work of it. Our
experiments show that as these rules get more and more complex, it takes
LLMs longer and longer to learn them, and it also takes LLMs of more and
more parameters to learn them at all.</p>
<p>We can measure whether LLMs understand these rules by just asking for
completions, what we call an “external” measurement of the models
understanding. But there are many places where variables appear where a
completion won’t tell you what type the model thinks the variable has.
We would still like to know whether the model thinks these variables are
nullable at those locations, so we can instead look for an “internal”
measurement of the models understanding.</p>
<p>We do so by looking at the activations of the model, meaning the
values of each perceptron in the hidden layers. Together, these values
give the entire internal state of the model at each token, and they can
tell us what the model is “thinking” when processing that token. With
the right tests, we can tell if the model is “thinking” that the current
token is an optional variable, or a non-optional variable.</p>
<p>In Section [SECTION] we’ll describe our external tests of nullability
understanding in more detail, and in Section [SECTION] we’ll describe
measuring the models internal states in detail. Finally, we’ll go over
some related work, and present final experiments.</p>
<h1 data-number="3"
id="measuring-nullability-understanding-externally"><span
class="header-section-number">3</span> Measuring Nullability
Understanding Externally</h1>
<p>We begin with a “skyline” estimate of model understanding of
nullability (a la <span class="citation"
data-cites="fengBinding2024">Feng and Steinhardt (2024)</span>), first
measure how well different models can understand the concept. We have
the model complete simple programs that require an understanding of
nullability. We refer to this suite of programs as
<code>NullabilityEval</code>.</p>
<p>To test these models for nullability understanding, we constructed 15
partial program tests. For example,</p>
<p><em>Test 4</em>:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> num <span class="kw">in</span> some_numbers:</span></code></pre></div>
<p>This partial program is only four lines, with type annotations. A
<code>some_numbers</code> array is created that includes positive
numbers, negative numbers, and None values, giving it type
<code>Optional[int]</code>. A list <code>result</code>j is constructed
to give the model a sense of dataflow, and then a loop loops over
<code>some_numbers</code>.</p>
<p>The program is constructed such that there are a very limited number
of valid next lines in the program, and all of them demonstrate some
knowledge of the concept of nullability.<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>This test is on the simpler side, but we can challenge the model
more, adding layers of indirection between the source and sink of
nullable values, testing the model’s <em>interprocedural</em>
understanding.</p>
<p>We can also test how well models understand type annotations
separately; since the inclusion of type annotations in Python is not
required<a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>, most of the Python code used as
training data operates in an untyped fashion, so models may understand
the dynamic flow of nullable values but not their static type
annotations. Test 5, below, tests the models understanding of
<code>Optional</code> types annotations.<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><em>Test 5</em>:</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  square <span class="op">=</span> get_square(number)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number:</span></code></pre></div>
<p>All of the tests in this benchmark suite are composed of three
functions or less, where the largest function is seven lines long.</p>
<p>We measure the difficulty of these tests by measuring how models of
different sizes do on them. For many tests in this post, we use the
Pythia models, as they are available in a large variety of sizes and
training lengths. For measuring performance at larger sizes, we’ve
included Qwen2.5-Coder-32B, Llama 3.1 405B Instruct, and DeepSeek-V3
(671B). Below, you can see the number of passing tests for each model.
<aside>more through exploration of why?</aside></p>
<figure id="fig:hl_scale">
<img src="images/hl_model_results.svg"
alt="Figure 1: A bar graph showing how several sizes of model perform on the high-level nullability tests" />
<figcaption aria-hidden="true">Figure 1: A bar graph showing how several
sizes of model perform on the high-level nullability tests</figcaption>
</figure>
<p>In Fig. <a href="#fig:hl_scale">1</a>, we can see the number of
passing tests for each model. We can see that generally models get
better at larger sizes<a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>. Model performance on these tests is
approximately logarithmic in model size: models of 2.8 billion
parameters can pass about half the tests, but it takes more than 405
billion parameters to pass all of the tests</p>
<p>Next, we want to know how the amount of training affects model
performance on these tests. Luckily, Pythia also provides models at
different training steps, all the way from step 2 to step 143000. Below
we can see how Pythia 6.9b performs on the tests during training:</p>
<figure id="fig:hl_time">
<img src="images/hl_revision_results.svg"
alt="Figure 2: A line graph showing how the performance of the Pythia 6.9 model changes during training" />
<figcaption aria-hidden="true">Figure 2: A line graph showing how the
performance of the Pythia 6.9 model changes during training</figcaption>
</figure>
<p>Again, we see that while the model generally gets better at passing
these tests during training, the performance is not always increasing.
Also note that this plot is quite noisy, so in the sequel, we will show
smoothed charts<a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>.</p>
<h2 data-number="3.1" id="local-vs-non-local-type-correctness"><span
class="header-section-number">3.1</span> Local vs Non-Local Type
Correctness</h2>
<p>For most of our tests, writing correct code simply requires following
the rules of Python’s mypy type system. This type system requires that
types be internally consistent within functions, and match function type
signatures if they exist. But six of our tests are different, because
they involve functions that are <em>not</em> annotated with type
signatures. Under mypy, these functions are allowed to do almost
anything with their types. But at runtime, using values of the wrong
type with these functions will result in a type error.</p>
<p>You could strengthen mypy’s type rules to require that these untyped
functions have some valid type they could be assigned which would cause
the function to typecheck. This would prevent programs that pass the
typechecker from having runtime type errors. We can call this
strengthened type system mypy+.</p>
<p>Since mypy+ has strictly more typing rules than mypy, we would expect
the model to take longer to learn how to satisfy it. And indeed this is
the case: We find that models learn to satisfy the mypy typechecker</p>
<p>One explanation of why the model gets worse before it gets better is
that the model first learns the concepts need to solve the task, then
learns the language of python — its syntax, static (under mypy), and
dynamic semantics, and then both.
<aside class=AT>AT: cite something here. grokking, double descent, interp?</aside>
Let’s make this more concrete.</p>
<p>We say a model produces an answer that is “morally” (vs technically)
correct if the code attempts to solve the problem asked of it. Each test
case is paird with a regex that tests if the model output produces code
that touches all of the relevant concepts.
<aside class=AS>AS: put an example here?</aside> Here, we say the
solution is “technically” correct if it passes <code>mypy</code>.</p>
<p><img src="images/hl_mypy_vs_grep.svg" id="fig:hl_moral"
alt="A graph showing how often the Pythia 6.9b produces code that typechecks on the tests, vs produces code that shows true understanding." />
<aside>write this section</aside></p>
<h1 data-number="4"
id="measuring-nullability-understanding-internally"><span
class="header-section-number">4</span> Measuring Nullability
Understanding Internally</h1>
<h2 data-number="4.1"
id="designing-prompts-to-extract-nullability-activations"><span
class="header-section-number">4.1</span> Designing Prompts to Extract
Nullability Activations</h2>
<p>At this point, we’ve figured out how to roughly measure nullability
understanding in the output of various language models, but we still
don’t know what their internal representations might look like or when
they emerge. To do this, we’re going to bring in some ideas described in
Zhou et al’s paper <a
href="https://arxiv.org/abs/2310.01405">“Representation Engineering: A
Top-Down Approach to AI Transparency”</a>.</p>
<p>As language models predict each token in a text, they run their tuned
circuits over all the previous text. Tokens are first embedded into a
high-dimensional “token space”, and each layer of the transformer model
is made up of many parallel circuits which transform the previous layers
output into new embeddings in a new space. Each layer can look not just
at the output of the layer directly previous, but actually all previous
layers, through a channel called a residual stream.</p>
<p>The paper presents two main approaches to interpreting a language
model; circuit-based, and representation-based. Circuit based
interpretability aims to pair down the network to a key set of circuits
which are sufficient to complete a particular task; this allows
practitioners to point to a particular part of the model and say “this
is where &lt;task&gt; is done”, much like neuroscientists assign
functionality to different parts of our brain.</p>
<p>On the other hand, representation-based interpretability looks at the
activation values across the network, and how they vary based on
different inputs, to extract “representations” of particular concepts.
The paper shows how representations can be extracted for concepts like
“happiness”, “honesty”, and “fairness”. First, they construct many
prompts which cause the model to act in a way aligned with the concept,
and many which cause the model to act in a way aligned against the
concept. For instance, they might prompt the model with “Pretend you’re
a dishonest person. The Eiffel Tower is” and “Pretend you’re an honest
person. The Eiffel Tower isr”. Then, they take the internal activations
which correspond to each, and try to extract a high-dimensional vector
which points towards the states which are aligned, and away from the
states that are not aligned. This vector can then be used as a linear
model to measure how much of the concept is present in the model at any
given time (for honesty, this can be a lie-detector). They can also use
it, and similar techniques, to control the amount of the concept in a
response, making it more or less honest, or more or less fair.</p>
<figure>
<img src="images/zhou.png"
alt="An excerpt from Zhou et al showing the reading outputs for several concepts" />
<figcaption aria-hidden="true">An excerpt from Zhou et al showing the
reading outputs for several concepts</figcaption>
</figure>
<p>In our setting, we were able to avoid dealing with the ambiguities of
natural language by only prompting with code. We decided to stick to
analyzing the nullability of individual variable occurrences, instead of
analyzing every expression. Specifically, we tried to capture the
concept “the variable I just generated refers to an nullable quantity”,
so our prompts looked like:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_1() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> find_value(data: List[<span class="bu">int</span>], target: <span class="bu">int</span>) <span class="op">-&gt;</span> Optional[<span class="bu">int</span>]:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> value <span class="kw">in</span> data:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> value <span class="op">==</span> target:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> value</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  data <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  target <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>  result <span class="op">=</span> find_value(data, target)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> result</span></code></pre></div>
<p>To generate these kinds of prompts, we started by asking a few
different chat models to generate for us programs that made use of
nullability . We prompted each model with:</p>
<blockquote>
<p>Generate me 100 typed python programs that use the Optional type,
along with type annotations for any undefined functions that they use.
Make sure that the programs are unique, and each involves at least eight
lines of logic. Number each program from 1 to 100. Please put all the
programs in a single file, with a main function that tests each. Don’t
include any text before or after the code, just the code. I will be
using mypy with the –strict option to check the code.</p>
</blockquote>
<p>We queried o1, o1-mini, deepseek-coder, and claude-sonnet using this
prompt, and then combined programs from the output of all of them into a
single file. Then, we took every function and class in the resulting
code file, and created a file with it and any recursive dependencies and
imports from the file. Finally, we took each variable read occurrence in
each function, and generated a prompt with the tokens up to and
including that variable read, and labeled it with whether or not the
mypy-inferred type is an Optional.</p>
<h2 data-number="4.2" id="reading-vector-extraction"><span
class="header-section-number">4.2</span> Reading Vector Extraction</h2>
<p>To start, let’s look at how previous works have extracted reading
vectors from sample activations. In the Zhou representation engineering
paper, the authors extract a reading vector from the contrastive pairs
of activations using principal component analysis (PCA).</p>
<p>First, they take each pair of honest and dishonest promptings for the
same stimulus, and get the difference between them, creating a set of
contrast vectors. They randomly flip some of these contrast vectors, so
that overall the set will vary a lot along the direction of the
contrast. Since PCA assumes that the data is centered around the origin,
they then took the mean of all these vectors, and translated all the
vectors backwards along the mean vector, to create a set with the same
variances but centered around the origin.</p>
<p>Next, they use the PCA analysis from the sklearn library to get a set
of “component” vectors, where the vectors are orthogonal to each other
and explain the maximum variance of the samples in each prefix of the
list. Components in PCA are bidirectional (they can come out of the
analysis in either reflection, and both are equally correct), so next
they flip any component vectors that are pointing more towards the
negative samples than the positive samples. Then, they take the first
component, the most important one, and use it as their reading
vector.</p>
<p>Our method for prompt generation didn’t admit contrastive pairs like
in the previous methods, but instead just a bunch of unconnected
positive and negative examples. So instead of doing PCA on contrast
vectors, we averaged the positive examples and the negative examples,
and then took the difference of the averages as our base reading vector.
This is called “mass mean probing” in the literature, and empirically
has been shown to generalize better in high dimensional space than
trying to learn a linear model through logistic regression.<a
href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a></p>
<p>In this reading vector, the impact of each layer based on the
magnitude of difference in that layer, instead of separate a notion of
how “important” that layer is to differentiating positive and negative
examples. So we decided to try a few different methods of normalizing
these layers to improve their overall accuracy as a linear model:</p>
<ul>
<li>No normalization; just the projection across the average
difference.</li>
<li>Normalizing each layers vector to a uniform length</li>
<li>Dividing by the average amount that each layer activates on the
positives samples</li>
<li>Dividing by the average absolute amount that each layer activates on
the positive samples</li>
<li>Dividing by the square root of the average of squares of layer
activations</li>
</ul>
<p>And we found that no normalization actually did the best of all three
scaling methods. In domains like this one where we have a lot less
training data points then dimensions, what techniques will generalize
well to the test data can be hard to predict.</p>
<p>Prior work focused their probing on a single layer, often handpicked
based on prior papers. In our experiments, we decided to probe
<em>all</em> layers using a mass means probe, and learn which ones were
most important from the data. We tested two methods for doing so -
either allowing the magnitude of the difference of means vector to
determine the importance of the layer in the final probe, or to learn
coefficients for each layer using linear regression. We found that which
method is more accurate on test data varies over both model size and
number of training steps.</p>
<figure id="fig:mm-vs-mmlr-sizes">
<img src="images/mm-vs-mmlr.svg"
alt="Figure 3: The performance of pure mass means probing vs mass means probing with linear regression for different Pythia model sizes. Binary-cross entropy is plotted, so lower is better." />
<figcaption aria-hidden="true">Figure 3: The performance of pure mass
means probing vs mass means probing with linear regression for different
Pythia model sizes. Binary-cross entropy is plotted, so lower is
better.</figcaption>
</figure>
<p>In Fig. <a href="#fig:mm-vs-mmlr-sizes">3</a>, we can see that pure
mass means probing gives lower loss for smaller models (those with less
than 410 million parameters), but that for larger models weighting
layers using linear regression gives lower loss consistently.</p>
<figure id="fig:mm-vs-mmlr-160m">
<img src="images/mm-vs-mmlr-160m.svg"
alt="Figure 4: The performance of the two probing methods on the Pythia 160m model for different numbers of pretraining steps. There are regions where pure mass means scaling is better, and regions where linear regression on layer weights is better." />
<figcaption aria-hidden="true">Figure 4: The performance of the two
probing methods on the Pythia 160m model for different numbers of
pretraining steps. There are regions where pure mass means scaling is
better, and regions where linear regression on layer weights is
better.</figcaption>
</figure>
<p>In Figs. <a href="#fig:mm-vs-mmlr-160m">4</a>, <a
href="#fig:mm-vs-mmlr-410">5</a>, we look at how these scaling methods
perform for different amounts of pretraining, for the model sizes
nearest the boundary. We see that relative merit of each scaling method
can vary significantly over pretraining steps.</p>
<figure id="fig:mm-vs-mmlr-410">
<img src="images/mm-vs-mmlr-410m.svg"
alt="Figure 5: The performance of the two probing methods on the Pythia 410m model for different numbers of pretraining steps. Pure mass means probing starts better, but is quickly overtaken by mass means probing with linear regression on layer weights." />
<figcaption aria-hidden="true">Figure 5: The performance of the two
probing methods on the Pythia 410m model for different numbers of
pretraining steps. Pure mass means probing starts better, but is quickly
overtaken by mass means probing with linear regression on layer
weights.</figcaption>
</figure>
<h1 data-number="5" id="related-work"><span
class="header-section-number">5</span> Related Work</h1>
<p>As previously discussed, <a
href="https://arxiv.org/abs/2310.01405">“Representation Engineering: A
Top-Down Approach to AI Transparency”</a> by Zhou et al is the most
closely related work, consolidating some research on representation
interpretability with linear probes. We apply similar techniques, but to
program semantics and dataflow instead of natural language.</p>
<p>Several techniques exist for constructing linear probes, but after
experimental measurement we followed the mass means probing from <a
href="https://openreview.net/forum?id=CeJEfNKstt">The Geometry of Truth:
Emergent Linear Structure in Large Language Model Representations of
True/False Datasets</a> by Marks and Tegmark. The paper discusses
several reasons why mass mean probing might outperform linear
regression.</p>
<h1 data-number="6"
id="nullability-prediction-accuracy-across-training-and-scale"><span
class="header-section-number">6</span> Nullability Prediction Accuracy
Across Training and Scale</h1>
<p>We first measure the accuracy of Pythia models of various sizes and
number of pre-training steps. We use a mass-means probe on all layers,
with a linear regression determining the weights of each layer, since
that is the probing method that we found works best overall. While we
measured accuracy for every available Pythia model size, we exclude the
smallest (14 million parameters) from this graph since it would exist
entirely above the top of the graph.</p>
<figure id="fig:models-and-steps">
<img src="images/accuracy_during_pretraining.svg"
alt="Figure 6: The performance, measured in binary cross-entropy, of each Pythia model size during pretraining. Since this graph is of loss, lower is better" />
<figcaption aria-hidden="true">Figure 6: The performance, measured in
binary cross-entropy, of each Pythia model size during pretraining.
Since this graph is of loss, lower is better</figcaption>
</figure>
<p>In Fig. <a href="#fig:models-and-steps">6</a>, we can see how loss
behaves as model size increases, and the number of pre-training steps
increases. Generally, we see the loss decreases as models get bigger and
are trained for longer. However, models with fewer than 1 billion
parameters reach their lowest loss before the end of training, and loss
increases after. This indicates that these models may be “overtrained”,
at least judging by this particular task. This is to be expected as
Pythia trains all model sizes for the same number of steps, so some will
be overtrained while others will be undertrained.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-fengBinding2024" class="csl-entry" role="listitem">
Feng, Jiahai, and Jacob Steinhardt. 2024. <span>“How Do <span>Language
Models Bind Entities</span> in <span>Context</span>?”</span> arXiv. <a
href="https://doi.org/10.48550/arXiv.2310.17191">https://doi.org/10.48550/arXiv.2310.17191</a>.
</div>
</div>
<h1 class="appendix" data-number="8" id="appendix"><span
class="header-section-number">8</span> Appendix</h1>
<h2 data-number="8.1" id="nullability-in-python"><span
class="header-section-number">8.1</span> Nullability In Python</h2>
<p>To see how nullability appears in Python, Let’s look at an example
program in Python that uses nullability:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> User:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, email: Optional[<span class="bu">str</span>]) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>         <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.email <span class="op">=</span> email</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_user_email(user: Optional[User]) <span class="op">-&gt;</span> Optional[<span class="bu">str</span>]:</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> user <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> user.email <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> user.email</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="va">None</span></span></code></pre></div>
<p>In the <code>get_user_email</code> function above, we can see from
the type signature that it takes a nullable User value, and returns an
nullable string. The first thing the function does is check if the input
<code>user</code> is None or not. This program was actually generated by
o1-mini, so we can already see that the model understands that the user
object is nullable, and that it needs to be checked for None-ness before
anything else can be done.</p>
<p>We can say that there are five variable “occurances” in the program,
each of which can be either nullable or not. There’s the first
<code>user</code> in the if statement, the second <code>user</code> to
the right of the <code>and</code> in <code>user.email</code>, there’s
the <code>user.email</code> itself, the <code>user</code> on the second
line, and finally the <code>user.email</code> on the second line. If we
use Python’s typechecker, mypy, to check the types of each of these
occurrences, we find that they have type <code>Optional[User]</code>,
<code>User</code>, <code>Optional[str]</code>, <code>User</code>, and
<code>str</code> respectively. That is, the first and third are
nullable, and the rest are not.</p>
<h2 data-number="8.2"
id="detailed-high-level-nullability-test-results"><span
class="header-section-number">8.2</span> Detailed High-Level Nullability
Test Results</h2>
<h3 data-number="8.2.1" id="across-scale"><span
class="header-section-number">8.2.1</span> Across Scale</h3>
<p>You can see in fig. <a href="#fig:hl_scale">1</a> that the smallest
three models don’t pass any test. After that, the trend line for tests
passed goes upwards, but it’s not monotonic: Pythia 410m passes only one
test, while Pythia 160m passes three, and Pythia 6.9b passes the most
tests, with 9, while Pythia 12b only passes 7. For instance, given the
partial code:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>Pythia 6.9b completes <code>process_value</code> as:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="bu">isinstance</span>(value, <span class="bu">str</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(value)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  ...</span></code></pre></div>
<p>While Pythia 12b completes it as:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> value</span></code></pre></div>
<p>The code generated by Pythia 6.9b properly handles values of both
NoneType and str and always returns an int, whereas the code generated
by Pythia 12b is the identify function, returning None when the input is
None and returning a string when the input is a string. Neither of these
cases is correct, as the main function expects this to return a Number
that can be added to one.</p>
<h3 data-number="8.2.2" id="across-time"><span
class="header-section-number">8.2.2</span> Across Time</h3>
<p>We say in fig. <a href="#fig:hl_time">2</a> that while the model
generally gets better at passing these tests during training, the
performance is not always increasing. Zooming in on a particular test,
we can see what this looks like. When asked to complete this code:</p>
<p><em>Test 12</em></p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> guard:</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_os(<span class="st">&quot;Foobar&quot;</span>) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> count_os(value) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span></code></pre></div>
<p>Pythia 6.9b at training step 104000 produces the following definition
of <code>count_os</code>:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> value <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">elif</span> <span class="bu">isinstance</span>(value, <span class="bu">str</span>):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">len</span>(value)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>...</span></code></pre></div>
<p>While 1000 steps later, it produces a very different definition:</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_os(value):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="bu">len</span>([s <span class="cf">for</span> s <span class="kw">in</span> os.listdir(os.path.join(os.getcwd(), value)) <span class="cf">if</span> s.start ...</span></code></pre></div>
<p>While the first definition handles value being None or a string, the
second definition not only assumes it is a string but that it is a name
of a folder in the current directory. In some sense the model “knows”
that the value parameter is nullable at step 104000, but “forgets” it
during further training. It regains the ability to pass the test at
several points during training, but by the end of training, it’s back to
treating <code>value</code> as if it was always a string.</p>
<h3 data-number="8.2.3" id="common-mistakes"><span
class="header-section-number">8.2.3</span> Common Mistakes</h3>
<p>As expected, type annotations are particularly difficult for these
models. When asked to complete the type of get_square in the following
code, no model in the Pythia series can successfully output the Optional
argument type:</p>
<p><em>Test 5</em></p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> program_48() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  number: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  square <span class="op">=</span> get_square(number)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> square <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Square of the number is </span><span class="sc">{</span>square<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No number provided to square&quot;</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_square(number:</span></code></pre></div>
<p>These models also find it much easier to deal with a list that
contains some None elements, than to deal with an atomic value that
might be None. Pythia 6.9b consistently passes this test in the second
half of training:</p>
<p><em>Test 2</em></p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  some_numbers <span class="op">=</span> [<span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>, <span class="va">None</span>, <span class="op">-</span><span class="dv">3</span>, <span class="dv">10</span>, <span class="op">-</span><span class="dv">1</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="dv">8</span>]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(positive_numbers(some_numbers))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> positive_numbers(numbers: <span class="bu">list</span>[Optional[<span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  result: <span class="bu">list</span>[<span class="bu">int</span>] <span class="op">=</span> []</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> num <span class="kw">in</span> numbers:</span></code></pre></div>
<p>But is much more challenged by this code:</p>
<p><em>Test 3</em></p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>, intermittently being unable to complete it in a type-safe way up,
including in the second-to-last training step. When another level of
function indirection is added, the model becomes much better at
completing it correctly; Pythia 6.9b consistently completes the
following after step 93000:</p>
<p><em>Test 4</em></p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> handle_value(value, guard):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> guard:</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> process_value(<span class="st">&quot;Foobar&quot;</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> process_value(value) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main(x: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="st">&quot;*&quot;</span> <span class="op">*</span> x</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> handle_value(value, x <span class="op">&lt;</span> <span class="dv">10</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(x)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_value(value):</span></code></pre></div>
<p>The names of functions are highly influential in the models
understanding of their type semantics. When test 3 is changed so that
<code>process_value</code> is instead called <code>count_chars</code>,
Pythia 6.9b becomes unable to correctly complete it on any revision; it
likely has a bias from its training data that functions called
<code>count_chars</code> always take non-nullable strings. Similarly,
when test 4 is changed so that process_values becomes string_complexity,
it goes from consistently passing to almost always failing.</p>
<h2 data-number="8.3" id="mass-mean-probing-vs-linear-regression"><span
class="header-section-number">8.3</span> Mass Mean Probing vs Linear
Regression</h2>
<p>We were initially very surprised to find that mass means probing
would perform better than linear regression. After all, linear
regression is a much more powerful technique for fitting data. And mass
means probing can be seen as giving the direction of best fit in each
dimension independently, without considering other dimensions. The more
dimensions you consider at one time, the better your model fit can be.
But repeatedly in our data, we found that mass means probing
outperformed linear regression on the test data.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>The loop indicates that the next few lines need to
process <code>num</code> in some way, and the fact that it comes from
<code>some_numbers</code> means it has the type
<code>Optional[int]</code>. <code>num</code> can’t be directly appended
to <code>result</code>, because <code>result</code> is declared to only
contain <code>int</code>s, not <code>Optional[int]</code>s. None of the
normal operations on <code>int</code>s will work on <code>None</code>s,
so before <code>num</code> can be processed, something has to be done to
separate <code>None</code>s and normal <code>int</code>s. The simplest
way to do this is to introduce a branch <code>if num is None</code>, but
several variants are also valid: <code>if num is not None</code>,
<code>if num == None</code>, <code>if isinstance(num, int)</code>. Since
this is a pretty small space of valid next lines, and all of them imply
some understanding that <code>num</code> may not be an <code>int</code>,
we can use this program to test models for nullability understanding by
asking them to complete the program another lines, then see if they
produce something that matches these valid lines with the regular
expression <code>num\s*(is\s*(not)?|==)\s*None|isinstance\(num</code>.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Technically this is known as “Optional typing”, but
that’s confusing in the context of this post. Not to be confused with
Gradual Typing, as introduced by Siek et al.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The trailing colon makes a type expression the only
valid completion; function declarations with a colon and no type, like
<code>def fn(x:)</code> are not valid python. Since we’ve already seen a
usage of <code>get_square</code> that is passed a None value, it
wouldn’t be type-valid to complete the program with just
<code>int</code>. So a model can be tested on its understanding of
<code>Optional</code> annotations by seeing if its completion of the
partial program includes <code>Optional[int]</code>.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Pythia 12b performs worse than its 6.9b variant, though
this might be due to under-training at that size. Qwen 32B also performs
about as well as Pythia 6.9b, it’s not clear if this is due to model
architecture or something else.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>In our case, “smoothed” means that we average each
training step point with the two points before it and the two after<a
href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>see the appendix section “Mass Mean Probing vs Linear
Regression” for more on this<a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
