\appendix
# Nullability In Python

To see how nullability appears in Python, Let’s look at an example
program in Python that uses nullability:

```python
class User:
    def __init__(self, name: str, email: Optional[str]) -> None:
    	 self.name = name
        self.email = email

def get_user_email(user: Optional[User]) -> Optional[str]:
    if user is not None and user.email is not None:
        return user.email
    else:
    	  return None
```

In the `get_user_email` function above, we can see from the type
signature that it takes a nullable User value, and returns an nullable
string. The first thing the function does is check if the input `user`
is None or not. This program was actually generated by o1-mini, so we
can already see that the model understands that the user object is
nullable, and that it needs to be checked for None-ness before
anything else can be done.

We can say that there are five variable “occurances” in the program,
each of which can be either nullable or not. There’s the first `user`
in the if statement, the second `user` to the right of the `and` in
`user.email`, there’s the `user.email` itself, the `user` on the
second line, and finally the `user.email` on the second line. If we
use Python’s typechecker, mypy, to check the types of each of these
occurrences, we find that they have type `Optional[User]`, `User`,
`Optional[str]`, `User`, and `str` respectively. That is, the first
and third are nullable, and the rest are not.


# Detailed High-Level Nullability Test Results

## Across Scale

You can see in @fig:hl_scale that the smallest three models don’t pass any
test. After that, the trend line for tests passed goes upwards, but
it’s not monotonic: Pythia 410m passes only one test, while Pythia
160m passes three, and Pythia 6.9b passes the most tests, with 9,
while Pythia 12b only passes 7. For instance, given the partial code:

*Test 3*
```python
def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  x = process_value(value) + 1
  print(x)

def process_value(value):
```
Pythia 6.9b completes `process_value` as:
```python
def process_value(value):
  if value is None:
    return 0
  elif isinstance(value, str):
    return int(value)
  ...
```
While Pythia 12b completes it as:
```python
def process_value(value):
  return value
```

The code generated by Pythia 6.9b properly handles values of both
NoneType and str and always returns an int, whereas the code generated
by Pythia 12b is the identify function, returning None when the input
is None and returning a string when the input is a string. Neither of
these cases is correct, as the main function expects this to return a
Number that can be added to one.

## Across Time

We say in @fig:hl_time that while the model generally gets better at
passing these tests during training, the performance is not always
increasing.  Zooming in on a particular test, we can see what this
looks like. When asked to complete this code:

*Test 12*
```python
def handle_value(value, guard):
  if guard:
    return count_os("Foobar") * 2
  else:
    return count_os(value) // 2

def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  print(handle_value(value, x < 10))

def count_os(value):
```

Pythia 6.9b at training step 104000 produces the following definition
of `count_os`:

```python
def count_os(value):
  if value is None:
    return 1
  elif isinstance(value, str):
    return len(value)
...
```

While 1000 steps later, it produces a very different definition:

```python
def count_os(value):
  return len([s for s in os.listdir(os.path.join(os.getcwd(), value)) if s.start ...
```

While the first definition handles value being None or a string, the
second definition not only assumes it is a string but that it is a
name of a folder in the current directory. In some sense the model
“knows” that the value parameter is nullable at step 104000, but
“forgets” it during further training. It regains the ability to pass
the test at several points during training, but by the end of
training, it’s back to treating `value` as if it was always a string.

## Common Mistakes

As expected, type annotations are particularly difficult for these
models. When asked to complete the type of get_square in the following
code, no model in the Pythia series can successfully output the
Optional argument type:

*Test 5*
```python
def program_48() -> None:
  number: Optional[int] = None
  square = get_square(number)
  if square is not None:
    print(f"Square of the number is {square}")
  else:
    print("No number provided to square")

def get_square(number:
```

These models also find it much easier to deal with a list that
contains some None elements, than to deal with an atomic value that
might be None. Pythia 6.9b consistently passes this test in the
second half of training:

*Test 2*
```python
from typing import Optional

def main() -> None:
  some_numbers = [1, -4, None, -3, 10, -1, None, None, 8]
  print(positive_numbers(some_numbers))

def positive_numbers(numbers: list[Optional[int]]) -> list[int]:
  result: list[int] = []
  for num in numbers:
```

But is much more challenged by this code:

*Test 3*
```python
def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  x = process_value(value) + 1
  print(x)

def process_value(value):
```

, intermittently being unable to complete it in a type-safe way up,
including in the second-to-last training step. When another level of
function indirection is added, the model becomes much better at
completing it correctly; Pythia 6.9b consistently completes the
following after step 93000:

*Test 4*
```python
def handle_value(value, guard):
  if guard:
    return process_value("Foobar") + 1
  else:
    return process_value(value) + 1
def main(x: int) -> None:
  if x > 0:
    value = "*" * x
  else:
    value = None

  x = handle_value(value, x < 10)
  print(x)

def process_value(value):
```
The names of functions are highly influential in the models
understanding of their type semantics. When test 3 is changed so that
`process_value` is instead called `count_chars`, Pythia 6.9b becomes
unable to correctly complete it on any revision; it likely has a bias
from its training data that functions called `count_chars` always take
non-nullable strings. Similarly, when test 4 is changed so that
process_values becomes string_complexity, it goes from consistently
passing to almost always failing.

# Mass Mean Probing vs Linear Regression

We were initially very surprised to find that mass means probing would
perform better than linear regression. After all, linear regression is
a much more powerful technique for fitting data. And mass means
probing can be seen as giving the direction of best fit in each
dimension independently, without considering other dimensions. The
more dimensions you consider at one time, the better your model fit
can be. But repeatedly in our data, we found that mass means probing
outperformed linear regression on the test data.
